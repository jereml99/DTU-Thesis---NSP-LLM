\chapter{Introduction}
\label{ch:introduction}

\section{Background and Context}
\label{sec:background-context}

Generative agents powered by Large Language Models (LLMs) can simulate complex, human-like behavior in interactive environments such as virtual worlds, simulation games, and social scenarios. These agents enable rich narrative experiences and dynamic interactions by drawing on the commonsense reasoning and natural language capabilities of LLMs \cite{parkGenerativeAgentsInteractive2023a}. However, purely neural approaches to agent behavior often produce logical inconsistencies and incoherent action sequences. For example, an agent may attempt actions that violate environmental constraints (such as opening a door that does not exist) or pursue goals that conflict with its stated motivations, undermining the plausibility and believability of the simulation \cite{batesRoleEmotionBelievable1994}.

Park et al.\ \cite{parkGenerativeAgentsInteractive2023a} introduced a generative agent architecture that uses memory streams, reflection, and hierarchical planning to guide agent behavior. While this architecture demonstrates compelling emergent social dynamics, the hierarchical planning component lacks explicit mechanisms for verifying logical consistency or ensuring adherence to environmental constraints. As a result, agents can generate plans that are fluent and contextually plausible but nonetheless violate hard constraints or exhibit temporal inconsistencies.

Recent work in neuro-symbolic AI explores combining neural generation with symbolic planning representations to address these limitations. Tantakoun et al.\ \cite{tantakounLLMsPlanningModelers2025} demonstrate that LLMs can serve as planning modelers, generating Planning Domain Definition Language (PDDL) schemas that formalize domain constraints and action preconditions. By pairing LLM-generated PDDL with symbolic planners or validators, such approaches provide explicit, inspectable guarantees about constraint satisfaction and logical coherence. This neuro-symbolic strategy retains the flexibility and commonsense reasoning of LLMs while introducing structured verification to detect and repair invalid plans before they are executed in the environment.


\section{Problem Statement}
\label{sec:problem-statement}

Generative agents produce dynamic, human-like interactions, but their lack of consistent, constraint-respecting planning undermines realism. Current approaches either rely too heavily on symbolic systems, limiting flexibility, or on purely neural models, which lack logical guarantees.

\textbf{Research question:} How can a neuro-symbolic planning framework improve the coherence and believability of LLM-driven generative agents in interactive environments?


\section{Research Aim and Objectives}
\label{sec:research-aim-objectives}

\textbf{Aim:} To develop and evaluate a hybrid neuro-symbolic planning system in which an LLM generates PDDL-based plans and a symbolic validator verifies logical consistency, identifies constraint violations, and suggests repairs.

\textbf{Objectives:}

\begin{enumerate}
    \item Reimplement the generative agents architecture with a modular, extensible codebase that supports integration of symbolic planning components.
    \item Design and implement a PDDL-based validator that formalizes environmental constraints, detects planning violations, and generates actionable diagnostic feedback.
    \item Develop visualization and explanation tools that make agent plans, constraint violations, and repair proposals inspectable to researchers and evaluators.
    \item Evaluate the system using (a) quantitative metrics (constraint violation rates, plan success rates, and repair efficiency) comparing a baseline hierarchical planner against the neuro-symbolic approach, and (b) qualitative human evaluation of perceived believability and narrative coherence.
\end{enumerate}


\section{Methodological Overview}
\label{sec:methodological-overview}

This study builds on the generative agents architecture \cite{parkGenerativeAgentsInteractive2023a} by replacing the hierarchical planning component with a neuro-symbolic planning framework. The LLM generates PDDL action schemas and plans, which are then validated by a symbolic checker that detects constraint violations and proposes repairs. The approach is evaluated through two methods:

\begin{itemize}
    \item \textbf{Quantitative evaluation}: Constraint violation counts and rates are measured on matched scenarios for (i) a baseline system using hierarchical planning and (ii) the neuro-symbolic system with validator-guided revision rounds. Metrics include violation rates per 100 actions, plan success rates (zero violations), and repair efficiency.
    \item \textbf{Qualitative evaluation}: Perceived believability and narrative coherence are assessed via a within-subjects user study comparing agent behaviors from both systems.
\end{itemize}


\section{Scope and Limitations}
\label{sec:scope-limitations}

This project focuses on simulation environments rather than real-world robotics, where sensing uncertainty and physical dynamics introduce additional complexity. The symbolic planning component uses PDDL, which assumes deterministic action effects and complete observability within the simulated domain. Evaluation emphasizes narrative consistency, environmental constraint adherence, and perceived believability rather than real-time performance or scalability to large multi-agent systems.


\section{Thesis Structure}
\label{sec:thesis-structure}

The remainder of the thesis is organized as follows:

\begin{itemize}
    \item \textbf{Chapter 2: Theoretical Background} --- Establishes core concepts (LLMs, agents, planning paradigms including PDDL) and reviews relevant literature. Emphasizes the Generative Agents architecture and neuro-symbolic approaches that combine LLM flexibility with symbolic constraint enforcement.
    \item \textbf{Chapter 3: Methodology} --- Describes the system design, experimental setup, and evaluation protocols. Details the symbolic validator architecture and the within-subjects user study for assessing believability and constraint adherence.
    \item \textbf{Chapter 4: Results} --- Reports quantitative constraint-violation metrics and qualitative believability findings from the user study.
    \item \textbf{Chapter 5: Discussion} --- Interprets results, situates findings within the literature, and discusses limitations and implications for agent design.
    \item \textbf{Chapter 6: Conclusion and Future Work} --- Summarizes contributions and suggests directions for future research.
\end{itemize}
