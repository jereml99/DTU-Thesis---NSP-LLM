\chapter{Introduction}
\label{ch:introduction}

\section{Background and Context}
\label{sec:background-context}

Believable computational agents that simulate human behavior enable diverse applications: immersive virtual environments and social simulations \cite{parkGenerativeAgentsInteractive2023a}, rehearsal spaces for practicing interpersonal communication, and prototyping tools testing social scenarios. A critical requirement for these applications is \textit{believability}---defined here as the perceived realism of an agent's behavior, where actions appear consistent with its goals, personality, and knowledge. While non-player characters in open-world games, virtual assistants, and social robots all benefit from more natural interactions, achieving this requires an architecture that produces behavior consistent with past experience, reacts believably to environmental changes, and maintains coherence over extended interactions. A formal treatment of believability is provided in \cref{ch:background}.

Large Language Model (LLM)-driven generative agents simulate human-like behavior by combining LLMs with mechanisms for memory, reflection, and dynamic retrieval. These agents leverage commonsense reasoning and natural language capabilities to populate virtual worlds and social scenarios \cite{parkGenerativeAgentsInteractive2023a}. However, purely neural approaches often produce logical inconsistencies: agents attempt impossible actions (e.g., opening nonexistent doors), interact with absent entities, or pursue conflicting goals, distinctively undermining believability \cite{batesRoleEmotionBelievable1994}.

Park et al.\ \cite{parkGenerativeAgentsInteractive2023a} demonstrated emergent social dynamics through memory streams, reflection, and hierarchical planning. Yet, their planning component lacks mechanisms to verify logical consistency or enforce constraints, often producing plans that are contextually plausible but violate physical or temporal rules. Throughout this thesis, we refer to the work of Park et al.\ as \emph{the original paper}---the foundational architecture that we extend with symbolic planning.

To address these limitations, Neuro-symbolic AI offers a promising direction by combining neural generation with symbolic planning \cite{tantakounLLMsPlanningModelers2025}---or as in our approach, symbolic validation. Rather than relying solely on the LLM's inherent reasoning, this hybrid approach builds structured \textit{scaffolding}---formal constraints, validators, and iterative refinement mechanisms---around the core intelligence of the model. As recently as December 2025, research continues to demonstrate that such scaffolding remains a key direction for improving LLM-driven systems \cite{zhang2025recursive}. By embedding symbolic validation and constraint enforcement alongside neural generation, we create an architecture in which both components complement each other: the LLM provides flexibility and commonsense reasoning, while symbolic planning ensures logical consistency and adherence to environmental constraints.

\section*{Note on the Use of AI}
\label{sec:ai-disclaimer}

AI-assisted tools, including large language models, were used during the preparation of this thesis for tasks such as literature review, code development, writing assistance, and figure generation. A detailed account of AI usage, including specific tools, tasks, and the extent of human oversight, is provided in Chapter~\ref{ch:ai-usage}.

\section{Problem Statement}
\label{sec:problem-statement}

Existing agent architectures face a fundamental tradeoff between the logical consistency of symbolic systems and the flexibility and commonsense reasoning of LLM-driven agents.

\textbf{Research question:} How can a neuro-symbolic planning framework improve the believability of LLM-driven generative agents?\footnote{Formal definitions are provided in \cref{ch:background}: \emph{Large Language Models} (\cref{subsubsec:llm}), \emph{Agents} (\cref{def:agent}), \emph{Planning} (\cref{def:planning-problem}), \emph{Believability} (\cref{def:believability}), and \emph{Neuro-Symbolic Systems for Planning} (\cref{subsec:neuro-symbolic}).}

\section{Research Aim and Objectives}
\label{sec:research-aim-objectives}

\textbf{Aim:} To develop and evaluate a hybrid neuro-symbolic planning system where an LLM generates hierarchical plans and formal PDDL \cite{mcdermottPDDLPlanningDomain1998} specifications (actions and constraints), while a symbolic validator enforces logical consistency and guides iterative refinement.

\textbf{Objectives:}

\begin{enumerate}
    \item Reimplement the generative agents architecture \cite{parkGenerativeAgentsInteractive2023a} with a modular, extensible codebase that supports integration of symbolic planning components.
    \item Design and implement a PDDL-based validator that formalizes environmental constraints, detects planning violations, and outputs actionable diagnostic feedback.
    \item Develop and deploy a replay-based evaluation interface that allows human evaluators to assess agent believability by flagging specific actions.
    \item Evaluate the system using (a) quantitative metrics (constraint violation rates, plan success rates, and repair efficiency) comparing a baseline hierarchical planner against the neuro-symbolic approach, and (b) a within-subjects user study to qualitatively assess the perceived believability of agent behaviors.
\end{enumerate}


% Section removed as it was repetitive with the Objectives section.


\section{Scope and Limitations}
\label{sec:scope-limitations}


This project focuses on simulation environments with deterministic action effects and complete observability. Evaluation focuses on constraint adherence and perceived believability rather than real-time performance or scalability to large multi-agent systems.

% To rewrite at the end.
\section{Thesis Structure}
\label{sec:thesis-structure}
The remainder of the thesis is organized as follows:
\begin{itemize}
    \item \textbf{Chapter 2: Theoretical Background} --- Establishes core concepts (LLMs, agents, planning paradigms including PDDL) and reviews relevant literature.
    \item \textbf{Chapter 3: Methodology} --- Describes the system design, experimental setup, and evaluation protocols. Details the symbolic validator architecture and the within-subjects user study for assessing believability and constraint adherence.
    \item \textbf{Chapter 4: Results} --- Reports quantitative constraint-violation metrics and qualitative believability findings from the user study.
    \item \textbf{Chapter 5: Discussion} --- Interprets results, situates findings within the literature, and discusses limitations and implications for agent design.
    \item \textbf{Chapter 6: Conclusion and Future Work} --- Summarizes contributions and suggests directions for future research.
    \item \textbf{Chapter 7: Use of AI in this Thesis} --- Declaration of AI tools used in the thesis preparation.
\end{itemize}
