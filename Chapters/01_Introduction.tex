\chapter{Introduction}

\section{Background and context}

Artificial intelligence has increasingly turned toward \textbf{Large Language Models (LLMs)} for generating human-like text and behaviour. When embedded in generative agents, these models can simulate complex social dynamics and interactive narratives. Despite their linguistic fluency, such agents frequently exhibit logical inconsistencies and incoherent action sequences within simulated environments.

Recent work (for example, \cite{park2023} and \cite{zhao2023}) suggests that combining symbolic reasoning with LLMs can improve task coherence and narrative believability. This thesis focuses first on a neuro-symbolic validator, a form of symbolic scaffolding: the LLM continues to propose goals and hierarchical sketches, while a symbolic validator checks proposed plans against explicit constraints, reports violations, and suggests repairs or diagnostics. The aim is to obtain the benefits of symbolic guarantees such as constraint enforcement, temporal checks, and explainability without initially replacing the LLM's proposal role with a planner.

\section{Problem statement}

Generative agents produce dynamic, human-like interactions, but their lack of consistent, constraint-respecting planning undermines realism. Current approaches either rely too heavily on symbolic systems, limiting flexibility, or on purely neural models, which lack logical guarantees.

	extbf{Problem statement:} How can a neuro-symbolic scaffolding system improve the coherence and believability of LLM-driven generative agents in interactive environments?

\section{Research aim and objectives}

	extbf{Aim:} To develop and evaluate a hybrid neuro-symbolic scaffolding system in which an LLM proposes plans and a symbolic validator verifies, critiques, and suggests repairs to those plans.

	extbf{Objectives:}
\begin{enumerate}
\item Reimplement the original system codebase in a modular, well-documented, and extensible architecture to ease future extensions and maintenance.
\item Design a validator schema and data contract for expressing constraints, diagnostics, and suggested repairs, and implement a prototype that integrates an LLM sketching module with this symbolic validator, including tooling to surface explanations and repair proposals.
\item Improve UI and UX tooling to surface explanations, visualizations, and decision rationales so that agent decisions and plans are inspectable by researchers and evaluators.
\item Evaluate the prototype using both symbolic evaluation (constraint adherence, repair frequency, diagnostic coverage) and human-centered evaluation (perceived believability, narrative coherence).
\end{enumerate}

\section{Methodological overview}

The study combines computational implementation with human-centered evaluation, integrating a large language model with a symbolic validator that inspects LLM-generated plans. Evaluation comprises two complementary strands: symbolic evaluation (constraint adherence, repair frequency, and diagnostic coverage) and human-centered evaluation (perceived believability, narrative coherence, and qualitative feedback on explainability and user-facing tooling).

\section{Scope and limitations}

The project focuses on simulation environments rather than real-world robotics. Symbolic representations are limited to deterministic domains, and results primarily assess narrative consistency and social plausibility.

\section{Thesis structure}
\begin{itemize}
	\item \textbf{Chapter 2:} Theoretical background and related work. This chapter situates the validator first approach within prior work on symbolic planners, neural planners, and hybrid systems. It also reviews literature on agent believability and techniques for explanation and debugging in interactive settings.
	\item \textbf{Chapter 3:} Methodology. This chapter describes the running example, the environment and data used in experiments, the validator schema and data contract, and the evaluation protocol for both symbolic metrics and human subject studies.
	\item \textbf{Chapter 4:} Implementation and Evaluation. Here we present the software architecture, the LLM sketching module, the symbolic validator implementation, and the visualization and explainability tools used to surface diagnostics and repair proposals.
	\item \textbf{Chapter 5:} Results and Discussion. This chapter reports symbolic evaluation results, summarizes outcomes from human centered studies, analyzes common failure modes, and interprets what the results imply for agent coherence and believability.
	\item \textbf{Chapter 6:} Conclusion and Future Work. We close by summarizing contributions and limitations, and by outlining an agenda for next steps such as integrating planning components, expanding to non deterministic domains, and improving user facing explainability.
\end{itemize}
