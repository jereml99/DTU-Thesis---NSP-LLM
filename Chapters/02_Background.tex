\chapter{Theoretical Background}
\label{ch:background}

This chapter establishes conceptual foundations and reviews prior work motivating this thesis.
It defines core concepts (LLMs, agents, planning paradigms including PDDL), reviews LLM-driven generative agents with emphasis on the foundational Generative Agents paper \cite{parkGenerativeAgentsInteractive2023a}, and examines hybrid neuro-symbolic approaches combining LLM flexibility with symbolic planning guarantees \cite{huangPlanningDarkLLMSymbolic2024, tantakounLLMsPlanningModelers2025}. We highlight the challenge of maintaining coherence (adherence to environmental constraints) and believability (human-perceived realism) in LLM-driven agents.

\begin{example}[Running Example: Student NPC]
    \label{ex:running-student}
    Throughout this chapter, we illustrate concepts using a running example: an NPC simulating a university student managing academic and social commitments. The student must coordinate coursework (attending lectures, completing assignments), part-time work (café shifts), and social activities (meeting friends, attending events) while respecting environmental rules (must be enrolled in a course to attend its lectures). 
    
    \textit{Note: While this example references temporal and location constraints for pedagogical completeness, this thesis focuses on validating action preconditions and environmental feasibility within a single-agent, fully observable simulation environment. The neuro-symbolic framework is designed to handle temporal and location constraints in principle; their absence from this work reflects the specific scope of the simulation.}
    
    A complete PDDL formulation of this student scheduling domain, including domain predicates, action schemas, and a concrete problem instance with a valid plan, is provided in \cref{app:pddl-example}.
\end{example}


\section{Core Concepts and Definitions}

\subsection{Planning}
\label{subsec:planning}

To ground our approach, we introduce the formal concepts underlying planning.

\begin{definition}[State]
\label{def:state}
A \emph{state} $s \in S$ is a complete description of the environment at a given time, capturing all facts that are relevant for action execution and goal satisfaction. In classical planning, states are assumed to be fully observable and are typically represented as sets of propositional facts that are true in the world; any fact not included in the set is assumed to be false (closed-world assumption).
\end{definition}

\begin{definition}[Action]
\label{def:action}
An \emph{action} is a discrete event that transitions the system from one state to another. Formally, in a planning problem, an action $a \in A$ is an element of the action set. An action is characterized by:
\begin{itemize}
    \item \textbf{Preconditions}: facts that must be true in the current state for the action to be executable;
    \item \textbf{Effects}: facts that become true (or false) after the action executes.
\end{itemize}

More precisely, an action $a$ can be represented as a tuple $a = (\text{pre}(a), \text{eff}^{+}(a), \text{eff}^{-}(a))$, where:
\begin{itemize}
    \item $\text{pre}(a) \subseteq S$ is the set of precondition facts; an action is applicable in state $s$ if $\text{pre}(a) \subseteq s$ (under the closed-world assumption);
    \item $\text{eff}^{+}(a) \subseteq S$ are the \emph{positive effects}—facts that become true after execution;
    \item $\text{eff}^{-}(a) \subseteq S$ are the \emph{negative effects}—facts that become false after execution.
\end{itemize}

\begin{definition}[Transition Function]
\label{def:transition}
A \emph{transition function} $T: S \times A \to S$ maps a current state and an action to a resulting state. For an action $a$ and state $s$, $T(s, a) = s'$ describes how the environment evolves when action $a$ is executed in state $s$.
\end{definition}

The transition function for an action is thus: if $\text{pre}(a) \subseteq s$, then $T(s, a) = (s \setminus \text{eff}^{-}(a)) \cup \text{eff}^{+}(a)$, producing a new state $s'$ that reflects the additions and deletions induced by the action.

When the preconditions of an action are satisfied, the action is said to be \emph{applicable} in the current state. Executing an applicable action transitions the system via the transition function $T$: $T(s, a) = s'$, where $s$ is the current state and $s'$ is the resulting state after the action's effects are applied.
\end{definition}

\begin{definition}[Classical Planning Problem]
\label{def:planning-problem}
A \emph{classical planning problem} is a tuple $\langle S, A, T, I, G \rangle$:
\begin{itemize}
    \item $S$ is a finite set of states;
    \item $A$ is a finite set of actions;
    \item $T: S \times A \to S$ is a deterministic transition function mapping a state and action to a next state;
    \item $I \subseteq S$ is a non-empty set of initial states;
    \item $G \subseteq S$ is a non-empty set of goal states (any state in $G$ is an acceptable outcome).
\end{itemize}
\end{definition}

\begin{definition}[Plan, Validity, and Solution]
\label{def:plan}
A \emph{plan} is a finite sequence of actions $\pi = \langle a_1, \ldots, a_n \rangle$. For an initial state $s_0 \in I$, the \emph{execution} of $\pi$ from $s_0$ is defined recursively: let $s_0' = s_0$, and for $i = 1, \ldots, n$, let $s_i' = T(s_{i-1}', a_i)$. 

The plan $\pi$ is \emph{valid} for a planning problem $\langle S, A, T, I, G \rangle$ (from Definition \ref{def:planning-problem}) if, for every initial state $s_0 \in I$, the execution of $\pi$ reaches a goal state, i.e., $s_n' \in G$. A valid plan is called a \emph{solution} to the planning problem.
\end{definition}

\paragraph{Classical Planning Assumptions}
The formulation above relies on standard assumptions commonly adopted in classical planning:
\begin{itemize}
    \item \textbf{Determinism}: actions have deterministic outcomes;
    \item \textbf{Full observability}: the planner has complete knowledge of the current state;
    \item \textbf{Instantaneous actions}: actions do not consume time;
    \item \textbf{Single-agent setting}: interactions with other agents are not considered.
\end{itemize}
These assumptions enable precise reasoning about plan validity and correctness. Relaxing them leads to more expressive planning models but can lead to fewer logical guarantees; some of these are discussed later in this thesis.

\begin{definition}[Planner]
\label{def:planner}
A \emph{planner} is an algorithm that, given a classical planning problem $\langle S, A, T, I, G \rangle$, searches for a valid plan $\pi$ (if one exists) that transforms an initial state $I$ into a state satisfying the goal condition $G$ by applying actions whose preconditions are satisfied and whose effects induce state transitions \cite{russell2010aima,ghallab2004automated}.

Planners explore the induced state space using different search strategies, including breadth-first search, depth-first search, and heuristic-guided search, and may optimize for criteria such as plan length, action cost, or makespan \cite{ghallab2004automated}. Classical planning systems include STRIPS \cite{fikes1971strips}, GraphPlan \cite{blum1997graphplan}, and modern heuristic planners such as Fast Downward \cite{helmert2006fast}.
\end{definition}

\begin{example}[Simple Classical Planning Problem]
\label{ex:simple-planning}
Consider a minimal instance for our student running example.

\textbf{States}: The student can be in two locations: home or café. The state set is:
\[
S = \{\texttt{at-home}, \texttt{at-cafe}\}.
\]

\textbf{Actions}: The student can move between locations:
\[
A = \{\texttt{go-to-cafe}, \texttt{go-to-home}\}.
\]

\textbf{Transition function}: The transition function $T$ encodes how each action changes the state. For example:
\begin{itemize}
    \item $T(\texttt{at-home}, \texttt{go-to-cafe}) = \texttt{at-cafe}$;
    \item $T(\texttt{at-cafe}, \texttt{go-to-home}) = \texttt{at-home}$;
    \item $T(\texttt{at-home}, \texttt{go-to-home}) = \texttt{at-home}$ (no change);
    \item $T(\texttt{at-cafe}, \texttt{go-to-cafe}) = \texttt{at-cafe}$ (no change).
\end{itemize}

\textbf{Initial state}: The student begins at home:
\[
I = \{\texttt{at-home}\}.
\]

\textbf{Goal state}: The student wants to work at the café:
\[
G = \{\texttt{at-cafe}\}.
\]

\textbf{Solution}: The plan $\pi = \langle \texttt{go-to-cafe} \rangle$ is valid:
\begin{itemize}
    \item Start at $s_0' = \texttt{at-home} \in I$.
    \item Execute action $\texttt{go-to-cafe}$: 
    \[
    s_1' = T(\texttt{at-home}, \texttt{go-to-cafe}) = \texttt{at-cafe}.
    \]
    \item End state: $s_1' \in G$.
\end{itemize}

This minimal problem has a unique solution; extending it with time, resources, or additional constraints would yield more complex planning instances where multiple or no solutions may exist.
\end{example}

\subsubsection{Hierarchical Planning}

Hierarchical task networks (HTN) represent an extension of classical planning that organizes planning as a multi-level process. Rather than finding a sequence of primitive actions directly from a goal (as classical planners do), HTN planning starts with high-level tasks and repeatedly decomposes them into simpler subtasks using predefined decomposition methods, until primitive actions are reached \cite{erolUMCPSoundComplete, nauSHOP2HTNPlanning2003}. This hierarchical structure enables abstraction (breaking complex goals into manageable pieces), reuse (sharing decomposition strategies across problems), and more efficient search through the planning space.

\begin{remark}[Task and Action Terminology]
\label{rem:task-action-terminology}
Throughout this thesis, we distinguish between two hierarchical levels of planning behavior. A \emph{task} is a high-level, daily-planned schedule unit (e.g., ``work café shift 09:00--12:00''), as synthesized in the Generative Agents architecture. An \emph{action}, in the formal planning sense above, is a primitive discrete event characterized by preconditions and effects. In the context of neuro-symbolic scaffolding, a task is decomposed into a sequence of fine-grained actions that, when executed, fulfill the task's objective. The action definition here (Definition \ref{def:action}) corresponds to the primitive action units into which tasks are decomposed in our framework.
\end{remark}

Park et al.'s agents generate daily plans with morning, afternoon, and evening blocks containing embedded tasks, resembling HTN decomposition without explicit HTN semantics or validation \cite{parkGenerativeAgentsInteractive2023a}.

Informally, we envision generating high-level tasks from high-level goals and then decomposing them into sequences of primitive actions. In our running example, the task ``prepare café for 09:00 opening'' might decompose into a sequence of dependent actions: ``unlock café door,'' ``disarm alarm system,'' ``turn on lights,'' ``unlock and prepare register,'' and ``count opening cash float.'' Each action must be executed in order, as later actions depend on earlier ones (e.g., the register cannot be prepared before the door is unlocked). 

\subsection{Symbolic Planning}
\label{subsec:symbolic-planning}

Symbolic planning uses explicit, compositional representations to algorithmically search for valid plans \cite{fikesStripsNewApproach1971, mcdermottPDDLPlanningDomain1998}. Key strengths:

\begin{enumerate}
    \item \textbf{Explainability}: Plans are sequences of named actions with explicit preconditions and effects, enabling causal trace inspection.
    \item \textbf{Constraint enforcement}: Planners guarantee that plans satisfy all preconditions, avoid violated invariants, and respect temporal and resource bounds.
    \item \textbf{Optimality}: Many planners find optimal or bounded-suboptimal solutions under well-defined cost models.
\end{enumerate}

These properties directly address coherence: if behavior is synthesized via a symbolic planner, environmental constraints are satisfied by construction.

\subsubsection{PDDL: Planning Domain Definition Language}

To enable symbolic planning, domains and problems must be specified formally. The Planning Domain Definition Language (PDDL) is the standard representation used by most automated planners.

\begin{definition}[PDDL Domain and Problem]
\label{def:pddl}
A PDDL \emph{domain} specifies:
\begin{itemize}
    \item \textbf{Predicates}: atomic facts about the world (e.g., \texttt{(at agent location)}, \texttt{(has-key agent key)});
    \item \textbf{Action schemas}: parameterized templates specifying action preconditions and effects, which ground to concrete actions when parameters are instantiated.
\end{itemize}

A PDDL \emph{problem} specifies:
\begin{itemize}
    \item \textbf{Initial state}: predicates that are true at the start;
    \item \textbf{Goal specification}: predicates that must be true in the final state.
\end{itemize}

Together, a domain and problem define a planning problem in the sense of Definition \ref{def:planning-problem}.
\end{definition}

PDDL is valuable because it is explicit and verifiable: once a domain and problem are specified, any algorithm can check that action preconditions are met, goals are reachable, and no contradictions are present. Prior work demonstrates that large language models can translate natural-language task and domain descriptions into PDDL predicates, action schemas, and problem instances, which can be solved reliably when embedded within neuro-symbolic pipelines that enforce syntactic and semantic validity through external planners and validators \cite{silver2024generalized, kwonFastAccurateTask}. 

While newer LLMs exhibit improved reasoning and code generation capabilities, existing evidence suggests that reliability in symbolic planning arises primarily from scaffolding mechanisms—such as validation, decomposition, and repair—rather than from model scale alone. Consequently, improvements in underlying LLMs are best viewed as enhancing the effectiveness of such pipelines, rather than eliminating the need for formal correctness guarantees. This motivation directly underlies the neuro-symbolic approach: leverage the LLM's language understanding to generate initial PDDL schemas from natural language descriptions, then validate and refine those schemas using automated planning tools or constraint verification. This combination preserves the flexibility and commonsense reasoning of LLMs while enforcing the consistency guarantees that formal planning requires.

The primary limitation is \textbf{authoring cost}: PDDL domains require manual specification of predicates, actions, preconditions, and effects, which is brittle and labor-intensive for open-ended environments. Symbolic planning also struggles with commonsense reasoning and social nuance unless explicitly encoded \cite{haslumIntroductionPlanningDomain2019}.

\subsubsection{Plan Validation and the VAL Tool}

A critical challenge across all planning approaches is verification: how do we confirm that a proposed plan is logically correct before execution? While symbolic planners guarantee correctness by construction (valid plans are synthesized directly), plans from other sources—whether hierarchically decomposed, manually authored, or generated by other means—require post-hoc verification against the formal domain specification.

\textbf{VAL} (the Plan Validator) is a domain-independent tool that checks whether a plan is valid with respect to a PDDL domain and problem specification \cite{howey2004val}. Given a domain, problem, and candidate plan, VAL simulates the plan's execution and verifies:

\begin{itemize}
    \item \textbf{Action applicability}: each action's preconditions are satisfied before execution;
    \item \textbf{State transitions}: effects are correctly applied, yielding well-defined successor states;
    \item \textbf{Goal achievement}: the final state satisfies the problem goal.
\end{itemize}

If any check fails, VAL reports the plan as invalid with diagnostic details, such as which precondition was unsatisfied or at what step the plan diverged from expectations.

VAL is widely used in automated planning research and competitions as the standard for validating planner outputs. Operationally, it follows the formal semantics of PDDL, making it a trusted oracle for correctness.

\begin{example}[VAL Validation in the Student Domain]
    \label{ex:val-student}
    Consider the student task ``work café shift 09:00 to 12:00,'' decomposed by the LLM into: \texttt{(walk-to-cafe)}, \texttt{(unlock-cafe-door)}, \texttt{(work-shift 3-hours)}. 

    VAL checks preconditions in sequence:
    \begin{itemize}
        \item \texttt{(walk-to-cafe)}: precondition \texttt{(at student home)} is true in the initial state. Action succeeds; agent is now at café.
        \item \texttt{(unlock-cafe-door)}: precondition \texttt{(has-key student cafe-key)} is checked. If the agent does not possess the key in the initial state, VAL reports: ``precondition unsatisfied; plan invalid.''
    \end{itemize}

    The LLM's decomposition is rejected, and feedback is provided so the LLM can refine the plan (e.g., retrieve the key before attempting to unlock).
\end{example}

\paragraph{Limitations} 
VAL only validates against the given PDDL model. If the model is incomplete or misspecified, a plan may pass validation despite being problematic in practice. Additionally, VAL does not measure plan quality, plausibility, or believability—it enforces logical correctness only.

\subsection{Agents and Believability}
\label{subsec:agents-believability}

\begin{definition}[Agent]
    \label{def:agent}
    An \emph{agent} receives percepts and produces actions via sensors and actuators, implementing a mapping $f: P^{*} \to A$ from percept histories $P^{*}$ to actions $A$ \cite{russellArtificialIntelligenceModern2022}.
\end{definition}

\begin{definition}[Generative Agent]
    \label{def:generative-agent}
    A \emph{generative agent} is a computational software agent that simulates believable human behavior by leveraging large language models to generate action sequences, maintain memory of past experiences, reflect on accumulated observations, and plan future behavior based on goals and social context.
\end{definition}

In simulated environments and games, non-player characters (NPCs) prioritize producing behavior that human observers find plausible, consistent, and engaging rather than maximizing numerical rewards \cite{mateasOzCentricReviewInteractive1999, loyallBelievableAgentsBuilding}. As a result, the evaluation of such agents commonly emphasizes observer perception over internal optimality or correctness.

\begin{definition}[Believability]
    \label{def:believability}
    \emph{Believability}---the ``illusion of life''---is the human-perceived realism of agent behavior: whether actions align with character goals, personality, knowledge, and social norms \cite{batesRoleEmotionBelievable1994, loyallBelievableAgentsBuilding}.
\end{definition}

Importantly, believability differs from psychological (emotional and mental states) or cognitive (thinking and reasoning) realism. Following Bates' analysis of character animation, believability permits suspension of disbelief—the observer accepts the character as having intentions and coherence—rather than requiring fidelity to actual human cognition (how people really think) or motivation (why they act) \cite{batesRoleEmotionBelievable1994}. An animated character can be believable despite vastly simplified internal mechanisms. In the case of NPC agents simulating university students in daily routines, there is natural alignment between believability and realism: student behaviors (attending lectures, working shifts, socializing) are grounded in observable, familiar reality. However, this alignment is contingent on context; the goal remains observer acceptance and perceived intentionality, not psychological accuracy. If an agent's actions respect temporal and causal constraints while being consistent with its apparent goals and knowledge, observers will find it believable, even if its internal reasoning mechanisms are simplified or heuristic.

Constraining agents with realistic physical and environmental limits increases perceived believability by reducing violations known to break immersion, such as impossible actions or logically inconsistent plans \cite{batesRoleEmotionBelievable1994, bogdanovychWhatMakesVirtual2016}. Park et al.\ showed that LLM-driven agents with memory, reflection, and planning were rated more believable than human crowdworkers role-playing agents in controlled evaluations. The crowdworker baseline was given access to each agent's full memory stream and simulated behavior, yet LLM-driven agents still significantly outperformed this human-authored comparison \cite{parkGenerativeAgentsInteractive2023a}. Xiao et al.\ propose metrics such as Consistency and Robustness for agent simulations based on character profiles \cite{xiaoHowFarAre2024}.

\begin{definition}[Coherence]
    \label{def:coherence}
    In the context of agent behavior, \emph{coherence} is the degree to which an action sequence respects domain constraints: preconditions are satisfied before each action, temporal and resource constraints are not violated, and the sequence does not contradict environmental invariants or prior commitments \cite{youngOverviewMimesisArchitecture, riedlNarrativePlanningBalancing2010}. Formally, a sequence of actions $\pi = \langle a_1, \ldots, a_n \rangle$ is \emph{coherent with respect to a planning problem} $\langle S, A, T, I, G \rangle$ (from \cref{def:planning-problem}) if executing $\pi$ from the initial state(s) $I$ via the transition function $T$ neither violates action preconditions nor environmental invariants. In other words, a coherent action sequence corresponds to a valid plan (though not necessarily an optimal one, and independent of whether it reaches the goal state $G$).
\end{definition}

Operationally, coherence is enforced through constraint adherence which for us will be solely satisfaction of action preconditions. While believability literature uses ``coherence'' informally to denote plausible behavior, in this thesis we use it in the stricter sense of constraint satisfaction validated by symbolic validation tools.

\begin{example}[Coherence Violation]
    \label{ex:coherence-violation}
    In our running example (\cref{ex:running-student}), a coherent agent must not schedule overlapping commitments (e.g., attending two simultaneous classes) or attempt impossible actions (e.g., submitting an assignment before completing it). A purely LLM-based planner might generate ``attend lecture at 10:00'' and ``work café shift 09:00 to 12:00'' without detecting the temporal conflict.
\end{example}


\begin{definition}[Epistemic Plausibility]
    \label{def:epistemic-plausibility}
    \emph{Epistemic plausibility} is the consistency of an agent's actions with what it could reasonably know or believe given its observations and prior experience.
\end{definition}

While formal epistemic planning explicitly represents agents' beliefs \cite{Bolander_2017}, such approaches typically assume a closed and fully specified domain with a finite set of actions and observability conditions \cite{bolanderEpistemicPlanning2011}. In contrast, open-world systems with natural-language action spaces, such as LLM-driven agents, cannot feasibly maintain complete epistemic state tracking. Thus, epistemic plausibility is not tractable as a system-level enforcement objective in this work. However, improving coherence reduces certain epistemic violations: actions respecting environmental and causal constraints are less likely to presuppose unreasonable or unobserved information, indirectly supporting perceived believability.

\begin{example}[Coherent but Epistemically Implausible]
    \label{ex:epistemic-implausible}
    A detective agent immediately accuses the correct culprit. The accusation is coherent (fits the agent's role, doesn't violate timeline or resources) but epistemically implausible: the agent never observed the key evidence, and no belief update justifies such certainty. From a narrative perspective, the action appears believable; from a knowledge-tracking perspective, it breaks plausibility because the agent acts with unearned certainty.
\end{example}

We adopt the NPC perspective in which believability is the primary objective. While humans remain believable despite occasional planning mistakes, we hypothesize that in LLM-driven agents, *eliminating systematic incoherence*---via symbolic validation---meaningfully improves believability by reducing violations that are immediately recognizable as incoherent to observers.

For a character portrayed as cognitively sound, coherence boundaries are defined by what is physically or logically impossible versus what is merely implausible. A character has no reasonable justification for certain errors: cooking a meal before purchasing ingredients, or unlocking a door without possessing the key are physically or logically impossible actions. In contrast, more subtle mistakes (forgetting a prior commitment, miscalculating available time, submitting work that was not fully completed) are believable because they are mistakes a person of sound mind might plausibly make. These latter errors fall into a gray zone that observers can forgive or attribute to understandable human fallibility. However, in our system design, we choose not to tolerate plans that violate constraint satisfaction, even if such violations might be forgivable in certain contexts. Since our simulations span only one day, avoiding a single reality-breaking error that violates physical or logical laws is a worthwhile trade-off for preserving immersion, even if it means erring on the side of caution compared to human behavior. This strict interpretation of coherence is thus a pragmatic choice for our scope, and exploring the optimal balance between logical correctness and forgivable mistakes across longer simulations remains a direction for further work.


\subsection{LLM-Based Planning}
\label{subsec:llm-planning}

To understand how LLMs can be applied to planning, we first establish what Large Language Models are, their capabilities, and their limitations.

\subsubsection{Large Language Models (LLMs)}
\label{subsubsec:llm}

Large language models are neural networks trained on vast text corpora to generate text one word (or token) at a time. Given a prompt, an LLM predicts the most likely next word based on patterns learned during training. It repeats this process iteratively to produce multi-sentence responses, adapting its output to the context of the prompt and any specified instructions.

\begin{example}[LLM Input and Output]
    \label{ex:llm-example}
    \textbf{Input prompt}: ``The student woke up late and missed the morning lecture. How might they feel?''

    \textbf{LLM output}: ``The student likely feels frustrated and disappointed. They might worry about missing important content and falling behind in the course. Depending on the course policies, they may also feel anxious about whether they can catch up or access lecture notes.''

    The LLM generates this response through learned statistical associations in its training data. It produces plausible, fluent text by predicting probable next tokens based on context, rather than following explicit rules or reasoning steps.
\end{example}

Key capabilities of LLMs relevant to this work:
\begin{itemize}
    \item \textbf{Instruction following}: Responding appropriately to natural language directives (e.g., ``format this as a list'', ``explain in simple terms'').
    \item \textbf{Few-shot learning}: Improving performance when given a few examples before a task.
    \item \textbf{Structured output generation}: Producing formal notations (code, logical rules, task lists) when prompted\cite{StructuredModelOutputs}.
\end{itemize}

However, LLMs have critical limitations:
\begin{itemize}
    \item \textbf{No logical guarantees}: They perform statistical inference, not deductive reasoning. An LLM can produce plausible-sounding but logically inconsistent outputs\cite{bender2021dangers,valmeekam2024llms}. 
    \item \textbf{Context window limits}: They cannot retain information from arbitrarily long histories by design\cite{vaswani2017attention}; older context may be lost or forgotten.
    \item \textbf{Hallucination}: They may invent facts, actions, or constraints not grounded in the actual domain or agent memory \cite{huang2025survey,ji2023survey}.
\end{itemize}

To address these limitations, we adopt \emph{scaffolding}:

\begin{definition}[Scaffolding]
    \label{def:scaffolding}
    \emph{Scaffolding} refers to external processes or structures that support a reasoning system in accomplishing tasks beyond its unaided capabilities, by constraining, guiding, or validating its intermediate outputs, without modifying the underlying model \cite{wood1976scaffolding}.
\end{definition}

In the context of LLM-based agents, scaffolding can take the form of translating natural language outputs into formal representations, validating candidate actions against explicit constraints, and iteratively refining solutions using external feedback. These mechanisms directly mitigate common failure modes of LLM planning, including logical inconsistency, hallucination, and limited context sensitivity.

In this thesis, scaffolding is employed to support the generation of formal planning specifications and the decomposition of high-level goals into executable action sequences—domains where LLM fluency and commonsense reasoning are beneficial, but where formal correctness and consistency are required.

\subsubsection{Planning with LLMs}


\subsection{Neuro-Symbolic Systems for Planning}
\label{subsec:neuro-symbolic}

Neuro-symbolic systems combine learned (sub-symbolic) components with symbolic representations and reasoning to achieve both flexibility and guarantees \cite{garcezNeuralSymbolicComputingEffective2019, garcezNeurosymbolicAI3rd2020}. The central idea is to leverage the strengths of each paradigm: LLMs excel at natural-language understanding, commonsense reasoning, and flexible task decomposition, while symbolic systems enforce logical consistency and provide formal guarantees.

LLMs approximate hierarchical planning by proposing outlines, subgoals, and steps in natural language \cite{weiChainofThoughtPromptingElicits2023}. 

Some integration patterns include:

\begin{enumerate}
    \item \textbf{LLM-propose/symbolic-validate}: The LLM generates candidate action sequences; a symbolic validator (such as VAL) checks whether preconditions are satisfied and constraints respected \cite{huangPlanningDarkLLMSymbolic2024}.
    \item \textbf{Iterative refinement}: Plans are critiqued by symbolic validators, and feedback improves LLM proposals \cite{silverGeneralizedPlanningLarge2023, valmeekamonPlanningLargeLanguage2023}.
    \item \textbf{Shared world models}: A symbolic state representation is updated from neural perception and queried for decisions \cite{parkGenerativeAgentsInteractive2023a}.
\end{enumerate}

Recent work explores these patterns for PDDL generation. Tantakoun et al. survey approaches where LLMs construct PDDL domain and problem files from natural language, with symbolic planners validating and executing \cite{tantakounLLMsPlanningModelers2025}. Huang et al. propose a pipeline in which multiple LLM instances generate diverse candidate PDDL action schemas, which are filtered for semantic coherence and validated by a symbolic planner to ensure domain solvability \cite{huangPlanningDarkLLMSymbolic2024}. \footnote{Unlike the present work, Huang et al.\ use the symbolic planner not only for validation but also to generate complete plans once a consistent domain schema has been identified.}

Our approach combines multiple integration patterns: an LLM proposes a decomposition of a high-level task into primitive actions; a symbolic validator (such as VAL) then checks whether those actions respect domain constraints, enforcing the concept of \emph{coherence} defined in Section \ref{subsec:agents-believability}. This validation-as-feedback loop enables iterative refinement: violations are reported with diagnostics, allowing the LLM to improve its proposals until a coherent plan emerges.

\begin{example}[Neuro-Symbolic Validation of Action Decomposition]
    \label{ex:neurosymbolic-validation}
    The LLM receives a daily \emph{task} ``work café shift (09:00 to 12:00)'' and decomposes it into fine-grained \emph{actions}: ``walk to café,'' ``unlock door,'' ``turn on lights,'' ``serve customers,'' ``clean tables.'' The PDDL validator (VAL) checks each \emph{action}:
    \begin{itemize}
        \item Preconditions (door must exist and agent must have key before unlocking; lights require door to be unlocked and agent inside),
        \item Location constraints (agent must be at café to unlock door; cannot serve customers while at storage room),
        \item Temporal ordering (cannot turn on lights before unlocking door; cannot clean tables before serving customers ends).
    \end{itemize}
    Violations are flagged with diagnostics (e.g., ``action \texttt{unlock\_door} fails: precondition \texttt{(has-key agent cafe-key)} unsatisfied in initial state; agent inventory empty''). The rejected decomposition is returned to the LLM with feedback for refinement, operationalizing constraint-driven coherence validation.
\end{example}

\section{Literature Review}

This section reviews work that informed the implementation of the \textit{scaffold} in this thesis, focusing on the Generative Agents paper that motivates our system and hybrid approaches combining LLMs with symbolic planning.

\subsection{Generative Agents: Interactive Simulacra of Human Behavior (Park et al., 2023)}

Park et al.\ introduced generative agents: LLM-driven NPCs simulating believable human behavior in Smallville, a sandbox town environment \cite{parkGenerativeAgentsInteractive2023a}.

\begin{figure}[H]\centering
    \includegraphics[width=0.8\linewidth]{Pictures/GenerativeAgentsSims.jpg}
    \caption{Smallville environment (adapted from Park et al.\ \cite{parkGenerativeAgentsInteractive2023a}). A sandbox environment populated with generative agents where agents autonomously plan daily activities, exchange information, form relationships, and coordinate group events with minimal user intervention.}\label{fig:generative-agents-sims}
\end{figure}

The architecture comprises:

\begin{enumerate}
    \item \textbf{Memory Stream}: Time-stamped observations (own actions, others' actions, environment events), retrieved by weighted combination of recency (exponential decay), importance (LLM-rated 1 to 10), and relevance (embedding cosine similarity).
    \item \textbf{Reflection}: Periodic synthesis of experiences into higher-level insights. The LLM generates questions about recent observations, produces abstract insights with citations to source memories, and organizes these into reflection trees (observations → reflections → meta-reflections).
    \item \textbf{Planning}:
          \begin{enumerate}
              \item Daily objectives are synthesized from the agent's personality profile (innate traits, occupation, background), current situational goals (e.g., organizing a party, meeting deadlines), and established lifestyle patterns (sleep schedule, work hours, meal times, exercise routines).
              \item These objectives are then decomposed hierarchically: day-level objectives are refined into hour-level plan segments, which are further decomposed into concrete 5-to-15 minute \emph{action} sequences at execution time as each hour-level \emph{task} becomes due.
              \item Agents dynamically re-plan when circumstances change, with the LLM deciding whether to continue the current plan or react to new observations.
          \end{enumerate}
\end{enumerate}

\begin{figure}[H]\centering
    \includegraphics[width=0.8\linewidth]{Pictures/GenerativeAgents-System.jpg}
    \caption{Generative agent architecture (adapted from Park et al.\ \cite{parkGenerativeAgentsInteractive2023a}). Agents perceive their environment, and all perceptions are saved in a comprehensive record of the agent's experiences called the memory stream. Based on their perceptions, the architecture retrieves relevant memories and uses those retrieved actions to determine an action. These retrieved memories are also used to form longer-term plans and create higher-level reflections, both of which are entered into the memory stream for future use.}\label{fig:generative-agents-architecture}
\end{figure}

\textbf{Emergent behaviors} emerged in a 25-agent, two-day simulation where agents maintained coherent behavior. Park et al.\ demonstrated that LLM agents with memory, reflection, and planning achieve high believability. For example, information about an event started by one of the agents spread to 13 other agents (52\%), and notably, 5 of the 12 explicitly invited agents attended at the correct time and location.

\textbf{Evaluation} comprised a controlled study with 100 human participants recruited via an online crowdsourcing platform, who ranked agent responses using a TrueSkill-based aggregation. A human-authored baseline was constructed by recruiting 25 crowdworkers, each assigned to a single agent; these workers were given access to the agent's full memory stream and a replay of its simulated behavior, and were instructed to role-play the agent when answering interview questions. Baseline responses were manually inspected by the authors for coherence and consistency with the assigned persona, with low-quality or out-of-character submissions discarded and replaced. Evaluators were blind to condition and ranked responses from the full architecture, multiple ablations, and the human baseline. The full architecture (memory + reflection + planning) achieved $\mu = 29.89, \sigma = 0.72$, significantly outperforming both ablations and the human-authored baseline ($\mu = 22.95, \sigma = 0.69$), with effect size $d = 8.16$ ($p < 0.001$). Interview questions assessed self-knowledge, memory recall, future plans, reactive decisions, and reflections.

\begin{figure}[H]\centering
    \includegraphics[width=0.8\linewidth]{Pictures/GenerativeAgents-Results.jpg}
    \caption{Evaluation results (adapted from Park et al.\ \cite{parkGenerativeAgentsInteractive2023a}). The full generative agent architecture produces more believable behavior than the ablated architectures and the human crowdworkers. Each additional ablation reduces the performance of the architecture.}\label{fig:generative-agents-results}
\end{figure}

Notably, the crowdworker baseline ($\mu = 22.95$) scored lower than even the ablated condition without reflection and planning ($\mu = 25.64$), despite being given access to full memory streams and past behavior with manual quality curation. While we acknowledge Park et al.'s status as a highly influential paper (extensively cited across agent and LLM research), we lack access to open peer reviews that might contextualize this result. Absent such public critique, we accept the findings at face value, though the baseline performance invites questions about evaluation setup fairness—particularly whether crowdworkers role-playing from a past record could adequately capture how agents make decisions in real time.

However, this believability came at a cost. \textbf{Failure modes} included memory retrieval errors, hallucinations, and over-cooperation. Specifically, action generation lacked precondition validation; coherence relied on the LLM's implicit understanding during generation, with no explicit verification against domain constraints.

This limitation motivates our approach: we propose integrating PDDL-based symbolic validation to *enforce* constraint satisfaction and detect incoherent plans before execution, preserving the expressiveness of LLM-driven planning while eliminating logical inconsistencies that undermine believability.

\subsection{LLMs as Planning Modelers (Tantakoun et al., 2025)}

Tantakoun et al.\ survey how LLMs construct and refine automated planning models rather than directly perform planning \cite{tantakounLLMsPlanningModelers2025}. They review approximately 80 papers, positioning LLMs as tools for extracting planning models to support reliable planners, addressing the limitation that LLMs struggle with long-horizon planning requiring structured reasoning.

\textbf{Three planning paradigms} structure the survey:
\begin{enumerate}
    \item \textbf{LLMs-as-Heuristics}: Enhance planner search efficiency.
    \item \textbf{LLMs-as-Planners}: Directly generate action sequences or plan proposals.
    \item \textbf{LLMs-as-Modelers}: Construct PDDL domain and problem files (survey focus).
\end{enumerate}

Within LLMs-as-Modelers:

\textbf{Task Modeling} (approximately 30 papers): LLMs generate PDDL problem files from goal specifications, using few-shot prompting \cite{collinsGroundingLanguageGeneralized2022} to chain-of-thought techniques \cite{lyuFaithfulChainofThoughtReasoning2023}. Well-explored but relies on detailed predicate specification.

\textbf{Domain Modeling} (approximately 15 papers): LLMs generate PDDL domain files (predicates and action schemas), more challenging than task specification. Approaches include generate-test-critique loops incrementally building components \cite{guanLeveragingPretrainedLarge2023}. Single-domain generation risks misalignment with human expectations due to natural language ambiguity.

\textbf{Hybrid Modeling} (approximately 15 papers): LLMs generate both domain and problem files. Systems use human-in-the-loop approaches with anomaly detection \cite{ye2024domainindependent}. Coordinating both introduces complexity; linear pipelines risk cascading errors.

\textbf{Key findings} from the survey show that:
\begin{enumerate}
    \item LLMs generate syntactically valid PDDL but struggle with semantic consistency.
    \item Iterative refinement with symbolic planner feedback improves model quality.
\end{enumerate}

This grounds our approach: we position the LLM as a PDDL schema generator (domain modeling) with symbolic validator feedback and iterative refinement.

%% ADD GOOGLE PAPER ABOUT REPAIRING PDDL

\subsection{Other Neuro-Symbolic Planning Approaches}

Additional work combines LLMs with formal planning:

\textbf{Robotics and grounded planning}: SayCan pairs LLM language grounding with value estimates over affordances to select feasible actions in robotic manipulation \cite{ahnCanNotSay2022}.

\textbf{Iterative refinement loops}: LLM+P and related frameworks prompt an LLM to propose high-level plans, check them with a PDDL planner, and iterate until valid \cite{silverGeneralizedPlanningLarge2023, valmeekamonPlanningLargeLanguage2023, lyuFaithfulChainofThoughtReasoning2023}. Critique-and-repair loops such as Reflexion and Tree-of-Thought add self-evaluation and search, while symbolic constraints prune or guide search \cite{shinn2023reflexion, yao2023treeofthoughts}.

\textbf{Temporal planning integration}: Extensions incorporate duration and resource checks to prevent overlaps and enforce deadlines \cite{cashmorePlanningTemporalDomains2019}.

\subsection{Summary of Insights and Research Focus}

The literature suggests three converging insights:

\begin{enumerate}
    \item \textbf{LLM planning produces human-like behavior but lacks long-horizon coherence} due to missing symbolic grounding, context window limitations, and hallucination \cite{parkGenerativeAgentsInteractive2023a, xiaoHowFarAre2024}.
    \item \textbf{Neuro-symbolic methods offer structure and guarantees} by validating or synthesizing plans against explicit models of actions, time, and resources \cite{huangPlanningDarkLLMSymbolic2024, tantakounLLMsPlanningModelers2025}.
    \item \textbf{Believability and coherence should be assessed jointly}: constraint adherence is necessary for plausibility (coherence), but human evaluation is required to confirm realism (believability) \cite{parkGenerativeAgentsInteractive2023a, xiaoHowFarAre2024}.
\end{enumerate}

