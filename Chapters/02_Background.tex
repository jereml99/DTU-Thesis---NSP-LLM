\chapter{Theoretical Background}
\label{ch:background}

This chapter establishes conceptual foundations and reviews prior work motivating this thesis. It defines core concepts (LLMs, agents, planning paradigms including PDDL), reviews LLM-driven generative agents with emphasis on the seminal Generative Agents paper \cite{parkGenerativeAgentsInteractive2023a}, and examines hybrid neuro-symbolic approaches combining LLM flexibility with symbolic planning guarantees \cite{huangPlanningDarkLLMSymbolic2024, tantakounLLMsPlanningModelers2025}. We emphasize the challenge of maintaining coherence (adherence to environmental constraints) and believability (human-perceived realism) in LLM-driven agents.

\begin{example}[Running Example: Student NPC]
\label{ex:running-student}
Throughout this chapter, we illustrate concepts using a running example: an NPC simulating a university student managing academic and social commitments. The student must coordinate coursework (attending lectures, completing assignments), part-time work (café shifts), and social activities (meeting friends, attending events) while respecting temporal constraints (no overlapping commitments), location constraints (cannot be in two places simultaneously), and environmental rules (must be enrolled in a course to attend its lectures).
\end{example}


\section{Core Concepts and Definitions}

\subsection{Large Language Models (LLMs)}
\label{subsec:llm}

Large language models are neural networks trained on vast text corpora to generate text one word (or token) at a time. Given a prompt, an LLM predicts the most likely next word based on patterns learned during training. It repeats this process iteratively to produce multi-sentence responses, adapting its output to the context of the prompt and any specified instructions.

\begin{example}[LLM Input and Output]
\label{ex:llm-example}
\textbf{Input prompt}: ``The student woke up late and missed the morning lecture. How might they feel?''

\textbf{LLM output}: ``The student likely feels frustrated and disappointed. They might worry about missing important content and falling behind in the course. Depending on the course policies, they may also feel anxious about whether they can catch up or access lecture notes.''

The LLM generates this response through learned statistical associations in its training data. It produces plausible, fluent text by predicting probable next tokens based on context, rather than following explicit rules or reasoning steps.
\end{example}

Key capabilities of LLMs relevant to this work:
\begin{itemize}
    \item \textbf{Instruction following}: Responding appropriately to natural language directives (e.g., ``format this as a list'', ``explain in simple terms'').
    \item \textbf{Few-shot learning}: Improving performance when given a few examples before a task.
    \item \textbf{Structured output generation}: Producing formal notations (code, logical rules, task lists) when prompted.
\end{itemize}

However, LLMs have critical limitations:
\begin{itemize}
    \item \textbf{No logical guarantees}: They perform statistical inference, not deductive reasoning. An LLM can produce plausible-sounding but logically inconsistent outputs.
    \item \textbf{Context window limits}: They cannot retain information from arbitrarily long histories; older context may be lost or forgotten.
    \item \textbf{Hallucination}: They may invent facts, actions, or constraints not grounded in the actual domain or agent memory.
\end{itemize}

For this thesis, the critical use of LLMs is to generate formal planning specifications and decompose high-level goals into concrete action sequences—tasks where their fluency and commonsense are valuable, but where their lack of formal correctness must be constrained by external validation.

\subsection{Planning and PDDL}

To ground our approach, we introduce the formal language used to specify planning problems: PDDL (Planning Domain Definition Language).

\begin{definition}[Classical Planning Problem]
\label{def:planning-problem}
A \emph{classical planning problem} is a tuple $\langle S, A, T, I, G \rangle$: $S$ is a state set, $A$ is an action set, $T: S \times A \to S$ is a transition function, $I \subseteq S$ is the initial state set, and $G \subseteq S$ is the goal state set. A \emph{plan} is a finite action sequence $\pi = \langle a_1, \ldots, a_n \rangle$ such that executing $\pi$ from any $s \in I$ via $T$ reaches some $g \in G$ \cite{ghallabAutomatedPlanningTheory2004}.
\end{definition}

\begin{definition}[PDDL]
\label{def:pddl}
\emph{PDDL (Planning Domain Definition Language)} is a formal notation for specifying planning problems in a way that automated solvers can process \cite{mcdermottPDDLPlanningDomain1998, ghallabAutomatedPlanningTheory2004}. A PDDL specification has two parts:
\begin{enumerate}
    \item \textbf{Domain}: Defines predicates (facts that can be true or false) and action schemas (named actions with preconditions—facts required before execution—and effects—facts that change when the action runs).
    \item \textbf{Problem}: Specifies objects, the initial state (which facts are true at the start), and goal conditions (which facts must be true at the end).
\end{enumerate}
\end{definition}

\begin{example}[Student Scheduling Domain in PDDL]
\label{ex:student-pddl}
Consider scheduling a student for our running example. A simplified domain might define:

\textbf{Predicates}:
\begin{itemize}
    \item \texttt{(enrolled ?s - student ?c - course)}: student $s$ is enrolled in course $c$.
    \item \texttt{(at-location ?s - student ?l - location)}: student $s$ is at location $l$.
    \item \texttt{(completed ?s - student ?a - assignment)}: student $s$ has completed assignment $a$.
\end{itemize}

\textbf{Action schema} (simplified):
\begin{itemize}
    \item \texttt{attend-lecture}(?s - student, ?c - course, ?t - time):
    \begin{itemize}
        \item Preconditions: student must be enrolled in the course, the lecture must be scheduled at time $t$, the student must be at the lecture hall location.
        \item Effects: the student is now at the lecture hall, the student has learned the lecture content.
    \end{itemize}
\end{itemize}

\textbf{Problem instance}: The student is initially at home, enrolled in two courses (Math and History), and has no assignments completed. The goal is for the student to attend both lectures and complete one assignment by noon.

A PDDL solver (an automated planner) takes this domain and problem as input and produces a sequence of actions (a plan) that starts in the initial state and reaches the goal. The solver guarantees that all preconditions are satisfied before each action executes, and that temporal and resource constraints (if specified) are respected.
\end{example}

PDDL is valuable because it is explicit and verifiable: once a plan is generated, anyone (or any algorithm) can check that each step's preconditions are met and that contradictions are absent.

\textbf{Relevance to LLMs}: LLMs are good at understanding natural language domain descriptions (e.g., ``a student must be enrolled in a course before attending its lecture'') and can be prompted to output PDDL predicates and action schemas. However, LLMs may produce syntactically valid PDDL that is semantically inconsistent or incomplete. For instance, an LLM might forget to include a precondition (e.g., neglecting that the café key is required to unlock the café door), leading to invalid plans. This motivates our approach: use the LLM to generate initial PDDL schemas from natural language, then validate and refine them using automated planning tools.

\subsection{Agents and Believability}
\label{subsec:agents-believability}

\begin{definition}[Agent]
\label{def:agent}
An \emph{agent} receives percepts and produces actions via sensors and actuators, implementing a mapping $f: P^{*} \to A$ from percept histories $P^{*}$ to actions $A$ \cite{russellArtificialIntelligenceModern2022}.
\end{definition}

In simulated environments and games, non-player characters (NPCs) prioritize producing behavior that human observers find plausible, consistent, and engaging rather than maximizing numerical rewards \cite{mateasOzCentricReviewInteractive1999, loyallBelievableAgentsBuilding}. As a result, the evaluation of such agents commonly emphasizes observer perception over internal optimality or correctness.

\begin{definition}[Believability]
\label{def:believability}
\emph{Believability}---the ``illusion of life''---is the human-perceived realism of agent behavior: whether actions align with character goals, personality, knowledge, and social norms \cite{batesRoleEmotionBelievable1994, loyallBelievableAgentsBuilding}.
\end{definition}

Importantly, believability differs from psychological (emotional and mental states) or cognitive (thinking and reasoning) realism. Following Bates' analysis of character animation, believability permits suspension of disbelief—the observer accepts the character as having intentions and coherence—rather than requiring fidelity to actual human cognition (how people really think) or motivation (why they act) \cite{batesRoleEmotionBelievable1994}. An animated character can be believable despite vastly simplified internal mechanisms. In the case of NPC agents simulating university students in daily routines, there is natural alignment between believability and realism: student behaviors (attending lectures, working shifts, socializing) are grounded in observable, familiar reality. However, this alignment is contingent on context; the goal remains observer acceptance and perceived intentionality, not psychological accuracy. If an agent's actions respect temporal and causal constraints while being consistent with its apparent goals and knowledge, observers will find it believable, even if its internal reasoning mechanisms are simplified or heuristic.

Constraining agents with realistic physical and environmental limits increases perceived believability by reducing violations known to break immersion, such as impossible actions or logically inconsistent plans \cite{batesRoleEmotionBelievable1994, bogdanovychWhatMakesVirtual2016}. Park et al.\ showed that LLM-driven agents with memory, reflection, and planning were rated more believable than human crowdworkers role-playing agents in controlled evaluations. The crowdworker baseline was given access to each agent's full memory stream and simulated behavior, yet LLM-driven agents still significantly outperformed this human-authored comparison \cite{parkGenerativeAgentsInteractive2023a}. Xiao et al.\ propose metrics such as Consistency and Robustness for agent simulations based on character profiles \cite{xiaoHowFarAre2024}.

\begin{definition}[Coherence]
\label{def:coherence}
\emph{Coherence} is the causal and temporal consistency of behavior: whether actions are feasible, properly ordered, and do not contradict prior commitments or environmental constraints \cite{youngOverviewMimesisArchitecture, riedlNarrativePlanningBalancing2010}.
\end{definition}

Coherence is operationalized through constraint adherence: satisfaction of action preconditions, absence of temporal overlaps, respect for resource limits, and compliance with environmental invariants.

\begin{example}[Coherence Violation]
\label{ex:coherence-violation}
In our running example (\cref{ex:running-student}), a coherent agent must not schedule overlapping commitments (e.g., attending two simultaneous classes) or attempt impossible actions (e.g., submitting an assignment before completing it). A purely LLM-based planner might generate ``attend lecture at 10:00'' and ``work café shift 09:00 to 12:00'' without detecting the temporal conflict.
\end{example}

\begin{definition}[Epistemic Plausibility]
\label{def:epistemic-plausibility}
\emph{Epistemic plausibility} is the consistency of an agent's actions with what it could reasonably know or believe given its observations and prior experience.
\end{definition}

While formal epistemic planning explicitly represents agents' beliefs \cite{Bolander_2017}, such approaches typically assume a closed and fully specified domain with a finite set of actions and observability conditions \cite{bolanderEpistemicPlanning2011}. In contrast, open-world systems with natural-language action spaces, such as LLM-driven agents, cannot feasibly maintain complete epistemic state tracking. Thus, epistemic plausibility is not tractable as a system-level enforcement objective in this work. However, improving coherence reduces certain epistemic violations: actions respecting environmental and causal constraints are less likely to presuppose unreasonable or unobserved information, indirectly supporting perceived believability.

\begin{example}[Coherent but Epistemically Implausible]
\label{ex:epistemic-implausible}
A detective agent immediately accuses the correct culprit. The accusation is coherent (fits the agent's role, doesn't violate timeline or resources) but epistemically implausible: the agent never observed the key evidence, and no belief update justifies such certainty. From a narrative perspective, the action appears believable; from a knowledge-tracking perspective, it breaks plausibility because the agent acts with unearned certainty.
\end{example}

We adopt the NPC perspective in which believability is the primary objective. We hypothesize that coherence---enforced through symbolic validation---is a necessary but not sufficient condition for believability. The proposed neuro-symbolic approach preserves expressive, context-aware behavior from large language models while eliminating logical inconsistencies that undermine perceived intentionality.

\subsection{Symbolic Planning}

Symbolic planning uses explicit, compositional representations to algorithmically search for valid plans \cite{fikesStripsNewApproach1971, mcdermottPDDLPlanningDomain1998}. Key strengths:

\begin{enumerate}
    \item \textbf{Explainability}: Plans are sequences of named actions with explicit preconditions and effects, enabling causal trace inspection.
    \item \textbf{Constraint enforcement}: Planners guarantee that plans satisfy all preconditions, avoid violated invariants, and respect temporal and resource bounds.
    \item \textbf{Optimality}: Many planners find optimal or bounded-suboptimal solutions under well-defined cost models.
\end{enumerate}

These properties directly address coherence: if behavior is synthesized via a symbolic planner, environmental constraints are satisfied by construction.

The primary limitation is \textbf{authoring cost}: PDDL domains require manual specification of predicates, actions, preconditions, and effects, which is brittle and labor-intensive for open-ended environments. Symbolic planning also struggles with commonsense reasoning and social nuance unless explicitly encoded \cite{haslumIntroductionPlanningDomain2019}.

\subsection{LLM-Based Planning}

LLM-based planning uses language models to generate action sequences or subgoal decompositions directly from text \cite{ahnCanNotSay2022, huang2022zeroshotplanning}. LLMs can draft plausible multi-step procedures via chain-of-thought prompting \cite{weiChainofThoughtPromptingElicits2023}, propose alternatives, and adapt plans to soft preferences without formal domain models.

However, LLM-generated plans lack correctness guarantees and can:
\begin{itemize}
    \item Omit necessary preconditions (opening a door without checking accessibility),
    \item Violate environmental invariants (simultaneous presence in two locations),
    \item Drift temporally (forgetting earlier commitments when context windows truncate),
    \item Hallucinate actions or states not grounded in the environment \cite{xiaoHowFarAre2024}.
\end{itemize}

For believability-centric NPCs, these failures manifest as broken commitments, physical impossibilities, and social incoherence. Park et al.'s Generative Agents exhibited emergent social behaviors but lacked constraint enforcement, relying on LLM prompt engineering to maintain temporal coherence heuristically \cite{parkGenerativeAgentsInteractive2023a}.

\subsection{Hierarchical Planning}

Hierarchical task networks (HTN) decompose high-level tasks into ordered or partially ordered subtasks until primitive actions are reached, using methods encoding admissible refinements and constraints \cite{erolUMCPSoundComplete, nauSHOP2HTNPlanning2003}. Hierarchy supports abstraction, reuse, and tractable search.

LLMs approximate hierarchical planning by proposing outlines, subgoals, and steps in natural language \cite{weiChainofThoughtPromptingElicits2023}. Park et al.'s agents generate daily plans with morning, afternoon, and evening blocks containing embedded tasks, resembling HTN decomposition without explicit HTN semantics or validation \cite{parkGenerativeAgentsInteractive2023a}.

Formal HTN or temporal PDDL planners can validate such decompositions, ensuring high-level commitments refine into feasible, non-overlapping primitive actions. In our running example, ``complete coursework this week'' might decompose into ``attend lectures,'' ``complete assignments,'' and ``study for exam,'' with temporal constraints preventing overlaps and ensuring deadlines are met.

\subsection{Neuro-Symbolic Systems for Planning}
\label{subsec:neuro-symbolic}

Neuro-symbolic systems combine learned (sub-symbolic) components with symbolic representations and reasoning to achieve both flexibility and guarantees \cite{garcezNeuralSymbolicComputingEffective2019, garcezNeurosymbolicAI3rd2020}. Common integration patterns:

\begin{enumerate}
    \item \textbf{LLM-propose/symbolic-plan}: The LLM generates candidate schemas and plans; a symbolic planner forms a sequence of actions that satisfy the constraints \cite{huangPlanningDarkLLMSymbolic2024}.
    \item \textbf{Iterative refinement}: Plans are critiqued by symbolic validators or planners, and feedback improves LLM proposals \cite{silverGeneralizedPlanningLarge2023, valmeekamonPlanningLargeLanguage2023}.
    \item \textbf{Shared world models}: A symbolic state representation is updated from neural perception and queried for decisions.
\end{enumerate}

Recent work explores these patterns for PDDL generation. Tantakoun et al. survey approaches where LLMs construct PDDL domain and problem files from natural language, with symbolic planners validating and executing \cite{tantakounLLMsPlanningModelers2025}. Huang et al. propose a pipeline in which multiple LLM instances generate diverse candidate PDDL action schemas, which are filtered for semantic coherence and validated by a symbolic planner to ensure domain solvability \cite{huangPlanningDarkLLMSymbolic2024}. \footnote{Unlike the present work, Huang et al.\ use the symbolic planner not only for validation but also to generate complete plans once a consistent domain schema has been identified.}

This thesis follows the iterative refinement pattern with a symbolic verifier. The LLM generates tasks and action sequences grounded in agent memory; these are translated into PDDL schemas and validated by a symbolic planner to enforce temporal, causal, and resource constraints. Validation failures produce diagnostic feedback that can be returned to the LLM for iterative repair, preserving naturalistic behavior while ensuring logical coherence.

\begin{example}[Neuro-Symbolic Validation of Action Decomposition]
\label{ex:neurosymbolic-validation}
The LLM receives a daily task ``work café shift (09:00 to 12:00)'' and decomposes it into fine-grained actions: ``walk to café,'' ``unlock door,'' ``turn on lights,'' ``serve customers,'' ``clean tables.'' The PDDL validator checks each action:
\begin{itemize}
\item Preconditions (door must exist and agent must have key before unlocking; lights require door to be unlocked and agent inside),
\item Location constraints (agent must be at café to unlock door; cannot serve customers while at storage room),
\item Temporal ordering (cannot turn on lights before unlocking door; cannot clean tables before serving customers ends).
\end{itemize}
Violations are flagged with diagnostics (e.g., ``action \texttt{unlock\_door} fails: precondition \texttt{(has-key agent cafe-key)} unsatisfied in initial state; agent inventory empty'').
\end{example}

\section{Literature Review}

This section reviews work on neuro-symbolic planning for LLM-driven agents, focusing on the Generative Agents paper that motivates our system and hybrid approaches combining LLMs with symbolic planning.

\subsection{Generative Agents: Interactive Simulacra of Human Behavior (Park et al., 2023)}

Park et al.\ introduced generative agents: LLM-driven NPCs simulating believable human behavior in Smallville, a sandbox town environment \cite{parkGenerativeAgentsInteractive2023a}. 

\begin{figure}[H]\centering
    \includegraphics[width=0.8\linewidth]{Pictures/GenerativeAgentsSims.jpg}
    \caption{Smallville environment (adapted from Park et al.\ \cite{parkGenerativeAgentsInteractive2023a}). A sandbox environment populated with generative agents where agents autonomously plan daily activities, exchange information, form relationships, and coordinate group events with minimal user intervention.}\label{fig:generative-agents-sims}
\end{figure}

The architecture comprises:

\begin{enumerate}
    \item \textbf{Memory Stream}: Time-stamped observations (own actions, others' actions, environment events), retrieved by weighted combination of recency (exponential decay), importance (LLM-rated 1 to 10), and relevance (embedding cosine similarity).
    \item \textbf{Reflection}: Periodic synthesis of experiences into higher-level insights. The LLM generates questions about recent observations, produces abstract insights with citations to source memories, and organizes these into reflection trees (observations → reflections → meta-reflections).
    \item \textbf{Planning}: 
    \begin{enumerate}
        \item Daily objectives are synthesized from the agent's personality profile (innate traits, occupation, background), current situational goals (e.g., organizing a party, meeting deadlines), and established lifestyle patterns (sleep schedule, work hours, meal times, exercise routines).
        \item These objectives are then decomposed hierarchically: day-level objectives are refined into hour-level plan segments, which are further decomposed into concrete 5-to-15 minute action sequences at execution time as each hour-level task becomes due.
        \item Agents dynamically re-plan when circumstances change, with the LLM deciding whether to continue the current plan or react to new observations.
    \end{enumerate}
\end{enumerate}

\begin{figure}[H]\centering
    \includegraphics[width=0.8\linewidth]{Pictures/GenerativeAgents-System.jpg}
    \caption{Generative agent architecture (adapted from Park et al.\ \cite{parkGenerativeAgentsInteractive2023a}). Agents perceive their environment, and all perceptions are saved in a comprehensive record of the agent's experiences called the memory stream. Based on their perceptions, the architecture retrieves relevant memories and uses those retrieved actions to determine an action. These retrieved memories are also used to form longer-term plans and create higher-level reflections, both of which are entered into the memory stream for future use.}\label{fig:generative-agents-architecture}
\end{figure}

\textbf{Emergent behaviors} emerged in a 25-agent, two-day simulation where agents maintained temporally coherent behavior. Park et al.\ demonstrated that LLM agents with memory, reflection, and planning achieve high believability. Information about Isabella's Valentine's Day party spread to 13 agents (52\%), and notably, 5 of the 12 explicitly invited agents attended at the correct time and location.

\textbf{Evaluation} comprised a controlled study with 100 human participants recruited via an online crowdsourcing platform, who ranked agent responses using a TrueSkill-based aggregation. A human-authored baseline was constructed by recruiting 25 crowdworkers, each assigned to a single agent; these workers were given access to the agent's full memory stream and a replay of its simulated behavior, and were instructed to role-play the agent when answering interview questions. Baseline responses were manually inspected by the authors for coherence and consistency with the assigned persona, with low-quality or out-of-character submissions discarded and replaced. Evaluators were blind to condition and ranked responses from the full architecture, multiple ablations, and the human baseline. The full architecture (memory + reflection + planning) achieved $\mu = 29.89, \sigma = 0.72$, significantly outperforming both ablations and the human-authored baseline ($\mu = 22.95, \sigma = 0.69$), with effect size $d = 8.16$ ($p < 0.001$). Interview questions assessed self-knowledge, memory recall, future plans, reactive decisions, and reflections.

\begin{figure}[H]\centering
    \includegraphics[width=0.8\linewidth]{Pictures/GenerativeAgents-Results.jpg}
    \caption{Evaluation results (adapted from Park et al.\ \cite{parkGenerativeAgentsInteractive2023a}). The full generative agent architecture produces more believable behavior than the ablated architectures and the human crowdworkers. Each additional ablation reduces the performance of the architecture.}\label{fig:generative-agents-results}
\end{figure}

Notably, the crowdworker baseline ($\mu = 22.95$) scored lower than even the ablated condition without reflection and planning ($\mu = 25.64$), despite being given access to full memory streams and past behavior with manual quality curation. While we acknowledge Park et al.'s status as a highly influential paper (extensively cited across agent and LLM research), we lack access to open peer reviews that might contextualize this result. Absent such public critique, we accept the findings at face value, though the baseline performance invites questions about evaluation setup fairness—particularly whether crowdworkers role-playing from a past record could adequately capture how agents make decisions in real time.

However, this believability came at a cost. \textbf{Failure modes} included memory retrieval errors, hallucinations, and over-cooperation. More critically, the system lacked explicit symbolic grounding—no formal model of time, resources, or environmental constraints. Temporal coherence was maintained heuristically through textual schedules that could drift or produce overlaps. Action generation lacked precondition validation; coherence relied on the LLM's implicit understanding during generation, with no explicit verification against domain constraints.

This limitation motivates our approach: we propose integrating PDDL-based symbolic validation to *enforce* constraint satisfaction and detect incoherent plans before execution, preserving the expressiveness of LLM-driven planning while eliminating logical inconsistencies that undermine believability.

\subsection{LLMs as Planning Modelers (Tantakoun et al., 2025)}

Tantakoun et al.\ survey how LLMs construct and refine automated planning models rather than directly perform planning \cite{tantakounLLMsPlanningModelers2025}. They review approximately 80 papers, positioning LLMs as tools for extracting planning models to support reliable planners, addressing the limitation that LLMs struggle with long-horizon planning requiring structured reasoning.

\textbf{Three modeling paradigms} structure the survey:
\begin{enumerate}
    \item \textbf{LLMs-as-Heuristics}: Enhance planner search efficiency.
    \item \textbf{LLMs-as-Planners}: Directly generate action sequences or plan proposals.
    \item \textbf{LLMs-as-Modelers}: Construct PDDL domain and problem files (survey focus).
\end{enumerate}

Within LLMs-as-Modelers:

\textbf{Task Modeling} (approximately 30 papers): LLMs generate PDDL problem files from goal specifications, using few-shot prompting \cite{collinsGroundingLanguageGeneralized2022} to chain-of-thought techniques \cite{lyuFaithfulChainofThoughtReasoning2023}. Well-explored but relies on detailed predicate specification.

\textbf{Domain Modeling} (approximately 15 papers): LLMs generate PDDL domain files (predicates and action schemas), more challenging than task specification. Approaches include generate-test-critique loops incrementally building components \cite{guanLeveragingPretrainedLarge2023}. Single-domain generation risks misalignment with human expectations due to natural language ambiguity.

\textbf{Hybrid Modeling} (approximately 15 papers): LLMs generate both domain and problem files. Systems use human-in-the-loop approaches with anomaly detection \cite{ye2024domainindependent}. Coordinating both introduces complexity; linear pipelines risk cascading errors.

\textbf{Key findings} from the survey show that:
\begin{enumerate}
    \item LLMs generate syntactically valid PDDL but struggle with semantic consistency.
    \item Iterative refinement with symbolic planner feedback improves model quality.
\end{enumerate}

This grounds our approach: we position the LLM as a PDDL schema generator (domain modeling) with symbolic validator feedback and iterative refinement.

\subsection{Planning in the Dark: LLM-Symbolic Pipeline without Experts (Huang et al., 2024)}

Huang et al.\ \cite{huangPlanningDarkLLMSymbolic2024} propose an LLM-symbolic planning pipeline eliminating expert intervention in action schema generation and validation. Natural-language task descriptions are inherently ambiguous; under reasonable assumptions, a single LLM instance has less than 0.0001\% probability of generating a solvable action-schema set, but combining multiple LLM instances raises this to over 95\% (explanation follows) \cite{huangPlanningDarkLLMSymbolic2024}.

\textbf{The three-step architecture} comprises:

\begin{enumerate}
    \item \textbf{Diverse Schema Library}: Deploy $N$ LLM instances with high temperature to generate complete action schema sets for $M$ actions. Aggregate into a library with approximately $N^M$ possible combinations.
    \item \textbf{Semantic Coherence Filtering}: Use sentence encoders to compute cosine similarity between natural language descriptions $E(Z(\alpha))$ and generated schemas $E(\hat{\alpha})$. Apply Conformal Prediction (CP) to calculate threshold $\hat{q}$ at confidence level $1 - \epsilon$, filtering schemas below threshold. This reduces combinations from $N^M$ to $\prod_{i=1}^{M} m_i$ where $m_i$ is passing schemas per action. Fine-tune encoder with triplet loss using hard negatives (manipulated schemas with predicate swaps, negations, removals).
    \item \textbf{Plan Generation and Ranking}: Feed solvable schema sets into symbolic planner (DUAL-BWFS). Rank generated plans by cumulative semantic similarity.
\end{enumerate}

\textbf{Key findings} demonstrate that:
\begin{itemize}
    \item Layman (ambiguous) descriptions yield 2.35 times more distinct solvable schema sets than detailed expert descriptions (8039 vs.\ 3419 with 10 LLMs, no CP), reflecting diverse valid interpretations.
    \item With 10 LLM instances, probability of generating at least one solvable schema set exceeds 95\%. This result depends on the per-instance success probability $p$ (the chance a single LLM generates a solvable schema from an ambiguous description). Improvements in LLM capabilities would increase $p$, reducing the number of instances required to maintain high coverage. However, the fundamental source of diversity is natural-language ambiguity—multiple valid interpretations of a task description—which may persist even with stronger models, suggesting a continued role for multiple instances across improving LLM generations.
    \item CP filtering reduces combinations to 3.3\% of original (1051/31,483) while increasing solvable ratio from 10.9\% to 23.0\%.
    \item Human evaluators ranked pipeline-generated plans (mean 2.97) significantly higher than Tree-of-Thought baselines (3.58); gold standard ranked 1.79.
    \item Pipeline successfully solved Sussman Anomaly (requiring interleaved subgoal handling); direct LLM approaches (GLM, GPT-3.5, GPT-4o) failed by attempting linear goal ordering.
\end{itemize}

\textbf{Relevance}: Huang et al.\ demonstrate that LLM diversity and semantic validation produce solvable PDDL schemas without expert intervention. Their pipeline validates schema feasibility via symbolic planners, ensuring coherence. We adapt this: rather than generating diverse schema libraries, we use iterative refinement with planner feedback to improve schema quality.

\subsection{Other Neuro-Symbolic Planning Approaches}

Additional work combines LLMs with formal planning:

\textbf{Robotics and grounded planning}: SayCan pairs LLM language grounding with value estimates over affordances to select feasible actions in robotic manipulation \cite{ahnCanNotSay2022}.

\textbf{Iterative refinement loops}: LLM+P and related frameworks prompt an LLM to propose high-level plans, check them with a PDDL planner, and iterate until valid \cite{silverGeneralizedPlanningLarge2023, valmeekamonPlanningLargeLanguage2023, lyuFaithfulChainofThoughtReasoning2023}. Critique-and-repair loops such as Reflexion and Tree-of-Thought add self-evaluation and search, while symbolic constraints prune or guide search \cite{shinn2023reflexion, yao2023treeofthoughts}.

\textbf{Temporal planning integration}: Extensions incorporate duration and resource checks to prevent overlaps and enforce deadlines \cite{cashmorePlanningTemporalDomains2019}.

\subsection{Summary of Insights and Research Focus}

The literature suggests three converging insights:

\begin{enumerate}
    \item \textbf{LLM planning produces human-like behavior but lacks long-horizon coherence} due to missing symbolic grounding, context window limitations, and hallucination \cite{parkGenerativeAgentsInteractive2023a, xiaoHowFarAre2024}.
    \item \textbf{Neuro-symbolic methods offer structure and guarantees} by validating or synthesizing plans against explicit models of actions, time, and resources \cite{huangPlanningDarkLLMSymbolic2024, tantakounLLMsPlanningModelers2025}.
    \item \textbf{Believability and coherence should be assessed jointly}: constraint adherence is necessary for plausibility (coherence), but human evaluation is required to confirm realism (believability) \cite{parkGenerativeAgentsInteractive2023a, xiaoHowFarAre2024}.
\end{enumerate}

