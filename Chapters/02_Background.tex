\chapter{Theoretical Background}
\label{ch:background}

This chapter establishes conceptual foundations and reviews prior work motivating this thesis.
It defines core concepts (LLMs, agents, planning paradigms including PDDL), reviews LLM-driven generative agents with emphasis on the foundational Generative Agents paper \cite{parkGenerativeAgentsInteractive2023a}, and examines hybrid neuro-symbolic approaches combining LLM flexibility with symbolic planning guarantees \cite{huangPlanningDarkLLMSymbolic2024, tantakounLLMsPlanningModelers2025}. We highlight the challenge of maintaining coherence (adherence to environmental constraints) and believability (human-perceived realism) in LLM-driven agents.

\begin{example}[Running Example: Isabella Rodriguez (Cafe Owner)]
    \label{ex:running-student}
    Throughout this chapter, we illustrate concepts using a running example based on \textbf{Isabella Rodriguez}, a persona from our simulation. Isabella is the owner of Hobbs Cafe and is currently planning a Valentine's Day party. She must coordinate cafe operations (opening the cafe, serving customers, managing inventory), social interactions (planning with friends like Maria Lopez), and personal errands (shopping for party supplies), while respecting environmental rules (must be at the cafe to unlock it).

\end{example}


\section{Core Concepts and Definitions}

\subsection{Planning}
\label{subsec:planning}

To ground our approach, we introduce the formal concepts underlying planning.

\begin{definition}[State]
    \label{def:state}
    A \emph{state} $s \in S$ is a complete description of the environment at a given time, capturing all facts that are relevant for action execution and goal satisfaction. In classical planning, states are assumed to be fully observable and are typically represented as sets of propositional facts that are true in the world; any fact not included in the set is assumed to be false (closed-world assumption).
\end{definition}

\begin{definition}[Action]
    \label{def:action}
    An \emph{action} is a discrete event that transitions the system from one state to another. Formally, in a planning problem, an action $a \in A$ is an element of the action set. An action is characterized by:
    \begin{itemize}
        \item \textbf{Preconditions}: facts that must be true in the current state for the action to be executable;
        \item \textbf{Effects}: facts that become true (or false) after the action executes.
    \end{itemize}

    More precisely, an action $a$ can be represented as a tuple $a = (\text{pre}(a), \text{eff}^{+}(a), \text{eff}^{-}(a))$, where:
    \begin{itemize}
        \item $\text{pre}(a) \subseteq S$ is the set of precondition facts; an action is applicable in state $s$ if $\text{pre}(a) \subseteq s$ (under the closed-world assumption);
        \item $\text{eff}^{+}(a) \subseteq S$ are the \emph{positive effects}—facts that become true after execution;
        \item $\text{eff}^{-}(a) \subseteq S$ are the \emph{negative effects}—facts that become false after execution.
    \end{itemize}
    % Do that
    [Check math and maybe add some references]

    \begin{definition}[Transition Function]
        \label{def:transition}
        A \emph{transition function} $T: S \times A \to S$ maps a current state and an action to a resulting state. For an action $a$ and state $s$, $T(s, a) = s'$ describes how the environment evolves when action $a$ is executed in state $s$.
    \end{definition}

    The transition function for an action is thus: if $\text{pre}(a) \subseteq s$, then $T(s, a) = (s \setminus \text{eff}^{-}(a)) \cup \text{eff}^{+}(a)$, producing a new state $s'$ that reflects the additions and deletions induced by the action.

    When the preconditions of an action are satisfied, the action is said to be \emph{applicable} in the current state. Executing an applicable action transitions the system via the transition function $T$: $T(s, a) = s'$, where $s$ is the current state and $s'$ is the resulting state after the action's effects are applied.
\end{definition}

\begin{definition}[Classical Planning Problem]
    \label{def:planning-problem}
    A \emph{classical planning problem} is a tuple $\langle S, A, T, I, G \rangle$:
    \begin{itemize}
        \item $S$ is a finite set of states;
        \item $A$ is a finite set of actions;
        \item $T: S \times A \to S$ is a deterministic transition function mapping a state and action to a next state;
        \item $I \subseteq S$ is a non-empty set of initial states;
        \item $G \subseteq S$ is a non-empty set of goal states (any state in $G$ is an acceptable outcome).
    \end{itemize}
\end{definition}

\begin{definition}[Plan, Validity, and Solution]
    \label{def:plan}
    A \emph{plan} is a finite sequence of actions $\pi = \langle a_1, \ldots, a_n \rangle$. For an initial state $s_0 \in I$, the \emph{execution} of $\pi$ from $s_0$ is defined recursively: let $s_0' = s_0$, and for $i = 1, \ldots, n$, let $s_i' = T(s_{i-1}', a_i)$.

    The plan $\pi$ is \emph{valid} for a planning problem $\langle S, A, T, I, G \rangle$ (from Definition \ref{def:planning-problem}) if, for every initial state $s_0 \in I$ % Dose this make sense?
    , the execution of $\pi$ reaches a goal state, i.e., $s_n' \in G$. A valid plan is called a \emph{solution} to the planning problem.
\end{definition}

\paragraph{Classical Planning Assumptions}
The formulation above relies on standard assumptions commonly adopted in classical planning:
\begin{itemize}
    \item \textbf{Determinism}: actions have deterministic outcomes;
    \item \textbf{Full observability}: the planner has complete knowledge of the current state;
    \item \textbf{Instantaneous actions}: actions do not consume time;
    \item \textbf{Single-agent setting}: interactions with other agents are not considered.
\end{itemize}
These assumptions enable precise reasoning about plan validity and correctness. Relaxing them leads to more expressive planning models but can lead to fewer logical guarantees; some of these are discussed later in this thesis.

\begin{definition}[Planner]
    \label{def:planner}
    A \emph{planner} is an algorithm that, given a classical planning problem $\langle S, A, T, I, G \rangle$, searches for a valid plan $\pi$ (if one exists) that transforms an initial state $I$ into a state satisfying the goal condition $G$ by applying actions whose preconditions are satisfied and whose effects induce state transitions \cite{russell2010aima,ghallab2004automated}.

    Planners explore the induced state space using different search strategies, including breadth-first search, depth-first search, and heuristic-guided search, and may optimize for criteria such as plan length, action cost, or makespan \cite{ghallab2004automated}. Classical planning systems include STRIPS \cite{fikes1971strips}, GraphPlan \cite{blum1997graphplan}, and modern heuristic planners such as Fast Downward \cite{helmert2006fast}.
\end{definition}

% Okey in thise example we define the actions but the dont define the precodition and effect, is it correct or do we have some lacking in eather the example of the definition? I think like we mixing two deffinition of the planning above the action should not have preconditions and effects. (we have the transition function for that precondtion and effect are for STRIP language) 
\begin{example}[Simple Classical Planning Problem]
    \label{ex:simple-planning}
    Consider a minimal instance for Isabella's routine.

    \textbf{States}: Isabella can be in two locations: her apartment or Hobbs Cafe. The state set is:
    \[
        S = \{\texttt{at-apartment}, \texttt{at-cafe}\}.
    \]

    \textbf{Actions}: Isabella can move between locations:
    \[
        A = \{\texttt{walk-to-cafe}, \texttt{walk-to-apartment}\}.
    \]

    \textbf{Transition function}: The transition function $T$ encodes how each action changes the state. For example:
    \begin{itemize}
        \item $T(\texttt{at-apartment}, \texttt{walk-to-cafe}) = \texttt{at-cafe}$;
        \item $T(\texttt{at-cafe}, \texttt{walk-to-apartment}) = \texttt{at-apartment}$;
        \item $T(\texttt{at-apartment}, \texttt{walk-to-apartment}) = \texttt{at-apartment}$ (no change);
    \end{itemize}

    \textbf{Initial state}: Isabella begins at her apartment:
    \[
        I = \{\texttt{at-apartment}\}.
    \]

    \textbf{Goal state}: Isabella wants to be at the cafe to open it:
    \[
        G = \{\texttt{at-cafe}\}.
    \]

    \textbf{Solution}: The plan $\pi = \langle \texttt{walk-to-cafe} \rangle$ is valid:
    \begin{itemize}
        \item Start at $s_0' = \texttt{at-apartment} \in I$.
        \item Execute action $\texttt{walk-to-cafe}$:
              \[
                  s_1' = T(\texttt{at-apartment}, \texttt{walk-to-cafe}) = \texttt{at-cafe}.
              \]
        \item End state: $s_1' \in G$.
    \end{itemize}

    This minimal problem has a unique solution; extending it with additional actions (like \texttt{unlock-door}) would yield more complex planning instances.
\end{example}

\subsubsection{Hierarchical Planning}

Hierarchical task networks (HTN) represent an extension of classical planning that organizes planning as a multi-level process. Rather than finding a sequence of primitive actions directly from a goal (as classical planners do), HTN planning starts with high-level tasks and repeatedly decomposes them into simpler subtasks using predefined decomposition methods, until primitive actions are reached \cite{erolUMCPSoundComplete, nauSHOP2HTNPlanning2003}. This hierarchical structure enables abstraction (breaking complex goals into manageable pieces), reuse (sharing decomposition strategies across problems), and more efficient search through the planning space.

\begin{remark}[Task and Action Terminology]
    \label{rem:task-action-terminology}
    Throughout this thesis, we distinguish between two hierarchical levels of planning behavior. A \emph{task} is a high-level, daily-planned schedule unit (e.g., ``work café shift 09:00--12:00''), as synthesized in the Generative Agents architecture. An \emph{action}, in the formal planning sense above, is a primitive discrete event characterized by preconditions and effects. In the context of neuro-symbolic scaffolding, a task is decomposed into a sequence of fine-grained actions that, when executed, fulfill the task's objective. The action definition here (Definition \ref{def:action}) corresponds to the primitive action units into which tasks are decomposed in our framework.
\end{remark}

Park et al.'s agents generate daily plans with morning, afternoon, and evening blocks containing embedded tasks, resembling HTN decomposition without explicit HTN semantics or validation \cite{parkGenerativeAgentsInteractive2023a}.

Informally, we envision generating high-level tasks from high-level goals and then decomposing them into sequences of primitive actions.
In our running example, the task ``prepare café for 09:00 opening'' might decompose into a sequence of dependent actions: ``walk to cafe,'' ``unlock cafe door,'' ``start coffee service,'' and ``stock pastry cabinet.'' Each action must be executed in order, as later actions depend on earlier ones (e.g., the coffee service cannot be started before the door is unlocked and Isabella is inside).

\subsection{Symbolic Planning}
\label{subsec:symbolic-planning}
Symbolic planning relies on formal languages to model domains and problems, enabling algorithms to search for guaranteed valid solutions. In this work, we utilize the standard Planning Domain Definition Language (PDDL) \cite{mcdermottPDDLPlanningDomain1998} to represent the constraints and rules of the agent's environment, ensuring that generated behavior adheres to logical strictures. Key strengths of this approach include:

\begin{enumerate}
    \item \textbf{Explainability}: Plans are sequences of named actions with explicit preconditions and effects, enabling causal trace inspection.
    \item \textbf{Constraint enforcement}: Planners guarantee that plans satisfy all preconditions, avoid violated invariants, and respect temporal and resource bounds.
    \item \textbf{Optimality}: Many planners find optimal or bounded-suboptimal solutions under well-defined cost models.
\end{enumerate}

These properties directly address coherence: if behavior is synthesized via a symbolic planner, environmental constraints are satisfied by construction.

\subsubsection{PDDL: Planning Domain Definition Language}

To enable symbolic planning, domains and problems must be specified formally. The Planning Domain Definition Language (PDDL) is the standard representation used by most automated planners \cite{mcdermottPDDLPlanningDomain1998}.

\begin{definition}[PDDL Domain and Problem]
    \label{def:pddl}
    A PDDL \emph{domain} defines the invariant laws of the environment, consisting of:
    \begin{itemize}
        \item \textbf{Types}: A hierarchy of object classes (e.g., \texttt{agent}, \texttt{location}, \texttt{item}).
        \item \textbf{Predicates}: Atomic properties denoted $p(x_1, \ldots, x_k)$ that describe facts about objects. These can be $n$-ary relations (e.g., \texttt{(at ?a - agent ?l - location)}) or zero-arity propositions (e.g., \texttt{(shop-open)}).
        \item \textbf{Action Schemas}: Parameterized operators with preconditions and effects.
    \end{itemize}

    A PDDL \emph{problem} defines a specific instance:
    \begin{itemize}
        \item \textbf{Objects}: The specific entities in the world (e.g., \texttt{isabella}, \texttt{kitchen}).
        \item \textbf{Initial State}: A list of predicates true at $t=0$.
        \item \textbf{Goal}: A logical formula composed of predicates that must be satisfied.
    \end{itemize}

    Together, a domain and problem define a planning problem in the sense of Definition \ref{def:planning-problem}, which can then be solved by a planner (Definition \ref{def:planner}), such as Fast Downward, GraphPlan, or STRIPS.
\end{definition}

\begin{example}[PDDL Domain Fragment]
    \label{ex:pddl-fragment}
    A simplified PDDL domain for Isabella might include predicates for location and inventory, and an action schema for movement:
    \begin{small}
        \begin{verbatim}
    (define (domain isabella-world)
      (:predicates (at ?p - person ?l - location)
                   (has ?p - person ?i - item))

      (:action move
        :parameters (?p - person ?from ?to - location)
        :precondition (at ?p ?from)
        :effect (and (not (at ?p ?from)) (at ?p ?to)))
    )
    \end{verbatim}
    \end{small}
\end{example}

PDDL is valuable because it is explicit and verifiable: once a domain and problem are specified, standard planning algorithms can check that action preconditions are met, goals are reachable, and no contradictions are present in the plan. Prior work demonstrates that large language models can translate natural-language task and domain descriptions into a PDDL domain and problem instances, which can be solved reliably when embedded within neuro-symbolic pipelines that enforce syntactic and semantic validity through external planners and validators \cite{silver2024generalized, kwonFastAccurateTask}.

While newer LLMs exhibit improved reasoning and code generation capabilities, existing evidence suggests that reliability in symbolic planning arises primarily from scaffolding mechanisms—such as validation, decomposition, and repair—rather than from model scale alone \cite{zhang2025recursive}.
Consequently, improvements in underlying LLMs are best viewed as enhancing the effectiveness of such pipelines, rather than eliminating the need for formal correctness guarantees. This motivation directly underlies the neuro-symbolic approach: leverage the LLM's language understanding to generate initial PDDL schemas from natural language descriptions, then validate and refine those schemas using automated planning tools or constraint verification.

\paragraph{Limitations}
The primary limitation is \textbf{authoring cost}: PDDL domains require manual specification of predicates, actions, preconditions, and effects, which is brittle and labor-intensive for open-ended environments. Symbolic planning also struggles with commonsense reasoning and social nuance unless explicitly encoded \cite{haslumIntroductionPlanningDomain2019}.

\subsubsection{Plan Validation and the VAL Tool}

% Bellow could be rewrited to be more true and less halucynated
A critical challenge across all planning approaches is verification: how do we confirm that a proposed plan is logically correct before execution? While symbolic planners guarantee correctness by construction (valid plans are synthesized directly), plans from other sources—whether hierarchically decomposed, manually authored, or generated by other means—require post-hoc verification against the formal domain specification.

\textbf{VAL} (the Plan Validator) is a domain-independent tool that checks whether a plan is valid with respect to a PDDL domain and problem specification \cite{howey2004val}. Given a domain, problem, and candidate plan, VAL simulates the plan's execution and verifies:

\begin{itemize}
    \item \textbf{Action applicability}: each action's preconditions are satisfied before execution;
    \item \textbf{State transitions}: effects are correctly applied, yielding well-defined successor states;
    \item \textbf{Goal achievement}: the final state satisfies the problem goal.
\end{itemize}

If any check fails, VAL reports the plan as invalid with diagnostic details, such as which precondition was unsatisfied or at what step the plan diverged from expectations.

VAL has served as the standard validation tool for the International Planning Competition (IPC) since 2002, establishing it as a rigorous benchmark for plan correctness \cite{long20033rd}. Operationally, it follows the formal semantics of PDDL, making it a trusted oracle for correctness.

\begin{example}[VAL Validation in Isabella's Domain]
    \label{ex:val-student}
    Consider the task ``Open Hobbs Cafe,'' decomposed by the LLM into: \texttt{(walk-to-cafe)}, \texttt{(unlock-cafe-door)}, \texttt{(start-coffee-service)}.

    VAL checks preconditions in sequence:
    \begin{itemize}
        \item \texttt{(walk-to-cafe)}: precondition \texttt{(at isabella apartment)} is true in the initial state. Action succeeds; Isabella is now at the cafe.
        \item \texttt{(unlock-cafe-door)}: precondition \texttt{(has-key isabella cafe-key)} is checked. If Isabella does not possess the key in the initial state, VAL reports: ``precondition unsatisfied; plan invalid.''
    \end{itemize}

    The LLM's decomposition is rejected, and feedback is provided so the LLM can refine the plan (e.g., retrieve the key before attempting to unlock).
\end{example}

\paragraph{Limitations}
VAL only validates against the given PDDL model. If the model is incomplete or misspecified, a plan may pass validation despite being problematic in practice. Additionally, VAL does not measure plan quality, plausibility, or believability—it enforces logical correctness only.

\subsection{Agents and Believability}
\label{subsec:agents-believability}

\begin{definition}[Agent]
    [consider if we ever use this, if not delete]
    \label{def:agent}
    An \emph{agent} receives percepts and produces actions via sensors and actuators, implementing a mapping $f: P^{*} \to A$ from percept histories $P^{*}$ to actions $A$ \cite{russellArtificialIntelligenceModern2022}.
\end{definition}

\begin{definition}[Generative Agent]
    [consider if we ever use this, if not delete]

    \label{def:generative-agent}
    A \emph{generative agent} is a computational software agent that simulates believable human behavior by leveraging large language models to generate action sequences, maintain memory of past experiences, reflect on accumulated observations, and plan future behavior based on goals and social context.
\end{definition}

In simulated environments and games, non-player characters (NPCs) prioritize producing behavior that human observers find plausible, consistent, and engaging rather than maximizing numerical rewards \cite{mateasOzCentricReviewInteractive1999, loyallBelievableAgentsBuilding}. As a result, the evaluation of such agents commonly emphasizes observer perception over internal optimality or correctness.

\begin{definition}[Believability]
    \label{def:believability}
    \emph{Believability}---the ``illusion of life''---is the human-perceived realism of agent behavior: whether actions align with character goals, personality, knowledge, and social norms \cite{batesRoleEmotionBelievable1994, loyallBelievableAgentsBuilding}.
\end{definition}

Importantly, believability differs from psychological (emotional and mental states) or cognitive (thinking and reasoning) realism. Following Bates' analysis of character animation, believability permits suspension of disbelief—the observer accepts the character as having intentions and coherence—rather than requiring fidelity to actual human cognition (how people really think) or motivation (why they act) \cite{batesRoleEmotionBelievable1994}. An animated character can be believable despite vastly simplified internal mechanisms.
In the case of NPC agents simulating humans in daily routines, there is natural alignment between believability and realism: Isabellas behaviors (associating with customers, managing inventory, opening the Cafe) are grounded in observable, familiar reality. However, this alignment is contingent on context; the goal remains observer acceptance and perceived intentionality, not psychological accuracy. If an agent's actions respect temporal and causal constraints while being consistent with its apparent goals and knowledge, observers will find it believable, even if its internal reasoning mechanisms are simplified or heuristic.

Constraining agents with realistic physical and environmental limits increases perceived believability by reducing violations known to break immersion, such as impossible actions or logically inconsistent plans \cite{batesRoleEmotionBelievable1994, bogdanovychWhatMakesVirtual2016}. Park et al.\ showed that LLM-driven agents with memory, reflection, and planning were rated more believable than human crowdworkers role-playing agents in controlled evaluations. The crowdworker baseline was given access to each agent's full memory stream and simulated behavior, yet LLM-driven agents still significantly outperformed this human-authored comparison \cite{parkGenerativeAgentsInteractive2023a}. Xiao et al.\ propose metrics such as Consistency and Robustness for agent simulations based on character profiles \cite{xiaoHowFarAre2024}.

\begin{definition}[Coherence]
    \label{def:coherence}
    In the context of agent behavior, \emph{coherence} is the degree to which an action sequence respects domain constraints: preconditions are satisfied before each action, temporal and resource constraints are not violated, and the sequence does not contradict environmental invariants or prior commitments \cite{youngOverviewMimesisArchitecture, riedlNarrativePlanningBalancing2010}. Formally, a sequence of actions $\pi = \langle a_1, \ldots, a_n \rangle$ is \emph{coherent with respect to a planning problem} $\langle S, A, T, I, G \rangle$ (from \cref{def:planning-problem}) if executing $\pi$ from the initial state(s) $I$ via the transition function $T$ neither violates action preconditions nor environmental invariants. In other words, a coherent action sequence corresponds to a valid plan
    (though not necessarily an optimal one, and independent of whether it reaches the goal state $G$). % I don't understed this sentence
\end{definition}

Operationally, coherence is enforced through constraint adherence which for us will be solely satisfaction of action preconditions. While believability literature uses ``coherence'' informally to denote plausible behavior, in this thesis we use it in the stricter sense of constraint satisfaction validated by symbolic validation tools.

\begin{example}[Coherence Violation]
    \label{ex:coherence-violation}
    In our running example (\cref{ex:running-student}), a coherent agent must not execute actions for which preconditions are violated. A purely LLM-based planner might generate the action \texttt{make-toast} while the agent is still in the \texttt{bedroom}, violating the location precondition (must be in \texttt{kitchen}). Similarly, it might schedule \texttt{unlock-cafe} without a preceding \texttt{retrieve-keys} action, violating the inventory precondition.
\end{example}


We adopt the NPC perspective in which believability is the primary objective. While humans remain believable despite occasional planning mistakes, we hypothesize that in LLM-driven agents, *eliminating systematic incoherence*---via symbolic validation---meaningfully improves believability by reducing violations that are immediately recognizable as incoherent to observers.

% Maybe repetition? quite long as well
For a character portrayed as cognitively sound, coherence boundaries are defined by what is physically or logically impossible versus what is merely implausible. A character has no reasonable justification for certain errors: cooking a meal before purchasing ingredients, or unlocking a door without possessing the key are physically or logically impossible actions. In contrast, more subtle mistakes (forgetting a prior commitment, miscalculating available time, submitting work that was not fully completed) are believable because they are mistakes a person of sound mind might plausibly make. These latter errors fall into a gray zone that observers can forgive or attribute to understandable human fallibility. However, in our system design, we choose not to tolerate plans that violate constraint satisfaction, even if such violations might be forgivable in certain contexts. Since our simulations span only one day, avoiding a single reality-breaking error that violates physical or logical laws is a worthwhile trade-off for preserving immersion, even if it means erring on the side of caution compared to human behavior. This strict interpretation of coherence is thus a pragmatic choice for our scope, and exploring the optimal balance between logical correctness and forgivable mistakes across longer simulations remains a direction for further work.


\subsection{LLM-Based Planning}
\label{subsec:llm-planning}

To understand how LLMs can be applied to planning, we first establish what Large Language Models are, their capabilities, and their limitations.

\subsubsection{Large Language Models (LLMs)}
\label{subsubsec:llm}

Large language models are neural networks trained on vast text corpora to generate text one word (or token) at a time. Given a prompt, an LLM predicts the most likely next word based on patterns learned during training. It repeats this process iteratively to produce multi-sentence responses, adapting its output to the context of the prompt and any specified instructions.

\begin{example}[LLM Input and Output]
    \label{ex:llm-example}
    \textbf{Input prompt}: ``Isabella woke up late and missed the morning delivery of fresh pastries for Hobbs Cafe. How might she feel?''

    \textbf{LLM output}: ``Isabella likely feels stressed and anxious. She prides herself on running a smooth operation, and missing the delivery means she might not have enough inventory for the morning rush. She might worry about disappointing her regular customers and hastily try to bake backups or contact another supplier.''

    The LLM generates this response through learned statistical associations. It produces plausible, fluent text by predicting probable next tokens based on the persona context.
\end{example}

Key capabilities of LLMs relevant to this work:
\begin{itemize}
    \item \textbf{Instruction following}: Responding appropriately to natural language directives (e.g., ``format this as a list'', ``explain in simple terms'').
    \item \textbf{Few-shot learning}: Improving performance when given a few examples before a task.
    \item \textbf{Structured output generation}: Producing formal notations (code, logical rules, task lists) when prompted. Mechanisms such as OpenAI's Structured Outputs \cite{StructuredModelOutputs} or open-source libraries like XGrammar enforce schema adherence during token generation (constrained decoding), guaranteeing that the output matches a predefined JSON schema or grammar.
\end{itemize}

However, LLMs have critical limitations:
\begin{itemize}
    \item \textbf{No logical guarantees}: They perform statistical inference, not deductive reasoning. An LLM can produce plausible-sounding but logically inconsistent outputs\cite{bender2021dangers,valmeekam2024llms}.
    \item \textbf{Context window limits}: They cannot retain information from arbitrarily long histories by design\cite{vaswani2017attention}; older context may be lost or forgotten.
    \item \textbf{Hallucination}: They may invent facts, actions, or constraints not grounded in the actual domain or agent memory \cite{huang2025survey,ji2023survey}.
\end{itemize}

To address these limitations, we adopt \emph{scaffolding}:

\begin{definition}[Scaffolding]
    \label{def:scaffolding}
    \emph{Scaffolding} refers to external processes or structures that support a reasoning system in accomplishing tasks beyond its unaided capabilities, by constraining, guiding, or validating its intermediate outputs, without modifying the underlying model \cite{wood1976scaffolding}.
\end{definition}

In the context of LLM-based agents, scaffolding can take the form of translating natural language outputs into formal representations, validating candidate actions against explicit constraints, and iteratively refining solutions using external feedback. These mechanisms directly mitigate common failure modes of LLM planning, including logical inconsistency, hallucination, and limited context sensitivity.

In this thesis, scaffolding is employed to support the generation of formal planning specifications and the decomposition of high-level goals into executable action sequences—domains where LLM fluency and commonsense reasoning are beneficial, but where formal correctness and consistency are required.




\subsection{Neuro-Symbolic Systems for Planning}
\label{subsec:neuro-symbolic}

Neuro-symbolic systems combine learned (sub-symbolic) components with symbolic representations and reasoning to achieve both flexibility and guarantees \cite{garcezNeuralSymbolicComputingEffective2019, garcezNeurosymbolicAI3rd2020}. The central idea is to leverage the strengths of each paradigm: LLMs excel at natural-language understanding, commonsense reasoning, and flexible task decomposition, while symbolic systems enforce logical consistency and provide formal guarantees.

LLMs approximate hierarchical planning by proposing outlines, subgoals, and steps in natural language \cite{weiChainofThoughtPromptingElicits2023}.


Recent work explores these patterns for PDDL generation. Tantakoun et al. survey approaches where LLMs construct PDDL domain and problem files from natural language, with symbolic planners validating and executing \cite{tantakounLLMsPlanningModelers2025}. Huang et al. propose a pipeline in which multiple LLM instances generate diverse candidate PDDL action schemas, which are filtered for semantic coherence and validated by a symbolic planner to ensure domain solvability \cite{huangPlanningDarkLLMSymbolic2024}. \footnote{Unlike the present work, Huang et al.\ use the symbolic planner not only for validation but also to generate complete plans once a consistent domain schema has been identified.}

Our approach combines multiple integration patterns: an LLM proposes a decomposition of a high-level task into primitive actions; a symbolic validator (such as VAL) then checks whether those actions respect domain constraints, enforcing the concept of \emph{coherence} defined in Section \ref{subsec:agents-believability}.
% think about this example
\begin{example}[Neuro-Symbolic Validation of Action Decomposition]
    \label{ex:neurosymbolic-validation}
    The LLM receives a daily \emph{task} ``Decorate cafe for Valentine's Day (17:00 to 19:00)'' and decomposes it into fine-grained \emph{actions}: ``walk to cafe,'' ``unlock door,'' ``retrieve decorations,'' ``hang banners.'' The PDDL validator (VAL) checks each \emph{action}:
    \begin{itemize}
        \item Preconditions (must be at cafe to hang banners; must have decorations in inventory),
        \item Location constraints (cannot hang banners if at the market),
        \item Temporal ordering (cannot hang banners before retrieving them).
    \end{itemize}
    Violations are flagged with diagnostics (e.g., ``action \texttt{hang\_banners} fails: precondition \texttt{(has-item isabella banners)} unsatisfied; inventory empty''). The rejected decomposition is returned to the LLM with feedback for refinement.
\end{example}

\section{Literature Review}

This section reviews work that informed the implementation of the \textit{scaffold} in this thesis, focusing on the Generative Agents paper that motivates our system and hybrid approaches combining LLMs with symbolic planning.

\subsection{Generative Agents: Interactive Simulacra of Human Behavior (Park et al., 2023)}

Park et al.\ introduced generative agents: LLM-driven NPCs simulating believable human behavior in Smallville, a sandbox town environment \cite{parkGenerativeAgentsInteractive2023a}.

\begin{figure}[H]\centering
    \includegraphics[width=0.8\linewidth]{Pictures/GenerativeAgentsSims.jpg}
    \caption{Smallville environment (adapted from Park et al.\ \cite{parkGenerativeAgentsInteractive2023a}). A sandbox environment populated with generative agents where agents autonomously plan daily activities, exchange information, form relationships, and coordinate group events with minimal user intervention.}\label{fig:generative-agents-sims}
\end{figure}

The architecture comprises:

\begin{enumerate}
    \item \textbf{Memory Stream}: Time-stamped observations (own actions, others' actions, environment events), retrieved by weighted combination of recency (exponential decay), importance (LLM-rated 1 to 10), and relevance (embedding cosine similarity).
    \item \textbf{Reflection}: Periodic synthesis of experiences into higher-level insights. The LLM generates questions about recent observations, produces abstract insights with citations to source memories, and organizes these into reflection trees (observations → reflections → meta-reflections).
    \item \textbf{Planning}:
          \begin{enumerate}
              \item Daily objectives are synthesized from the agent's personality profile (innate traits, occupation, background), current situational goals (e.g., organizing a party, meeting deadlines), and established lifestyle patterns (sleep schedule, work hours, meal times, exercise routines).
              \item These objectives are then decomposed hierarchically: day-level objectives are refined into hour-level plan segments, which are further decomposed into concrete 5-to-15 minute \emph{action} sequences at execution time as each hour-level \emph{task} becomes due.
              \item Agents dynamically re-plan when circumstances change, with the LLM deciding whether to continue the current plan or react to new observations.
          \end{enumerate}
\end{enumerate}

\begin{figure}[H]\centering
    \includegraphics[width=0.8\linewidth]{Pictures/GenerativeAgents-System.jpg}
    \caption{Generative agent architecture (adapted from Park et al.\ \cite{parkGenerativeAgentsInteractive2023a}). Agents perceive their environment, and all perceptions are saved in a comprehensive record of the agent's experiences called the memory stream. Based on their perceptions, the architecture retrieves relevant memories and uses those retrieved actions to determine an action. These retrieved memories are also used to form longer-term plans and create higher-level reflections, both of which are entered into the memory stream for future use.}\label{fig:generative-agents-architecture}
\end{figure}

\textbf{Emergent behaviors} were observed in a 25-agent, two-day simulation where agents maintained coherent behavior. Park et al.\ demonstrated that LLM agents with memory, reflection, and planning achieve high believability. For example, information about an event started by one of the agents spread to 13 other agents (52\%), and notably, 5 of the 12 explicitly invited agents attended at the correct time and location.

\textbf{Evaluation} comprised a controlled study with 100 human participants recruited via an online crowdsourcing platform, who ranked agent responses using a TrueSkill-based aggregation. A human-authored baseline was constructed by recruiting 25 crowdworkers, each assigned to a single agent; these workers were given access to the agent's full memory stream and a replay of its simulated behavior, and were instructed to role-play the agent when answering interview questions. Baseline responses were manually inspected by the authors for coherence and consistency with the assigned persona, with low-quality or out-of-character submissions discarded and replaced. Evaluators were blind to condition and ranked responses from the full architecture, multiple ablations, and the human baseline. The full architecture (memory + reflection + planning) achieved $\mu = 29.89, \sigma = 0.72$, significantly outperforming both ablations and the human-authored baseline ($\mu = 22.95, \sigma = 0.69$), with effect size $d = 8.16$ ($p < 0.001$).
Interview questions assessed self-knowledge, memory recall, future plans, reactive decisions, and reflections of the agent.

\begin{figure}[H]\centering
    \includegraphics[width=0.8\linewidth]{Pictures/GenerativeAgents-Results.jpg}
    \caption{Evaluation results (adapted from Park et al.\ \cite{parkGenerativeAgentsInteractive2023a}). The full generative agent architecture produces more believable behavior than the ablated architectures and the human crowdworkers. Each additional ablation reduces the performance of the architecture.}\label{fig:generative-agents-results}
\end{figure}

Notably, the crowdworker baseline ($\mu = 22.95$) scored lower than even the ablated condition without reflection and planning ($\mu = 25.64$), despite being given access to full memory streams and past behavior with manual quality curation. While we acknowledge Park et al.'s status as a highly influential paper (extensively cited across agent and LLM research), we lack access to open peer reviews that might contextualize this result. Absent such public critique, we accept the findings at face value, though the baseline performance invites questions about evaluation setup fairness—particularly whether crowdworkers role-playing from a past record could adequately capture how agents make decisions in real time.

However, this believability came at a cost. \textbf{Failure modes} included memory retrieval errors, hallucinations, and over-cooperation. Specifically, action generation lacked precondition validation; coherence relied on the LLM's implicit understanding during generation, with no explicit verification against domain constraints.

This limitation motivates our approach: we propose integrating PDDL-based symbolic validation to *enforce* constraint satisfaction and detect incoherent plans before execution, preserving the expressiveness of LLM-driven planning while eliminating logical inconsistencies that undermine believability.

\subsection{LLMs as Planning Modelers (Tantakoun et al., 2025)}

Tantakoun et al.\ survey how LLMs construct and refine automated planning models rather than directly perform planning \cite{tantakounLLMsPlanningModelers2025}. They review approximately 80 papers, positioning LLMs as tools for extracting planning models to support reliable planners, addressing the limitation that LLMs struggle with long-horizon planning requiring structured reasoning.

\textbf{Three planning paradigms} structure the survey:
\begin{enumerate}
    \item \textbf{LLMs-as-Heuristics}: Enhance planner search efficiency.
    \item \textbf{LLMs-as-Planners}: Directly generate action sequences or plan proposals.
    \item \textbf{LLMs-as-Modelers}: Construct PDDL domain and problem files (survey focus).
\end{enumerate}

Within LLMs-as-Modelers:

\textbf{Task Modeling} (approximately 30 papers): LLMs generate PDDL problem files from goal specifications, using few-shot prompting \cite{collinsGroundingLanguageGeneralized2022} to chain-of-thought techniques \cite{lyuFaithfulChainofThoughtReasoning2023}, which encourage the LLM to decompose reasoning into intermediate steps before generating output. Well-explored but relies on detailed predicate specification.

\textbf{Domain Modeling} (approximately 15 papers): LLMs generate PDDL domain files (predicates and action schemas), more challenging than task specification. Approaches include generate-test-critique loops incrementally building components \cite{guanLeveragingPretrainedLarge2023}. Single-domain generation risks misalignment with human expectations due to natural language ambiguity.

% HMM wE basicly did that Haha
\textbf{Hybrid Modeling} (approximately 15 papers): LLMs generate both domain and problem files. Systems use human-in-the-loop approaches with anomaly detection \cite{ye2024domainindependent}. Coordinating both introduces complexity; linear pipelines risk cascading errors.

\textbf{Key findings} from the survey show that:
\begin{enumerate}
    \item LLMs generate syntactically valid PDDL but struggle with semantic consistency.
    \item Iterative refinement with symbolic planner feedback improves model quality.
\end{enumerate}

% we also genererate the task, but later in the pipeline
This grounds our approach: we position the LLM as a PDDL schema generator (domain modeling) with symbolic validator feedback and iterative refinement.

\subsection{Other Neuro-Symbolic Planning Approaches}

Additional work combines LLMs with formal planning:

\textbf{Iterative planning with VAL}: ISR-LLM employs an iterative self-refinement process using a symbolic validator to improve long-horizon task planning \cite{zhouISRLLMIterativeSelfRefined2023}.

\textbf{Iterative refinement loops}: LLM+P and related frameworks prompt an LLM to propose high-level plans, check them with a PDDL planner, and iterate until valid \cite{silverGeneralizedPlanningLarge2023, valmeekamonPlanningLargeLanguage2023, lyuFaithfulChainofThoughtReasoning2023}. Critique-and-repair loops such as Reflexion and Tree-of-Thought add self-evaluation and search, while symbolic constraints prune or guide search \cite{shinn2023reflexion, yao2023treeofthoughts}.


\subsection{Summary of Insights and Research Focus}

The literature suggests three converging insights:

\begin{enumerate}
    \item \textbf{LLM planning produces human-like behavior but entails risks of logical inconsistency}, including hallucinations and violations of environmental constraints \cite{parkGenerativeAgentsInteractive2023a, xiaoHowFarAre2024}.
    \item \textbf{Neuro-symbolic methods offer structure and guarantees} by validating or synthesizing plans against explicit models of actions, time, and resources \cite{huangPlanningDarkLLMSymbolic2024, tantakounLLMsPlanningModelers2025}.
    \item \textbf{Believability and coherence should be assessed jointly}: constraint adherence is necessary for plausibility (coherence), but human evaluation is required to confirm realism (believability) \cite{parkGenerativeAgentsInteractive2023a, xiaoHowFarAre2024}.
\end{enumerate}

