\chapter{Methodology}

\section{Experimental Setup}

Details of the experimental setup.

\section{System Design}

Description of the system architecture and design.

\section{Quantitative Evaluation — Validator-based (summary)}
\label{sec:quant-eval-summary}

We complement the human study with a concise, automated evaluation that compares a GA-like baseline against our validator-augmented system. The validator flags constraint violations; we optionally allow a small number of validator-guided revision rounds~$R$ and observe how violations change. Validation is separated into day-level (schedule) and task-level (within-task) checks.

\paragraph{Conditions} Baseline (no repair) vs. ours with small~$R$ (iterative critique-and-repair).

\paragraph{Scenarios/protocol} Matched scenarios with the same initial states and settings across conditions; for each run: generate plan~$\rightarrow$ validate~$\rightarrow$ (optional) repair~$\rightarrow$ re-run from the same initial state; uniform logging.

\paragraph{Metrics (concise)} Counts of validator-detected violations at day-level and task-level, plus an overall aggregate; simple derived indicators (e.g., violation rate per 100 actions, zero-violation success, rounds-to-zero, coarse plan edit/repair magnitude).

\paragraph{Analysis (brief)} Compare conditions on distributions of violation counts/rates and trends across small~$R$ (e.g., paired nonparametric tests or simple count models), with minimal plots/tables. Report day-level and task-level summaries separately and in aggregate.

\paragraph{Reporting} Concise tables/figures highlighting main differences and a small script to re-run scenarios with the validator.

\section{User Study: Believability Evaluation}
\label{sec:user-study-believability}

This section describes the human-subjects study we designed to test whether our approach improves the perceived believability of agent behaviour compared to the baseline architecture introduced in Generative Agents \cite{parkGenerativeAgentsInteractive2023a}. We focus the evaluation on the believability of \emph{actions} rather than only on agent personalities or prompted conversations.

\subsection{Objectives and hypotheses}
\label{subsec:objectives-hypotheses}
We evaluate two primary hypotheses:
\begin{itemize}
	\item H1 (overall believability): Participants judge agents powered by our method as more believable overall than the baseline Generative Agents architecture in matched scenarios.
	\item H2 (action believability): For the same scenario, participants flag fewer actions as ``unbelievable'' in our method than in the baseline.
\end{itemize}
We also explore two secondary outcomes: (i) perceived causal coherence of behaviour when the high-level plan is visible, and (ii) free-text reasons participants provide when they deem an action unbelievable (used for qualitative error analysis) \cite{batesRoleEmotionBelievable1994,bogdanovychWhatMakesVirtual2016,tenceAutomatableEvaluationMethod2010,xiaoHowFarAre2024}.

\subsection{Conditions}
\label{subsec:conditions}
We compare two between-system conditions on the same simulated world and character seeds:
\begin{enumerate}
	\item \textbf{Baseline (GA)}: Our faithful re-implementation of Generative Agents \cite{parkGenerativeAgentsInteractive2023a}.
	\item \textbf{Ours (Neuro-symbolic)}: The proposed system with symbolic planning and consistency checks integrated into deliberation and action selection.
\end{enumerate}
Each participant evaluates both conditions on the same character and scenario to enable within-subject comparison. Order is counterbalanced (Latin square) to reduce presentation effects.

\subsection{Participants}
\label{subsec:participants}
Small-to-moderate within-subject sample recruited from the university community and online platforms; English proficiency required. We will run a short pilot to validate timing and the interface. All participants provide informed consent and can withdraw at any time without penalty.

\subsection{Materials and stimuli}
\label{subsec:materials}
For each condition, participants view a replay of an agent’s day in the sandbox world with: (i) controllable playback; (ii) an optional overlay for the high-level plan and action log; and (iii) simple controls to flag unbelievable actions and provide short reasons. Replays use matched scenarios and character profiles across conditions.

\subsection{Procedure}
\label{subsec:procedure}
Brief introduction and practice; then two matched conditions in counterbalanced order. Participants watch, flag unbelievable actions with optional short reasons, and provide summary ratings. We record simple interaction logs (e.g., time-on-task, overlay openings) for context.


\subsection{Measures}
\label{subsec:measures}
We combine participant-reported and behaviour-linked measures:
\begin{itemize}
	\item \textbf{Primary:} overall believability (Likert), action-level unbelievable rate (e.g., flags per 100 actions), and pairwise preference.
	\item \textbf{Secondary:} perceived causal coherence and plan adherence (Likert), plus brief coding of reasons for flagged actions (e.g., goal inconsistency, rule/time violations, social norms, control failures).
	\item \textbf{Context logs:} simple usage signals (e.g., order, time-on-task, overlay openings) for potential order/interface effects.
\end{itemize}

\subsection{Data quality and exclusion}
\label{subsec:quality}
Exclusions will be based on straightforward attention/completeness checks and implausibly fast completion; rules will be preregistered.

\subsection{Analysis}
\label{subsec:analysis}
Within-subject comparisons between conditions for the primary outcomes (e.g., paired tests or simple mixed models for event-level data). We report concise effect summaries and confidence intervals; qualitative reasons are coded to contextualize quantitative findings.

\subsection{Ethics}
\label{subsec:ethics}
The study involves only minimal risk. No personal data beyond demographics is collected; all logs are anonymized and stored on encrypted drives. We will seek approval from the institutional ethics board prior to recruitment.

\subsection{Power and timing}
\label{subsec:power}
We target a practical within-subject study size appropriate for detecting moderate differences and will confirm with a simple power check; a short pilot informs timing and minor interface tweaks.

\subsection{Preregistration and availability}
\label{subsec:prereg}
We will preregister hypotheses, exclusion rules, and primary/secondary outcomes, and release the anonymized dataset, analysis scripts, and the evaluation interface after publication.
