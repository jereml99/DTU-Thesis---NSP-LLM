# Chapter 2 – Theoretical Background

This chapter unifies the conceptual foundations and the most relevant prior work that motivate the design choices in this thesis. It first introduces the core notions—large language models, agents, and planning paradigms (symbolic, neural, and hierarchical)—and clarifies how they interact in neuro-symbolic systems. It then reviews research on LLM-driven generative agents, with an emphasis on the original “Generative Agents” paper [Park et al., 2023], and contrasts it with hybrid planning approaches that combine symbolic guarantees with neural flexibility. Throughout, a running example is used: a small-town non-player character (NPC) named Maya, a café barista who lives, works, and socializes in a simulated environment. The example illustrates how different planning formalisms and LLM capabilities bear on believability and coherence.

## 2.2 Core Concepts and Definitions

### 2.2.1 Large Language Models (LLMs)

Large language models are transformer-based sequence predictors that learn conditional distributions over tokens from large corpora [Vaswani et al., 2017; Brown et al., 2020]. Given a context window, the model estimates the next-token distribution and iteratively generates text. Self-attention enables the model to integrate information across the prompt, in-context examples, and tool-augmented inputs (e.g., retrieval), which in practice yields emergent abilities such as instruction following, few-shot generalization, and approximate reasoning. However, LLMs do not compute logical consequences with formal guarantees; they perform pattern-conditioned statistical inference that can be steered with prompting and scaffolding but remains non-deductive in nature [Wei et al., 2022; Kojima et al., 2022].

In this thesis, the LLM is treated as a neural reasoning module within a broader agent architecture rather than the agent itself. Concretely, it is responsible for tasks such as: (i) inferring goals from observations and social context, (ii) elaborating task decompositions into natural-language steps, and (iii) synthesizing utterances and reflections. The LLM’s strengths—world knowledge, pragmatic inference, and linguistic fluency—are leveraged where open-ended interpretation is needed. Its weaknesses—lack of hard constraint enforcement, occasional hallucination, and limited temporal precision—are mitigated by a symbolic validator (symbolic scaffolding) that checks proposals against an explicit model of states, actions, and time and reports diagnostics or suggested repairs [Shinn et al., 2023; Yao et al., 2023]. In the running example, the LLM can draft Maya’s day plan from a calendar and social cues, but the final schedule must respect café hours, shift constraints, and travel times; the validator will flag violations and indicate repair options.

### 2.2.2 Agents

Classical AI defines an agent as an entity that perceives its environment through sensors and acts upon it through actuators to maximize a performance measure [Russell and Norvig, 2021]. Architectures range from reactive agents to goal- and utility-based agents, with internal states (beliefs, desires, intentions) modeling information about the world and future courses of action. In simulated environments and games, non-player characters (NPCs) extend this notion: their primary purpose is not reward maximization but producing behavior that human observers find plausible, consistent, and engaging over time [Mateas, 2002; Loyall, 1997].

This thesis adopts the NPC perspective. Agents are situated characters whose core objective is believability—psychological plausibility, narrative consistency, and social coherence—rather than numerical reward. This reorientation has two implications. First, plans must satisfy constraints that humans expect (e.g., working shifts, social commitments, physical limitations), even if trivial variances in utility exist. Second, explainability matters: the agent’s choices should be understandable post hoc via causal and temporal rationales grounded in an explicit world model. The running example makes this concrete: Maya’s choices (covering a colleague’s shift, attending an evening art class, sending a message about a delayed delivery) should fit with her goals, knowledge, and the environment’s rules, even when improvised by an LLM.

### 2.2.3 Planning

A (classical) planning problem can be formalized as a tuple (S, A, T, I, G), where S is a set of states, A a set of actions, T a transition relation or function T: S × A → S, I ⊆ S the set of initial states, and G ⊆ S the set of goal states [Ghallab et al., 2004]. A plan is a finite sequence of actions π = ⟨a1, …, an⟩ such that executing π from any s ∈ I via T reaches some g ∈ G; a policy generalizes this to a mapping from states to actions. Preconditions and effects of actions implicitly define the causal structure by which actions change the world. Temporal planning enriches this with durations and temporal constraints; epistemic extensions reason about agents’ knowledge and information change [Bolander and Andersen, 2011].

For simulated agents, planning is a means to causal and temporal coherence. By requiring goal achievement via explicit preconditions, effects, and constraints, planning ensures that the agent’s behavior is not merely locally plausible but globally consistent with the environment over time. In our example, a plan that schedules Maya to open the café must first ensure that keys are possessed, that commute time is available, and that the espresso machine warm-up precedes serving customers. Plans produce traces whose causal explanations can be inspected, supporting both believability and transparency.

### 2.2.4 Symbolic Planning

Symbolic planning represents states as logical structures and actions as operators with preconditions and effects, enabling algorithmic search over an explicit, compositional model (e.g., STRIPS and PDDL) [Fikes and Nilsson, 1971; McDermott et al., 1998; Ghallab et al., 2004]. Its key strengths include: (i) explainability via declarative models and plan traces; (ii) reliable constraint handling (state invariants, resource bounds, mutual exclusions); and (iii) optimality or bounded-suboptimality under well-defined cost models. These properties are valuable for ensuring that NPC behavior respects narrative and physical constraints and that deviations can be diagnosed.

The limitations are complementary to LLM strengths. Symbolic planning presumes a closed-world model that is costly to author and brittle under open-ended, underspecified goals. It struggles with commonsense interpretation, social nuance, and incomplete or stochastic dynamics unless extended with belief, probabilistic, or temporal logics, which increase complexity [Kvarnström, 2011; Haslum et al., 2019]. In our setting, a purely symbolic approach would require hand-coding the café domain’s social norms and exceptions (e.g., covering a sick colleague) and still fail to improvise natural dialogue. Hence, symbolic planning is necessary but insufficient for believability on its own.

### 2.2.5 Neural Planning

Neural or LLM-based planning refers to using learned models to produce action sequences or subgoal decompositions directly from textual descriptions, scenes, or trajectories [Ahn et al., 2022; Huang et al., 2022]. LLMs excel at drafting plausible multi-step procedures (“chain-of-thought”), proposing alternatives, and adapting plans to soft preferences. Retrieval and tool-use can augment LLMs with up-to-date knowledge (e.g., schedules), while sampling and verification loops can improve robustness [Shinn et al., 2023; Yao et al., 2023].

The downside is the absence of formal guarantees. LLM-generated plans can omit preconditions, violate invariants, or drift temporally, especially when context windows truncate earlier commitments. Moreover, stochastic decoding yields non-determinism that complicates reproducibility and debugging. For believability-centric NPCs, these failure modes manifest as broken commitments (double-booked meetings), physical impossibilities (being in two places at once), or social incoherence (forgetting a promise). These gaps motivate a neuro-symbolic design where neural proposals are filtered or refined against a symbolic model.

### 2.2.6 Hierarchical Planning

Hierarchical task networks (HTN) decompose high-level tasks into ordered or partially ordered subtasks until primitive actions are reached, using methods that encode admissible refinements and constraints [Erol et al., 1994; Nau et al., 2003]. Hierarchy supports abstraction, reuse, and tractable search, and mirrors human planning practices (e.g., “prepare café for opening” → “arrive,” “unlock,” “start machines,” “set up pastries”). Temporal and resource constraints can be attached at different levels, yielding coherent schedules.

LLMs often approximate hierarchical planning implicitly by proposing outlines, subgoals, and steps in natural language [Wei et al., 2022]. Systems like “Generative Agents” prompt an LLM to produce daily plans that look hierarchical—morning, afternoon, evening blocks with embedded tasks—without an explicit HTN semantics [Park et al., 2023]. This produces understandable behavior but lacks machine-checkable guarantees. A symbolic HTN or PDDL+temporal planner can validate and schedule such decompositions, ensuring that high-level commitments (e.g., “cover afternoon shift”) are refined into feasible, non-overlapping actions. In this thesis, we make hierarchy explicit and begin by implementing a symbolic validator (symbolic scaffolding) that inspects LLM decompositions for feasibility and suggests targeted repairs; full planner-based synthesis remains a possible extension.

### 2.2.7 Neuro-Symbolic Systems (Definition)

Neuro-symbolic systems combine learned, sub-symbolic components with symbolic representations and reasoning to achieve both flexibility and guarantees [Garcez et al., 2015; d’Avila Garcez and Lamb, 2020]. Common integration patterns include: (i) neural-propose/symbolic-dispose, where an LLM proposes goals or plans and a symbolic component validates them; (ii) symbolic-in-the-loop, where constraints are injected into decoding or used to critique/repair candidates; and (iii) shared-world models, where a symbolic state is updated from neural perception and queried for decision making. The interface must specify data contracts: how tasks are typed, how constraints are represented, and how failures are explained.

In this work, the LLM supplies open-ended interpretation (goal inference, social reasoning, utterance generation) and produces hierarchical sketches, which are tree-structured proposals that name high-level intentions and recursively refine them into subtasks. These sketches can optionally annotate nodes with duration, resources, and informal preconditions. The validator inspects the sketches and reports diagnostics or suggested repairs. We focus first on a symbolic validator (symbolic scaffolding) that enforces hard constraints (resources, temporal relations, invariants) as checks, produces diagnostics and suggested repairs, and can emit traces suitable for inspection and replay. The result aims to preserve the believability afforded by neural fluency while adding the coherence and explainability provided by symbolic checks. For the running example, the neuro-symbolic layer ensures that Maya’s socially driven, LLM-generated intentions are realized only after passing validator checks or being repaired to satisfy café domain constraints.

## 2.3 Literature Review

This section reviews work relevant to neuro-symbolic planning for LLM-driven agents, focusing on the “Generative Agents” paper that our system extends, and situating it among hybrid approaches that combine LLMs with symbolic models. The lens is pragmatic: how to obtain believable, coherent NPC behavior that is both flexible and constrained.

### 2.3.1 Original Paper – Generative Agents: Interactive Simulacra of Human Behavior (Park et al., 2023)

Park et al. propose a social simulation in which LLM-driven agents inhabit a sandbox town, yielding emergent behaviors such as planning a Valentine’s Day party [Park et al., 2023]. The architecture centers on three components: Memory, Reflection, and Planning. A memory stream stores time-stamped observations (own actions, others’ actions, environment events), with retrieval based on a weighted mix of recency, importance, and relevance. Reflection summarizes salient patterns into “insights” that shape future behavior (e.g., “Maya values punctuality at work”). Planning prompts the LLM to generate daily schedules and update them as the day unfolds. Inter-agent interactions are mediated by natural language; the environment is updated from parsed action descriptions.

This design yields lifelike narratives with minimal hand-coded rules. However, it lacks symbolic grounding. There is no explicit model of time, resources, or invariants with which plans must comply; temporal coherence is maintained heuristically through textual schedules that can drift. Actions are not verified against preconditions and effects beyond ad hoc checks, limiting transparency and reproducibility. The memory-retrieval heuristic does not guarantee that prior commitments are respected when the context window shifts. These limitations motivate integrating a neuro-symbolic planning layer that (i) encodes domain rules explicitly, (ii) validates or repairs LLM-generated plans, and (iii) produces executable schedules and traces that can be audited.

### 2.3.2 Neuro-Symbolic Planning Approaches

Several lines of work combine LLMs with formal planning to balance commonsense flexibility and constraint satisfaction. In robotics, SayCan pairs an LLM’s language grounding with value estimates over affordances to select feasible actions [Ahn et al., 2022]. In task planning, LLM+P and related frameworks prompt an LLM to propose tasks or high-level plans that are checked or completed by a PDDL/HTN planner, iterating until a valid plan is found [Silver et al., 2023; Valmeekam et al., 2023; Lyu et al., 2023]. Critique-and-repair loops (e.g., Reflexion, Tree-of-Thought/Graph-of-Thought) add self-evaluation and search over candidate plans, while symbolic constraints prune or guide the search [Shinn et al., 2023; Yao et al., 2023]. Temporal planning integrations incorporate duration and resource checks to prevent overlaps and enforce deadlines [Cashmore and Fox, 2019].

Compared to Park et al., these hybrid methods assume an explicit domain model and delegate feasibility checking to a solver. They trade authoring cost for guarantees: once a café domain and calendar constraints are specified, any LLM proposal must pass validation, yielding consistent, executable traces. Conversely, pure LLM systems require little modeling but provide weaker assurances, especially over long horizons. This thesis follows the hybrid path but emphasizes a validator-first approach: the LLM generates goals and hierarchical sketches; a symbolic validator (symbolic scaffolding) checks constraints and suggests repairs; and feedback (failures, explanations) is fed back to the LLM to improve future proposals and achieve coherence.

### 2.3.3 Believability and Coherence in Agent Simulations

Believability is the human-perceived realism of an agent’s behavior—whether actions align with the character’s goals, personality, knowledge, and social norms [Loyall, 1997; Mateas, 2002]. Coherence is the causal and temporal consistency of behavior—whether actions are feasible, properly ordered, and do not contradict prior commitments [Young, 2001; Riedl and Young, 2010]. Prior work assesses these properties through (i) human evaluation (Likert ratings, pairwise preferences, Turing-style interviews), (ii) constraint adherence metrics (violations of invariants, deadlines, and resource limits), and (iii) narrative structure measures (event-graph consistency, unresolved causal links) [Swanson and Gordon, 2010; Ammanabrolu et al., 2022]. Epistemic coherence adds that the agent’s beliefs should be internally consistent and updated appropriately after observations, often modeled with epistemic planning [Bolander and Andersen, 2011].

For our evaluation, these perspectives are unified. The symbolic layer provides measurable proxies, which could be things like counts of violated constraints, temporal overlaps, plan repair frequency, and explanation coverage, while human studies capture perceived realism and social plausibility. Concretely, we compare a GA-like baseline against our validator-augmented system across $R$ revision rounds, tracking how violations decrease as repairs are applied. In the running example, we can quantify whether Maya double-books or violates café policies, and we can ask evaluators whether her behavior feels natural given her role and relationships. This dual lens acknowledges that believability is ultimately a human judgment, but coherence can be instrumented and improved through planning.

### 2.3.4 Summary of Insights and Research Focus

The literature suggests three converging insights. First, pure LLM planning produces impressively human-like behavior but lacks consistency over longer horizons due to missing symbolic grounding. Second, neuro-symbolic methods offer a principled way to inject structure and guarantees by validating or synthesizing plans against explicit models of actions, time, and resources. Third, believability and coherence should be assessed jointly: formal constraint adherence is necessary for plausibility, but human evaluation is required to confirm perceived realism.

Motivated by these insights, this thesis investigates a neuro-symbolic planning layer for LLM-driven NPCs. The research focus is to design interfaces whereby an LLM proposes goals and hierarchical sketches, a symbolic planner validates and schedules them, and explanations and failures loop back to improve future proposals. The aim is to enhance causal and temporal coherence without sacrificing the improvisational richness that makes LLM agents engaging, thereby improving overall believability in simulated social environments.
