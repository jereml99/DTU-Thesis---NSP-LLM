@article{batesRoleEmotionBelievable1994,
  title = {The Role of Emotion in Believable Agents},
  author = {Bates, Joseph},
  date = {1994-07},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {37},
  number = {7},
  pages = {122--125},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/176789.176803},
  url = {https://dl.acm.org/doi/10.1145/176789.176803},
  urldate = {2025-10-20},
  langid = {english},
  file = {C:\Users\ledwo\Zotero\storage\3SJL4EXE\Bates - 1994 - The role of emotion in believable agents.pdf}
}

@article{bogdanovychWhatMakesVirtual2016,
  title = {What Makes Virtual Agents Believable?},
  author = {Bogdanovych, Anton and Trescak, Tomas and Simoff, Simeon},
  date = {2016-01-02},
  journaltitle = {Connection Science},
  volume = {28},
  number = {1},
  pages = {83--108},
  publisher = {Taylor \& Francis},
  issn = {0954-0091},
  doi = {10.1080/09540091.2015.1130021},
  url = {https://doi.org/10.1080/09540091.2015.1130021},
  urldate = {2025-10-20},
  abstract = {In this paper we investigate the concept of believability and make an attempt to isolate individual characteristics (features) that contribute to making virtual characters believable. As the result of this investigation we have produced a formalisation of believability and based on this formalisation built a computational framework focused on simulation of believable virtual agents that possess the identified features. In order to test whether the identified features are, in fact, responsible for agents being perceived as more believable, we have conducted a user study. In this study we tested user reactions towards the virtual characters that were created for a simulation of aboriginal inhabitants of a particular area of Sydney, Australia in 1770 A.D. The participants of our user study were exposed to short simulated scenes, in which virtual agents performed some behaviour in two different ways (while possessing a certain aspect of believability vs. not possessing it). The results of the study indicate that virtual agents that appear resource bounded, are aware of their environment, own interaction capabilities and their state in the world, agents that can adapt to changes in the environment and exist in correct social context are those that are being perceived as more believable. Further in the paper we discuss these and other believability features and provide a quantitative analysis of the level of contribution for each such feature to the overall perceived believability of a virtual agent.},
  file = {C:\Users\ledwo\Zotero\storage\K8V7X9TH\Bogdanovych et al. - 2016 - What makes virtual agents believable.pdf}
}

@online{chengEvolvingBeYour2024,
  title = {Evolving to Be {{Your Soulmate}}: {{Personalized Dialogue Agents}} with {{Dynamically Adapted Personas}}},
  shorttitle = {Evolving to Be {{Your Soulmate}}},
  author = {Cheng, Yi and Liu, Wenge and Xu, Kaishuai and Hou, Wenjun and Ouyang, Yi and Leong, Chak Tou and Wu, Xian and Zheng, Yefeng},
  date = {2024-06-20},
  eprint = {2406.13960},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2406.13960},
  url = {http://arxiv.org/abs/2406.13960},
  urldate = {2025-09-10},
  abstract = {Previous research on persona-based dialogue agents typically preset the agent's persona before deployment, which remains static thereafter. In this paper, we take a step further and explore a new paradigm called Self-evolving Personalized Dialogue Agents (SPDA), where the agent continuously evolves during the conversation to better align with the user's anticipation by dynamically adapting its persona. This paradigm could enable better personalization for each user, but also introduce unique challenges, which mainly lie in the process of persona adaptation. Two key issues include how to achieve persona alignment with the user and how to ensure smooth transition in the adaptation process. To address them, we propose a novel framework that refines the persona at hierarchical levels to progressively align better with the user in a controllable way. Experiments show that integrating the personas adapted by our framework consistently enhances personalization and overall dialogue performance across various base systems.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\ledwo\\Zotero\\storage\\DSP8PDYR\\Cheng et al. - 2024 - Evolving to be Your Soulmate Personalized Dialogue Agents with Dynamically Adapted Personas.pdf;C\:\\Users\\ledwo\\Zotero\\storage\\NB7JS929\\2406.html}
}

@online{delimarskySpecdrivenDevelopmentAI2025,
  title = {Spec-Driven Development with {{AI}}: {{Get}} Started with a New Open Source Toolkit},
  shorttitle = {Spec-Driven Development with {{AI}}},
  author = {Delimarsky, Den},
  date = {2025-09-02T16:48:03+00:00},
  url = {https://github.blog/ai-and-ml/generative-ai/spec-driven-development-with-ai-get-started-with-a-new-open-source-toolkit/},
  urldate = {2025-09-18},
  abstract = {Developers can use their AI tool of choice for spec-driven development with this open source toolkit.},
  langid = {american},
  organization = {The GitHub Blog},
  file = {C:\Users\ledwo\Zotero\storage\35TURP65\spec-driven-development-with-ai-get-started-with-a-new-open-source-toolkit.html}
}

@article{fabianoHowOptimizeSystematic2024,
  title = {How to Optimize the Systematic Review Process Using {{AI}} Tools},
  author = {Fabiano, Nicholas and Gupta, Arnav and Bhambra, Nishaant and Luu, Brandon and Wong, Stanley and Maaz, Muhammad and Fiedorowicz, Jess G. and Smith, Andrew L. and Solmi, Marco},
  date = {2024-06},
  journaltitle = {JCPP Advances},
  shortjournal = {JCPP Advances},
  volume = {4},
  number = {2},
  pages = {e12234},
  issn = {2692-9384, 2692-9384},
  doi = {10.1002/jcv2.12234},
  url = {https://acamh.onlinelibrary.wiley.com/doi/10.1002/jcv2.12234},
  urldate = {2025-10-28},
  abstract = {Systematic reviews are a cornerstone for synthesizing the available evidence on a given topic. They simultaneously allow for gaps in the literature to be identified and provide direction for future research. However, due to the ever‐increasing volume and complexity of the available literature, traditional methods for conducting systematic reviews are less efficient and more time‐consuming. Numerous artificial intelligence (AI) tools are being released with the potential to optimize efficiency in academic writing and assist with various stages of the systematic review process including developing and refining search strategies, screening titles and abstracts for inclusion or exclusion criteria, extracting essential data from studies and summarizing findings. Therefore, in this article we provide an overview of the currently available tools and how they can be incorporated into the systematic review process to improve efficiency and quality of research synthesis. We emphasize that authors must report all AI tools that have been used at each stage to ensure replicability as part of reporting in methods.},
  langid = {english},
  file = {C:\Users\ledwo\Zotero\storage\IIYLUVRU\Fabiano et al. - 2024 - How to optimize the systematic review process using AI tools.pdf}
}

@online{huangPlanningDarkLLMSymbolic2024,
  title = {Planning in the {{Dark}}: {{LLM-Symbolic Planning Pipeline}} without {{Experts}}},
  shorttitle = {Planning in the {{Dark}}},
  author = {Huang, Sukai and Lipovetzky, Nir and Cohn, Trevor},
  date = {2024-09-24},
  eprint = {2409.15915},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2409.15915},
  url = {http://arxiv.org/abs/2409.15915},
  urldate = {2025-11-04},
  abstract = {Large Language Models (LLMs) have shown promise in solving natural language-described planning tasks, but their direct use often leads to inconsistent reasoning and hallucination. While hybrid LLM-symbolic planning pipelines have emerged as a more robust alternative, they typically require extensive expert intervention to refine and validate generated action schemas. It not only limits scalability but also introduces a potential for biased interpretation, as a single expert's interpretation of ambiguous natural language descriptions might not align with the user's actual intent. To address this, we propose a novel approach that constructs an action schema library to generate multiple candidates, accounting for the diverse possible interpretations of natural language descriptions. We further introduce a semantic validation and ranking module that automatically filter and rank the generated schemas and plans without expert-in-the-loop. The experiments showed our pipeline maintains superiority in planning over the direct LLM planning approach. These findings demonstrate the feasibility of a fully automated end-to-end LLM-symbolic planner that requires no expert intervention, opening up the possibility for a broader audience to engage with AI planning with less prerequisite of domain expertise.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence},
  file = {C\:\\Users\\ledwo\\Zotero\\storage\\TRJ63QFI\\Huang et al. - 2024 - Planning in the Dark LLM-Symbolic Planning Pipeline without Experts.pdf;C\:\\Users\\ledwo\\Zotero\\storage\\XD8X6Z4N\\2409.html}
}

@article{kwonFastAccurateTask,
  title = {Fast and {{Accurate Task Planning}} Using {{Neuro-Symbolic Language Models}} and {{Multi-level Goal Decomposition}}},
  author = {Kwon, Minseo and Kim, Yaesol and Kim, Young J},
  abstract = {In robotic task planning, symbolic planners using rule-based representations like PDDL are effective but struggle with long-sequential tasks in complicated environments due to exponentially increasing search space. Meanwhile, LLM-based approaches, which are grounded in artificial neural networks, offer faster inference and commonsense reasoning but suffer from lower success rates. To address the limitations of the current symbolic (slow speed) or LLM-based approaches (low accuracy), we propose a novel neuro-symbolic task planner that decomposes complex tasks into subgoals using LLM and carries out task planning for each subgoal using either symbolic or MCTS-based LLM planners, depending on the subgoal complexity. This decomposition reduces planning time and improves success rates by narrowing the search space and enabling LLMs to focus on more manageable tasks. Our method significantly reduces planning time while maintaining high success rates across task planning domains, as well as realworld and simulated robotics environments. More details are available at http://graphics.ewha.ac.kr/LLMTAMP/.},
  langid = {english},
  file = {C:\Users\ledwo\Zotero\storage\YJJ7RIXK\Kwon et al. - Fast and Accurate Task Planning using Neuro-Symbolic Language Models and Multi-level Goal Decomposit.pdf}
}

@online{huangPlanningDarkLLMSymbolic2024,
  title = {Planning in the {{Dark}}: {{LLM-Symbolic Planning Pipeline}} without {{Experts}}},
  shorttitle = {Planning in the {{Dark}}},
  author = {Huang, Sukai and Lipovetzky, Nir and Cohn, Trevor},
  date = {2024-09-24},
  eprint = {2409.15915},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2409.15915},
  url = {http://arxiv.org/abs/2409.15915},
  urldate = {2025-11-04},
  abstract = {Large Language Models (LLMs) have shown promise in solving natural language-described planning tasks, but their direct use often leads to inconsistent reasoning and hallucination. While hybrid LLM-symbolic planning pipelines have emerged as a more robust alternative, they typically require extensive expert intervention to refine and validate generated action schemas. It not only limits scalability but also introduces a potential for biased interpretation, as a single expert's interpretation of ambiguous natural language descriptions might not align with the user's actual intent. To address this, we propose a novel approach that constructs an action schema library to generate multiple candidates, accounting for the diverse possible interpretations of natural language descriptions. We further introduce a semantic validation and ranking module that automatically filter and rank the generated schemas and plans without expert-in-the-loop. The experiments showed our pipeline maintains superiority in planning over the direct LLM planning approach. These findings demonstrate the feasibility of a fully automated end-to-end LLM-symbolic planner that requires no expert intervention, opening up the possibility for a broader audience to engage with AI planning with less prerequisite of domain expertise.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence},
  file = {C\:\\Users\\s233148\\Zotero\\storage\\TRJ63QFI\\Huang et al. - 2024 - Planning in the Dark LLM-Symbolic Planning Pipeline without Experts.pdf;C\:\\Users\\s233148\\Zotero\\storage\\XD8X6Z4N\\2409.html}
}

@article{loyallBelievableAgentsBuilding,
  title = {Believable {{Agents}}: {{Building Interactive Personalities}}},
  author = {Loyall, A Bryan},
  langid = {english},
  file = {C:\Users\ledwo\Zotero\storage\4BEAP3J6\Loyall - Believable Agents Building Interactive Personalities.pdf}
}

@article{nayakLongHorizonPlanningMultiAgent,
  title = {Long-{{Horizon Planning}} for {{Multi-Agent Robots}} in {{Partially Observable Environments}}},
  author = {Nayak, Siddharth and Orozco, Adelmo Morrison and Zhang, Jackson and Chen, Darren and Kapoor, Aditya and Robinson, Eric and Gopalakrishnan, Karthik and Harrison, James and Ichter, Brian and Mahajan, Anuj and Balakrishnan, Hamsa},
  langid = {english},
  file = {C:\Users\ledwo\Zotero\storage\7JRYCB3W\Nayak et al. - Long-Horizon Planning for Multi-Agent Robots in Partially Observable Environments.pdf}
}

@online{parkGenerativeAgentsInteractive2023,
  title = {Generative {{Agents}}: {{Interactive Simulacra}} of {{Human Behavior}}},
  shorttitle = {Generative {{Agents}}},
  author = {Park, Joon Sung and O'Brien, Joseph C. and Cai, Carrie J. and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
  date = {2023-08-06},
  eprint = {2304.03442},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.03442},
  url = {http://arxiv.org/abs/2304.03442},
  urldate = {2025-09-10},
  abstract = {Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents--computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors: for example, starting with only a single user-specified notion that one agent wants to throw a Valentine's Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture--observation, planning, and reflection--each contribute critically to the believability of agent behavior. By fusing large language models with computational, interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  file = {C\:\\Users\\ledwo\\Zotero\\storage\\768K5FHG\\Park et al. - 2023 - Generative Agents Interactive Simulacra of Human Behavior.pdf;C\:\\Users\\ledwo\\Zotero\\storage\\HFY36LME\\2304.html}
}

@inproceedings{parkGenerativeAgentsInteractive2023a,
  title = {Generative {{Agents}}: {{Interactive Simulacra}} of {{Human Behavior}}},
  shorttitle = {Generative {{Agents}}},
  booktitle = {Proceedings of the 36th {{Annual ACM Symposium}} on {{User Interface Software}} and {{Technology}}},
  author = {Park, Joon Sung and O'Brien, Joseph and Cai, Carrie Jun and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
  date = {2023-10-29},
  series = {{{UIST}} '23},
  pages = {1--22},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3586183.3606763},
  url = {https://dl.acm.org/doi/10.1145/3586183.3606763},
  urldate = {2025-09-10},
  abstract = {Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents: computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent’s experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty-five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors. For example, starting with only a single user-specified notion that one agent wants to throw a Valentine’s Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture—observation, planning, and reflection—each contribute critically to the believability of agent behavior. By fusing large language models with computational interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.},
  isbn = {979-8-4007-0132-0},
  file = {C:\Users\ledwo\Zotero\storage\F2TZRCHS\Park et al. - 2023 - Generative Agents Interactive Simulacra of Human Behavior.pdf}
}

@online{tantakounLLMsPlanningModelers2025,
  title = {{{LLMs}} as {{Planning Modelers}}: {{A Survey}} for {{Leveraging Large Language Models}} to {{Construct Automated Planning Models}}},
  shorttitle = {{{LLMs}} as {{Planning Modelers}}},
  author = {Tantakoun, Marcus and Zhu, Xiaodan and Muise, Christian},
  date = {2025-03-22},
  eprint = {2503.18971},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2503.18971},
  url = {http://arxiv.org/abs/2503.18971},
  urldate = {2025-09-10},
  abstract = {Large Language Models (LLMs) excel in various natural language tasks but often struggle with long-horizon planning problems requiring structured reasoning. This limitation has drawn interest in integrating neuro-symbolic approaches within the Automated Planning (AP) and Natural Language Processing (NLP) communities. However, identifying optimal AP deployment frameworks can be daunting. This paper aims to provide a timely survey of the current research with an in-depth analysis, positioning LLMs as tools for extracting and refining planning models to support reliable AP planners. By systematically reviewing the current state of research, we highlight methodologies, and identify critical challenges and future directions, hoping to contribute to the joint research on NLP and Automated Planning.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence},
  file = {C\:\\Users\\ledwo\\Zotero\\storage\\ZKXTYSW5\\Tantakoun et al. - 2025 - LLMs as Planning Modelers A Survey for Leveraging Large Language Models to Construct Automated Plan.pdf;C\:\\Users\\ledwo\\Zotero\\storage\\W7TGQFIZ\\2503.html}
}

@online{tenceAutomatableEvaluationMethod2010,
  title = {Automatable {{Evaluation Method Oriented}} toward {{Behaviour Believability}} for {{Video Games}}},
  author = {Tencé, Fabien and Buche, Cédric},
  date = {2010-09-02},
  eprint = {1009.0501},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1009.0501},
  url = {http://arxiv.org/abs/1009.0501},
  urldate = {2025-10-20},
  abstract = {Classic evaluation methods of believable agents are time-consuming because they involve many human to judge agents. They are well suited to validate work on new believable behaviours models. However, during the implementation, numerous experiments can help to improve agents' believability. We propose a method which aim at assessing how much an agent's behaviour looks like humans' behaviours. By representing behaviours with vectors, we can store data computed for humans and then evaluate as many agents as needed without further need of humans. We present a test experiment which shows that even a simple evaluation following our method can reveal differences between quite believable agents and humans. This method seems promising although, as shown in our experiment, results' analysis can be difficult.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence},
  file = {C\:\\Users\\ledwo\\Zotero\\storage\\7PYZAXBN\\Tencé and Buche - 2010 - Automatable Evaluation Method Oriented toward Behaviour Believability for Video Games.pdf;C\:\\Users\\ledwo\\Zotero\\storage\\BEY9MXXW\\1009.html}
}

@online{xiaoHowFarAre2024,
  title = {How {{Far Are LLMs}} from {{Believable AI}}? {{A Benchmark}} for {{Evaluating}} the {{Believability}} of {{Human Behavior Simulation}}},
  shorttitle = {How {{Far Are LLMs}} from {{Believable AI}}?},
  author = {Xiao, Yang and Cheng, Yi and Fu, Jinlan and Wang, Jiashuo and Li, Wenjie and Liu, Pengfei},
  date = {2024-06-15},
  eprint = {2312.17115},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.17115},
  url = {http://arxiv.org/abs/2312.17115},
  urldate = {2025-10-20},
  abstract = {In recent years, AI has demonstrated remarkable capabilities in simulating human behaviors, particularly those implemented with large language models (LLMs). However, due to the lack of systematic evaluation of LLMs' simulated behaviors, the believability of LLMs among humans remains ambiguous, i.e., it is unclear which behaviors of LLMs are convincingly human-like and which need further improvements. In this work, we design SimulateBench to evaluate the believability of LLMs when simulating human behaviors. In specific, we evaluate the believability of LLMs based on two critical dimensions: 1) consistency: the extent to which LLMs can behave consistently with the given information of a human to simulate; and 2) robustness: the ability of LLMs' simulated behaviors to remain robust when faced with perturbations. SimulateBench includes 65 character profiles and a total of 8,400 questions to examine LLMs' simulated behaviors. Based on SimulateBench, we evaluate the performances of 10 widely used LLMs when simulating characters. The experimental results reveal that current LLMs struggle to align their behaviors with assigned characters and are vulnerable to perturbations in certain factors.},
  pubstate = {prepublished},
  version = {2},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society},
  file = {C\:\\Users\\ledwo\\Zotero\\storage\\RNPMXZDD\\Xiao et al. - 2024 - How Far Are LLMs from Believable AI A Benchmark for Evaluating the Believability of Human Behavior.pdf;C\:\\Users\\ledwo\\Zotero\\storage\\A2JUFH4H\\2312.html}
}

@online{zhangAgenticContextEngineering2025,
  title = {Agentic {{Context Engineering}}: {{Evolving Contexts}} for {{Self-Improving Language Models}}},
  shorttitle = {Agentic {{Context Engineering}}},
  author = {Zhang, Qizheng and Hu, Changran and Upasani, Shubhangi and Ma, Boyuan and Hong, Fenglu and Kamanuru, Vamsidhar and Rainton, Jay and Wu, Chen and Ji, Mengmeng and Li, Hanchen and Thakker, Urmish and Zou, James and Olukotun, Kunle},
  date = {2025-10-06},
  eprint = {2510.04618},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2510.04618},
  url = {http://arxiv.org/abs/2510.04618},
  urldate = {2025-10-13},
  abstract = {Large language model (LLM) applications such as agents and domain-specific reasoning increasingly rely on context adaptation -- modifying inputs with instructions, strategies, or evidence, rather than weight updates. Prior approaches improve usability but often suffer from brevity bias, which drops domain insights for concise summaries, and from context collapse, where iterative rewriting erodes details over time. Building on the adaptive memory introduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context Engineering), a framework that treats contexts as evolving playbooks that accumulate, refine, and organize strategies through a modular process of generation, reflection, and curation. ACE prevents collapse with structured, incremental updates that preserve detailed knowledge and scale with long-context models. Across agent and domain-specific benchmarks, ACE optimizes contexts both offline (e.g., system prompts) and online (e.g., agent memory), consistently outperforming strong baselines: +10.6\% on agents and +8.6\% on finance, while significantly reducing adaptation latency and rollout cost. Notably, ACE could adapt effectively without labeled supervision and instead by leveraging natural execution feedback. On the AppWorld leaderboard, ACE matches the top-ranked production-level agent on the overall average and surpasses it on the harder test-challenge split, despite using a smaller open-source model. These results show that comprehensive, evolving contexts enable scalable, efficient, and self-improving LLM systems with low overhead.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\ledwo\\Zotero\\storage\\NVNZX7HK\\Zhang et al. - 2025 - Agentic Context Engineering Evolving Contexts for Self-Improving Language Models.pdf;C\:\\Users\\ledwo\\Zotero\\storage\\WQPF2DRQ\\2510.html}
}

@online{zhangVerbalizedSamplingHow2025,
  title = {Verbalized {{Sampling}}: {{How}} to {{Mitigate Mode Collapse}} and {{Unlock LLM Diversity}}},
  shorttitle = {Verbalized {{Sampling}}},
  author = {Zhang, Jiayi and Yu, Simon and Chong, Derek and Sicilia, Anthony and Tomz, Michael R. and Manning, Christopher D. and Shi, Weiyan},
  date = {2025-10-10},
  eprint = {2510.01171},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2510.01171},
  url = {http://arxiv.org/abs/2510.01171},
  urldate = {2025-10-30},
  abstract = {Post-training alignment often reduces LLM diversity, leading to a phenomenon known as mode collapse. Unlike prior work that attributes this effect to algorithmic limitations, we identify a fundamental, pervasive data-level driver: typicality bias in preference data, whereby annotators systematically favor familiar text as a result of well-established findings in cognitive psychology. We formalize this bias theoretically, verify it on preference datasets empirically, and show that it plays a central role in mode collapse. Motivated by this analysis, we introduce Verbalized Sampling, a simple, training-free prompting strategy to circumvent mode collapse. VS prompts the model to verbalize a probability distribution over a set of responses (e.g., "Generate 5 jokes about coffee and their corresponding probabilities"). Comprehensive experiments show that VS significantly improves performance across creative writing (poems, stories, jokes), dialogue simulation, open-ended QA, and synthetic data generation, without sacrificing factual accuracy and safety. For instance, in creative writing, VS increases diversity by 1.6-2.1x over direct prompting. We further observe an emergent trend that more capable models benefit more from VS. In sum, our work provides a new data-centric perspective on mode collapse and a practical inference-time remedy that helps unlock pre-trained generative diversity.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\ledwo\\Zotero\\storage\\RDJ8WMU6\\Zhang et al. - 2025 - Verbalized Sampling How to Mitigate Mode Collapse and Unlock LLM Diversity.pdf;C\:\\Users\\ledwo\\Zotero\\storage\\LVY554YX\\2510.html}
}

@online{zhouISRLLMIterativeSelfRefined2023,
  title = {{{ISR-LLM}}: {{Iterative Self-Refined Large Language Model}} for {{Long-Horizon Sequential Task Planning}}},
  shorttitle = {{{ISR-LLM}}},
  author = {Zhou, Zhehua and Song, Jiayang and Yao, Kunpeng and Shu, Zhan and Ma, Lei},
  date = {2023-08-26},
  eprint = {2308.13724},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2308.13724},
  url = {http://arxiv.org/abs/2308.13724},
  urldate = {2025-11-04},
  abstract = {Motivated by the substantial achievements observed in Large Language Models (LLMs) in the field of natural language processing, recent research has commenced investigations into the application of LLMs for complex, long-horizon sequential task planning challenges in robotics. LLMs are advantageous in offering the potential to enhance the generalizability as task-agnostic planners and facilitate flexible interaction between human instructors and planning systems. However, task plans generated by LLMs often lack feasibility and correctness. To address this challenge, we introduce ISR-LLM, a novel framework that improves LLM-based planning through an iterative self-refinement process. The framework operates through three sequential steps: preprocessing, planning, and iterative self-refinement. During preprocessing, an LLM translator is employed to convert natural language input into a Planning Domain Definition Language (PDDL) formulation. In the planning phase, an LLM planner formulates an initial plan, which is then assessed and refined in the iterative self-refinement step by using a validator. We examine the performance of ISR-LLM across three distinct planning domains. The results show that ISR-LLM is able to achieve markedly higher success rates in task accomplishments compared to state-of-the-art LLM-based planners. Moreover, it also preserves the broad applicability and generalizability of working with natural language instructions.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {C\:\\Users\\ledwo\\Zotero\\storage\\MNH3JEXP\\Zhou et al. - 2023 - ISR-LLM Iterative Self-Refined Large Language Model for Long-Horizon Sequential Task Planning.pdf;C\:\\Users\\ledwo\\Zotero\\storage\\A7S42Z6M\\2308.html}
}
