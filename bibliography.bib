@article{batesRoleEmotionBelievable1994,
  title = {The Role of Emotion in Believable Agents},
  author = {Bates, Joseph},
  date = {1994-07},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {37},
  number = {7},
  pages = {122--125},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/176789.176803},
  url = {https://dl.acm.org/doi/10.1145/176789.176803},
  urldate = {2025-10-20},
  langid = {english},
  file = {C:\Users\s233148\Zotero\storage\3SJL4EXE\Bates - 1994 - The role of emotion in believable agents.pdf}
}

@article{bogdanovychWhatMakesVirtual2016,
  title = {What Makes Virtual Agents Believable?},
  author = {Bogdanovych, Anton and Trescak, Tomas and Simoff, Simeon},
  date = {2016-01-02},
  journaltitle = {Connection Science},
  volume = {28},
  number = {1},
  pages = {83--108},
  publisher = {Taylor \& Francis},
  issn = {0954-0091},
  doi = {10.1080/09540091.2015.1130021},
  url = {https://doi.org/10.1080/09540091.2015.1130021},
  urldate = {2025-10-20},
  abstract = {In this paper we investigate the concept of believability and make an attempt to isolate individual characteristics (features) that contribute to making virtual characters believable. As the result of this investigation we have produced a formalisation of believability and based on this formalisation built a computational framework focused on simulation of believable virtual agents that possess the identified features. In order to test whether the identified features are, in fact, responsible for agents being perceived as more believable, we have conducted a user study. In this study we tested user reactions towards the virtual characters that were created for a simulation of aboriginal inhabitants of a particular area of Sydney, Australia in 1770 A.D. The participants of our user study were exposed to short simulated scenes, in which virtual agents performed some behaviour in two different ways (while possessing a certain aspect of believability vs. not possessing it). The results of the study indicate that virtual agents that appear resource bounded, are aware of their environment, own interaction capabilities and their state in the world, agents that can adapt to changes in the environment and exist in correct social context are those that are being perceived as more believable. Further in the paper we discuss these and other believability features and provide a quantitative analysis of the level of contribution for each such feature to the overall perceived believability of a virtual agent.},
  file = {C:\Users\s233148\Zotero\storage\K8V7X9TH\Bogdanovych et al. - 2016 - What makes virtual agents believable.pdf}
}

@online{chengEvolvingBeYour2024,
  title = {Evolving to Be {{Your Soulmate}}: {{Personalized Dialogue Agents}} with {{Dynamically Adapted Personas}}},
  shorttitle = {Evolving to Be {{Your Soulmate}}},
  author = {Cheng, Yi and Liu, Wenge and Xu, Kaishuai and Hou, Wenjun and Ouyang, Yi and Leong, Chak Tou and Wu, Xian and Zheng, Yefeng},
  date = {2024-06-20},
  eprint = {2406.13960},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2406.13960},
  url = {http://arxiv.org/abs/2406.13960},
  urldate = {2025-09-10},
  abstract = {Previous research on persona-based dialogue agents typically preset the agent's persona before deployment, which remains static thereafter. In this paper, we take a step further and explore a new paradigm called Self-evolving Personalized Dialogue Agents (SPDA), where the agent continuously evolves during the conversation to better align with the user's anticipation by dynamically adapting its persona. This paradigm could enable better personalization for each user, but also introduce unique challenges, which mainly lie in the process of persona adaptation. Two key issues include how to achieve persona alignment with the user and how to ensure smooth transition in the adaptation process. To address them, we propose a novel framework that refines the persona at hierarchical levels to progressively align better with the user in a controllable way. Experiments show that integrating the personas adapted by our framework consistently enhances personalization and overall dialogue performance across various base systems.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\s233148\\Zotero\\storage\\DSP8PDYR\\Cheng et al. - 2024 - Evolving to be Your Soulmate Personalized Dialogue Agents with Dynamically Adapted Personas.pdf;C\:\\Users\\s233148\\Zotero\\storage\\NB7JS929\\2406.html}
}

@online{delimarskySpecdrivenDevelopmentAI2025,
  title = {Spec-Driven Development with {{AI}}: {{Get}} Started with a New Open Source Toolkit},
  shorttitle = {Spec-Driven Development with {{AI}}},
  author = {Delimarsky, Den},
  date = {2025-09-02T16:48:03+00:00},
  url = {https://github.blog/ai-and-ml/generative-ai/spec-driven-development-with-ai-get-started-with-a-new-open-source-toolkit/},
  urldate = {2025-09-18},
  abstract = {Developers can use their AI tool of choice for spec-driven development with this open source toolkit.},
  langid = {american},
  organization = {The GitHub Blog},
  file = {C:\Users\s233148\Zotero\storage\35TURP65\spec-driven-development-with-ai-get-started-with-a-new-open-source-toolkit.html}
}

@article{fabianoHowOptimizeSystematic2024,
  title = {How to Optimize the Systematic Review Process Using {{AI}} Tools},
  author = {Fabiano, Nicholas and Gupta, Arnav and Bhambra, Nishaant and Luu, Brandon and Wong, Stanley and Maaz, Muhammad and Fiedorowicz, Jess G. and Smith, Andrew L. and Solmi, Marco},
  date = {2024-06},
  journaltitle = {JCPP Advances},
  shortjournal = {JCPP Advances},
  volume = {4},
  number = {2},
  pages = {e12234},
  issn = {2692-9384, 2692-9384},
  doi = {10.1002/jcv2.12234},
  url = {https://acamh.onlinelibrary.wiley.com/doi/10.1002/jcv2.12234},
  urldate = {2025-10-28},
  abstract = {Systematic reviews are a cornerstone for synthesizing the available evidence on a given topic. They simultaneously allow for gaps in the literature to be identified and provide direction for future research. However, due to the ever‐increasing volume and complexity of the available literature, traditional methods for conducting systematic reviews are less efficient and more time‐consuming. Numerous artificial intelligence (AI) tools are being released with the potential to optimize efficiency in academic writing and assist with various stages of the systematic review process including developing and refining search strategies, screening titles and abstracts for inclusion or exclusion criteria, extracting essential data from studies and summarizing findings. Therefore, in this article we provide an overview of the currently available tools and how they can be incorporated into the systematic review process to improve efficiency and quality of research synthesis. We emphasize that authors must report all AI tools that have been used at each stage to ensure replicability as part of reporting in methods.},
  langid = {english},
  file = {C:\Users\s233148\Zotero\storage\IIYLUVRU\Fabiano et al. - 2024 - How to optimize the systematic review process using AI tools.pdf}
}

@article{loyallBelievableAgentsBuilding,
  title = {Believable {{Agents}}: {{Building Interactive Personalities}}},
  author = {Loyall, A Bryan},
  langid = {english},
  file = {C:\Users\s233148\Zotero\storage\4BEAP3J6\Loyall - Believable Agents Building Interactive Personalities.pdf}
}

@online{parkGenerativeAgentsInteractive2023,
  title = {Generative {{Agents}}: {{Interactive Simulacra}} of {{Human Behavior}}},
  shorttitle = {Generative {{Agents}}},
  author = {Park, Joon Sung and O'Brien, Joseph C. and Cai, Carrie J. and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
  date = {2023-08-06},
  eprint = {2304.03442},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.03442},
  url = {http://arxiv.org/abs/2304.03442},
  urldate = {2025-09-10},
  abstract = {Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents--computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors: for example, starting with only a single user-specified notion that one agent wants to throw a Valentine's Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture--observation, planning, and reflection--each contribute critically to the believability of agent behavior. By fusing large language models with computational, interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  file = {C\:\\Users\\s233148\\Zotero\\storage\\768K5FHG\\Park et al. - 2023 - Generative Agents Interactive Simulacra of Human Behavior.pdf;C\:\\Users\\s233148\\Zotero\\storage\\HFY36LME\\2304.html}
}

@inproceedings{parkGenerativeAgentsInteractive2023a,
  title = {Generative {{Agents}}: {{Interactive Simulacra}} of {{Human Behavior}}},
  shorttitle = {Generative {{Agents}}},
  booktitle = {Proceedings of the 36th {{Annual ACM Symposium}} on {{User Interface Software}} and {{Technology}}},
  author = {Park, Joon Sung and O'Brien, Joseph and Cai, Carrie Jun and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
  date = {2023-10-29},
  series = {{{UIST}} '23},
  pages = {1--22},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3586183.3606763},
  url = {https://dl.acm.org/doi/10.1145/3586183.3606763},
  urldate = {2025-09-10},
  abstract = {Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents: computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent’s experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty-five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors. For example, starting with only a single user-specified notion that one agent wants to throw a Valentine’s Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture—observation, planning, and reflection—each contribute critically to the believability of agent behavior. By fusing large language models with computational interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.},
  isbn = {979-8-4007-0132-0},
  file = {C:\Users\s233148\Zotero\storage\F2TZRCHS\Park et al. - 2023 - Generative Agents Interactive Simulacra of Human Behavior.pdf}
}

@online{tantakounLLMsPlanningModelers2025,
  title = {{{LLMs}} as {{Planning Modelers}}: {{A Survey}} for {{Leveraging Large Language Models}} to {{Construct Automated Planning Models}}},
  shorttitle = {{{LLMs}} as {{Planning Modelers}}},
  author = {Tantakoun, Marcus and Zhu, Xiaodan and Muise, Christian},
  date = {2025-03-22},
  eprint = {2503.18971},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2503.18971},
  url = {http://arxiv.org/abs/2503.18971},
  urldate = {2025-09-10},
  abstract = {Large Language Models (LLMs) excel in various natural language tasks but often struggle with long-horizon planning problems requiring structured reasoning. This limitation has drawn interest in integrating neuro-symbolic approaches within the Automated Planning (AP) and Natural Language Processing (NLP) communities. However, identifying optimal AP deployment frameworks can be daunting. This paper aims to provide a timely survey of the current research with an in-depth analysis, positioning LLMs as tools for extracting and refining planning models to support reliable AP planners. By systematically reviewing the current state of research, we highlight methodologies, and identify critical challenges and future directions, hoping to contribute to the joint research on NLP and Automated Planning.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence},
  file = {C\:\\Users\\s233148\\Zotero\\storage\\ZKXTYSW5\\Tantakoun et al. - 2025 - LLMs as Planning Modelers A Survey for Leveraging Large Language Models to Construct Automated Plan.pdf;C\:\\Users\\s233148\\Zotero\\storage\\W7TGQFIZ\\2503.html}
}

@online{tenceAutomatableEvaluationMethod2010,
  title = {Automatable {{Evaluation Method Oriented}} toward {{Behaviour Believability}} for {{Video Games}}},
  author = {Tencé, Fabien and Buche, Cédric},
  date = {2010-09-02},
  eprint = {1009.0501},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1009.0501},
  url = {http://arxiv.org/abs/1009.0501},
  urldate = {2025-10-20},
  abstract = {Classic evaluation methods of believable agents are time-consuming because they involve many human to judge agents. They are well suited to validate work on new believable behaviours models. However, during the implementation, numerous experiments can help to improve agents' believability. We propose a method which aim at assessing how much an agent's behaviour looks like humans' behaviours. By representing behaviours with vectors, we can store data computed for humans and then evaluate as many agents as needed without further need of humans. We present a test experiment which shows that even a simple evaluation following our method can reveal differences between quite believable agents and humans. This method seems promising although, as shown in our experiment, results' analysis can be difficult.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence},
  file = {C\:\\Users\\s233148\\Zotero\\storage\\7PYZAXBN\\Tencé and Buche - 2010 - Automatable Evaluation Method Oriented toward Behaviour Believability for Video Games.pdf;C\:\\Users\\s233148\\Zotero\\storage\\BEY9MXXW\\1009.html}
}

@online{xiaoHowFarAre2024,
  title = {How {{Far Are LLMs}} from {{Believable AI}}? {{A Benchmark}} for {{Evaluating}} the {{Believability}} of {{Human Behavior Simulation}}},
  shorttitle = {How {{Far Are LLMs}} from {{Believable AI}}?},
  author = {Xiao, Yang and Cheng, Yi and Fu, Jinlan and Wang, Jiashuo and Li, Wenjie and Liu, Pengfei},
  date = {2024-06-15},
  eprint = {2312.17115},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.17115},
  url = {http://arxiv.org/abs/2312.17115},
  urldate = {2025-10-20},
  abstract = {In recent years, AI has demonstrated remarkable capabilities in simulating human behaviors, particularly those implemented with large language models (LLMs). However, due to the lack of systematic evaluation of LLMs' simulated behaviors, the believability of LLMs among humans remains ambiguous, i.e., it is unclear which behaviors of LLMs are convincingly human-like and which need further improvements. In this work, we design SimulateBench to evaluate the believability of LLMs when simulating human behaviors. In specific, we evaluate the believability of LLMs based on two critical dimensions: 1) consistency: the extent to which LLMs can behave consistently with the given information of a human to simulate; and 2) robustness: the ability of LLMs' simulated behaviors to remain robust when faced with perturbations. SimulateBench includes 65 character profiles and a total of 8,400 questions to examine LLMs' simulated behaviors. Based on SimulateBench, we evaluate the performances of 10 widely used LLMs when simulating characters. The experimental results reveal that current LLMs struggle to align their behaviors with assigned characters and are vulnerable to perturbations in certain factors.},
  pubstate = {prepublished},
  version = {2},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society},
  file = {C\:\\Users\\s233148\\Zotero\\storage\\RNPMXZDD\\Xiao et al. - 2024 - How Far Are LLMs from Believable AI A Benchmark for Evaluating the Believability of Human Behavior.pdf;C\:\\Users\\s233148\\Zotero\\storage\\A2JUFH4H\\2312.html}
}

@online{zhangAgenticContextEngineering2025,
  title = {Agentic {{Context Engineering}}: {{Evolving Contexts}} for {{Self-Improving Language Models}}},
  shorttitle = {Agentic {{Context Engineering}}},
  author = {Zhang, Qizheng and Hu, Changran and Upasani, Shubhangi and Ma, Boyuan and Hong, Fenglu and Kamanuru, Vamsidhar and Rainton, Jay and Wu, Chen and Ji, Mengmeng and Li, Hanchen and Thakker, Urmish and Zou, James and Olukotun, Kunle},
  date = {2025-10-06},
  eprint = {2510.04618},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2510.04618},
  url = {http://arxiv.org/abs/2510.04618},
  urldate = {2025-10-13},
  abstract = {Large language model (LLM) applications such as agents and domain-specific reasoning increasingly rely on context adaptation -- modifying inputs with instructions, strategies, or evidence, rather than weight updates. Prior approaches improve usability but often suffer from brevity bias, which drops domain insights for concise summaries, and from context collapse, where iterative rewriting erodes details over time. Building on the adaptive memory introduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context Engineering), a framework that treats contexts as evolving playbooks that accumulate, refine, and organize strategies through a modular process of generation, reflection, and curation. ACE prevents collapse with structured, incremental updates that preserve detailed knowledge and scale with long-context models. Across agent and domain-specific benchmarks, ACE optimizes contexts both offline (e.g., system prompts) and online (e.g., agent memory), consistently outperforming strong baselines: +10.6\% on agents and +8.6\% on finance, while significantly reducing adaptation latency and rollout cost. Notably, ACE could adapt effectively without labeled supervision and instead by leveraging natural execution feedback. On the AppWorld leaderboard, ACE matches the top-ranked production-level agent on the overall average and surpasses it on the harder test-challenge split, despite using a smaller open-source model. These results show that comprehensive, evolving contexts enable scalable, efficient, and self-improving LLM systems with low overhead.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\s233148\\Zotero\\storage\\NVNZX7HK\\Zhang et al. - 2025 - Agentic Context Engineering Evolving Contexts for Self-Improving Language Models.pdf;C\:\\Users\\s233148\\Zotero\\storage\\WQPF2DRQ\\2510.html}
}

@online{zhangVerbalizedSamplingHow2025,
  title = {Verbalized {{Sampling}}: {{How}} to {{Mitigate Mode Collapse}} and {{Unlock LLM Diversity}}},
  shorttitle = {Verbalized {{Sampling}}},
  author = {Zhang, Jiayi and Yu, Simon and Chong, Derek and Sicilia, Anthony and Tomz, Michael R. and Manning, Christopher D. and Shi, Weiyan},
  date = {2025-10-10},
  eprint = {2510.01171},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2510.01171},
  url = {http://arxiv.org/abs/2510.01171},
  urldate = {2025-10-30},
  abstract = {Post-training alignment often reduces LLM diversity, leading to a phenomenon known as mode collapse. Unlike prior work that attributes this effect to algorithmic limitations, we identify a fundamental, pervasive data-level driver: typicality bias in preference data, whereby annotators systematically favor familiar text as a result of well-established findings in cognitive psychology. We formalize this bias theoretically, verify it on preference datasets empirically, and show that it plays a central role in mode collapse. Motivated by this analysis, we introduce Verbalized Sampling, a simple, training-free prompting strategy to circumvent mode collapse. VS prompts the model to verbalize a probability distribution over a set of responses (e.g., "Generate 5 jokes about coffee and their corresponding probabilities"). Comprehensive experiments show that VS significantly improves performance across creative writing (poems, stories, jokes), dialogue simulation, open-ended QA, and synthetic data generation, without sacrificing factual accuracy and safety. For instance, in creative writing, VS increases diversity by 1.6-2.1x over direct prompting. We further observe an emergent trend that more capable models benefit more from VS. In sum, our work provides a new data-centric perspective on mode collapse and a practical inference-time remedy that helps unlock pre-trained generative diversity.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\s233148\\Zotero\\storage\\RDJ8WMU6\\Zhang et al. - 2025 - Verbalized Sampling How to Mitigate Mode Collapse and Unlock LLM Diversity.pdf;C\:\\Users\\s233148\\Zotero\\storage\\LVY554YX\\2510.html}
}
