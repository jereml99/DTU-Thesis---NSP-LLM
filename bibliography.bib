@online{ahnCanNotSay2022,
  title = {Do {{As I Can}}, {{Not As I Say}}: {{Grounding Language}} in {{Robotic Affordances}}},
  shorttitle = {Do {{As I Can}}, {{Not As I Say}}},
  author = {Ahn, Michael and Brohan, Anthony and Brown, Noah and Chebotar, Yevgen and Cortes, Omar and David, Byron and Finn, Chelsea and Fu, Chuyuan and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Ho, Daniel and Hsu, Jasmine and Ibarz, Julian and Ichter, Brian and Irpan, Alex and Jang, Eric and Ruano, Rosario Jauregui and Jeffrey, Kyle and Jesmonth, Sally and Joshi, Nikhil J. and Julian, Ryan and Kalashnikov, Dmitry and Kuang, Yuheng and Lee, Kuang-Huei and Levine, Sergey and Lu, Yao and Luu, Linda and Parada, Carolina and Pastor, Peter and Quiambao, Jornell and Rao, Kanishka and Rettinghouse, Jarek and Reyes, Diego and Sermanet, Pierre and Sievers, Nicolas and Tan, Clayton and Toshev, Alexander and Vanhoucke, Vincent and Xia, Fei and Xiao, Ted and Xu, Peng and Xu, Sichun and Yan, Mengyuan and Zeng, Andy},
  date = {2022-08-16},
  eprint = {2204.01691},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2204.01691},
  url = {http://arxiv.org/abs/2204.01691},
  urldate = {2025-11-10},
  abstract = {Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model's "hands and eyes," while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally-extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project's website and the video can be found at https://say-can.github.io/.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C\:\\Users\\s233148\\Zotero\\storage\\NCMQV4XY\\Ahn et al. - 2022 - Do As I Can, Not As I Say Grounding Language in Robotic Affordances.pdf;C\:\\Users\\s233148\\Zotero\\storage\\JGIEQIEX\\2204.html}
}

@article{batesRoleEmotionBelievable1994,
  title = {The Role of Emotion in Believable Agents},
  author = {Bates, Joseph},
  date = {1994-07},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {37},
  number = {7},
  pages = {122--125},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/176789.176803},
  url = {https://dl.acm.org/doi/10.1145/176789.176803},
  urldate = {2025-10-20},
  langid = {english},
  file = {C:\Users\s233148\Zotero\storage\3SJL4EXE\Bates - 1994 - The role of emotion in believable agents.pdf}
}

@article{bogdanovychWhatMakesVirtual2016,
  title = {What Makes Virtual Agents Believable?},
  author = {Bogdanovych, Anton and Trescak, Tomas and Simoff, Simeon},
  date = {2016-01-02},
  journaltitle = {Connection Science},
  volume = {28},
  number = {1},
  pages = {83--108},
  publisher = {Taylor \& Francis},
  issn = {0954-0091},
  doi = {10.1080/09540091.2015.1130021},
  url = {https://doi.org/10.1080/09540091.2015.1130021},
  urldate = {2025-10-20},
  abstract = {In this paper we investigate the concept of believability and make an attempt to isolate individual characteristics (features) that contribute to making virtual characters believable. As the result of this investigation we have produced a formalisation of believability and based on this formalisation built a computational framework focused on simulation of believable virtual agents that possess the identified features. In order to test whether the identified features are, in fact, responsible for agents being perceived as more believable, we have conducted a user study. In this study we tested user reactions towards the virtual characters that were created for a simulation of aboriginal inhabitants of a particular area of Sydney, Australia in 1770 A.D. The participants of our user study were exposed to short simulated scenes, in which virtual agents performed some behaviour in two different ways (while possessing a certain aspect of believability vs. not possessing it). The results of the study indicate that virtual agents that appear resource bounded, are aware of their environment, own interaction capabilities and their state in the world, agents that can adapt to changes in the environment and exist in correct social context are those that are being perceived as more believable. Further in the paper we discuss these and other believability features and provide a quantitative analysis of the level of contribution for each such feature to the overall perceived believability of a virtual agent.},
  file = {C:\Users\s233148\Zotero\storage\K8V7X9TH\Bogdanovych et al. - 2016 - What makes virtual agents believable.pdf}
}

@inproceedings{brown2020language,
  title = {Language Models Are Few-Shot Learners},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  date = {2020},
  eprint = {2005.14165},
  eprinttype = {arXiv},
  url = {https://arxiv.org/abs/2005.14165}
}

@online{chengEvolvingBeYour2024,
  title = {Evolving to Be {{Your Soulmate}}: {{Personalized Dialogue Agents}} with {{Dynamically Adapted Personas}}},
  shorttitle = {Evolving to Be {{Your Soulmate}}},
  author = {Cheng, Yi and Liu, Wenge and Xu, Kaishuai and Hou, Wenjun and Ouyang, Yi and Leong, Chak Tou and Wu, Xian and Zheng, Yefeng},
  date = {2024-06-20},
  eprint = {2406.13960},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2406.13960},
  url = {http://arxiv.org/abs/2406.13960},
  urldate = {2025-09-10},
  abstract = {Previous research on persona-based dialogue agents typically preset the agent's persona before deployment, which remains static thereafter. In this paper, we take a step further and explore a new paradigm called Self-evolving Personalized Dialogue Agents (SPDA), where the agent continuously evolves during the conversation to better align with the user's anticipation by dynamically adapting its persona. This paradigm could enable better personalization for each user, but also introduce unique challenges, which mainly lie in the process of persona adaptation. Two key issues include how to achieve persona alignment with the user and how to ensure smooth transition in the adaptation process. To address them, we propose a novel framework that refines the persona at hierarchical levels to progressively align better with the user in a controllable way. Experiments show that integrating the personas adapted by our framework consistently enhances personalization and overall dialogue performance across various base systems.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\s233148\\Zotero\\storage\\DSP8PDYR\\Cheng et al. - 2024 - Evolving to be Your Soulmate Personalized Dialogue Agents with Dynamically Adapted Personas.pdf;C\:\\Users\\s233148\\Zotero\\storage\\NB7JS929\\2406.html}
}

@online{delimarskySpecdrivenDevelopmentAI2025,
  title = {Spec-Driven Development with {{AI}}: {{Get}} Started with a New Open Source Toolkit},
  shorttitle = {Spec-Driven Development with {{AI}}},
  author = {Delimarsky, Den},
  date = {2025-09-02T16:48:03+00:00},
  url = {https://github.blog/ai-and-ml/generative-ai/spec-driven-development-with-ai-get-started-with-a-new-open-source-toolkit/},
  urldate = {2025-09-18},
  abstract = {Developers can use their AI tool of choice for spec-driven development with this open source toolkit.},
  langid = {american},
  organization = {The GitHub Blog},
  file = {C:\Users\s233148\Zotero\storage\35TURP65\spec-driven-development-with-ai-get-started-with-a-new-open-source-toolkit.html}
}

@article{erolUMCPSoundComplete,
  title = {{{UMCP}}: {{A Sound}} and {{Complete Procedure}} for {{Hierarchical Task-Network Planning}}},
  author = {Erol, Kutluhan and Hendler, James and Nau, Dana S},
  abstract = {One big obstacle to understanding the nature of hierarchical task network (htn) planning has been the lack of a clear theoretical framework. In particular, no one has yet presented a clear and concise htn algorithm that is sound and complete. In this paper, we present a formal syntax and semantics for htn planning. Based on this syntax and semantics, we are able to define an algorithm for htn planning and prove it sound and complete.},
  langid = {english},
  file = {C:\Users\s233148\Zotero\storage\4QBJ6BWT\Erol et al. - UMCP A Sound and Complete Procedure for Hierarchical Task-Network Planning.pdf}
}

@article{fabianoHowOptimizeSystematic2024,
  title = {How to Optimize the Systematic Review Process Using {{AI}} Tools},
  author = {Fabiano, Nicholas and Gupta, Arnav and Bhambra, Nishaant and Luu, Brandon and Wong, Stanley and Maaz, Muhammad and Fiedorowicz, Jess G. and Smith, Andrew L. and Solmi, Marco},
  date = {2024-06},
  journaltitle = {JCPP Advances},
  shortjournal = {JCPP Advances},
  volume = {4},
  number = {2},
  pages = {e12234},
  issn = {2692-9384, 2692-9384},
  doi = {10.1002/jcv2.12234},
  url = {https://acamh.onlinelibrary.wiley.com/doi/10.1002/jcv2.12234},
  urldate = {2025-10-28},
  abstract = {Systematic reviews are a cornerstone for synthesizing the available evidence on a given topic. They simultaneously allow for gaps in the literature to be identified and provide direction for future research. However, due to the ever‐increasing volume and complexity of the available literature, traditional methods for conducting systematic reviews are less efficient and more time‐consuming. Numerous artificial intelligence (AI) tools are being released with the potential to optimize efficiency in academic writing and assist with various stages of the systematic review process including developing and refining search strategies, screening titles and abstracts for inclusion or exclusion criteria, extracting essential data from studies and summarizing findings. Therefore, in this article we provide an overview of the currently available tools and how they can be incorporated into the systematic review process to improve efficiency and quality of research synthesis. We emphasize that authors must report all AI tools that have been used at each stage to ensure replicability as part of reporting in methods.},
  langid = {english},
  file = {C:\Users\s233148\Zotero\storage\IIYLUVRU\Fabiano et al. - 2024 - How to optimize the systematic review process using AI tools.pdf}
}

@article{fikesStripsNewApproach1971,
  title = {Strips: {{A}} New Approach to the Application of Theorem Proving to Problem Solving},
  shorttitle = {Strips},
  author = {Fikes, Richard E. and Nilsson, Nils J.},
  date = {1971-12},
  journaltitle = {Artificial Intelligence},
  shortjournal = {Artificial Intelligence},
  volume = {2},
  number = {3--4},
  pages = {189--208},
  issn = {00043702},
  doi = {10.1016/0004-3702(71)90010-5},
  url = {https://linkinghub.elsevier.com/retrieve/pii/0004370271900105},
  urldate = {2025-11-10},
  abstract = {We describe a newproblem solver called STRIPS that attempts to find a sequence of operators in a spcce o f world models to transform a given initial world model into a model in which a given goal formula can be proven to be true. STRIPS represents a world n,\textasciitilde del as an arbitrary collection o f first-order predicate calculus formulas and is designed to work with .models consisting of large numbers o f formulas. It employs a resolution theorem prover to answer questions o fparticular models and uses means-ends analysis to guide it to the desired goal-satisfying model.},
  langid = {english},
  file = {C:\Users\s233148\Zotero\storage\YFJS7M78\Fikes and Nilsson - 1971 - Strips A new approach to the application of theorem proving to problem solving.pdf}
}

@online{garcezNeuralSymbolicComputingEffective2019,
  title = {Neural-{{Symbolic Computing}}: {{An Effective Methodology}} for {{Principled Integration}} of {{Machine Learning}} and {{Reasoning}}},
  shorttitle = {Neural-{{Symbolic Computing}}},
  author = {family=Garcez, given=Artur, prefix=d'Avila, useprefix=false and Gori, Marco and Lamb, Luis C. and Serafini, Luciano and Spranger, Michael and Tran, Son N.},
  date = {2019-05-15},
  eprint = {1905.06088},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1905.06088},
  url = {http://arxiv.org/abs/1905.06088},
  urldate = {2025-11-10},
  abstract = {Current advances in Artificial Intelligence and machine learning in general, and deep learning in particular have reached unprecedented impact not only across research communities, but also over popular media channels. However, concerns about interpretability and accountability of AI have been raised by influential thinkers. In spite of the recent impact of AI, several works have identified the need for principled knowledge representation and reasoning mechanisms integrated with deep learning-based systems to provide sound and explainable models for such systems. Neural-symbolic computing aims at integrating, as foreseen by Valiant, two most fundamental cognitive abilities: the ability to learn from the environment, and the ability to reason from what has been learned. Neural-symbolic computing has been an active topic of research for many years, reconciling the advantages of robust learning in neural networks and reasoning and interpretability of symbolic representation. In this paper, we survey recent accomplishments of neural-symbolic computing as a principled methodology for integrated machine learning and reasoning. We illustrate the effectiveness of the approach by outlining the main characteristics of the methodology: principled integration of neural learning with symbolic knowledge representation and reasoning allowing for the construction of explainable AI systems. The insights provided by neural-symbolic computing shed new light on the increasingly prominent need for interpretable and accountable AI systems.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence},
  file = {C\:\\Users\\s233148\\Zotero\\storage\\KGXP3ERJ\\Garcez et al. - 2019 - Neural-Symbolic Computing An Effective Methodology for Principled Integration of Machine Learning a.pdf;C\:\\Users\\s233148\\Zotero\\storage\\FNUR8SXN\\1905.html}
}

@online{garcezNeurosymbolicAI3rd2020,
  title = {Neurosymbolic {{AI}}: {{The}} 3rd {{Wave}}},
  shorttitle = {Neurosymbolic {{AI}}},
  author = {family=Garcez, given=Artur, prefix=d'Avila, useprefix=false and Lamb, Luis C.},
  date = {2020-12-16},
  eprint = {2012.05876},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2012.05876},
  url = {http://arxiv.org/abs/2012.05876},
  urldate = {2025-11-10},
  abstract = {Current advances in Artificial Intelligence (AI) and Machine Learning (ML) have achieved unprecedented impact across research communities and industry. Nevertheless, concerns about trust, safety, interpretability and accountability of AI were raised by influential thinkers. Many have identified the need for well-founded knowledge representation and reasoning to be integrated with deep learning and for sound explainability. Neural-symbolic computing has been an active area of research for many years seeking to bring together robust learning in neural networks with reasoning and explainability via symbolic representations for network models. In this paper, we relate recent and early research results in neurosymbolic AI with the objective of identifying the key ingredients of the next wave of AI systems. We focus on research that integrates in a principled way neural network-based learning with symbolic knowledge representation and logical reasoning. The insights provided by 20 years of neural-symbolic computing are shown to shed new light onto the increasingly prominent role of trust, safety, interpretability and accountability of AI. We also identify promising directions and challenges for the next decade of AI research from the perspective of neural-symbolic systems.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\s233148\\Zotero\\storage\\9TJS57EY\\Garcez and Lamb - 2020 - Neurosymbolic AI The 3rd Wave.pdf;C\:\\Users\\s233148\\Zotero\\storage\\VNR7I5NZ\\2012.html}
}

@book{ghallabAutomatedPlanningTheory2004,
  title = {Automated {{Planning}}: {{Theory}} and {{Practice}}},
  shorttitle = {Automated {{Planning}}},
  author = {Ghallab, Malik and Nau, Dana and Traverso, Paolo},
  date = {2004-05-03},
  eprint = {eCj3cKC_3ikC},
  eprinttype = {googlebooks},
  publisher = {Elsevier},
  abstract = {Automated planning technology now plays a significant role in a variety of demanding applications, ranging from controlling space vehicles and robots to playing the game of bridge. These real-world applications create new opportunities for synergy between theory and practice: observing what works well in practice leads to better theories of planning, and better theories lead to better performance of practical applications. Automated Planning mirrors this dialogue by offering a comprehensive, up-to-date resource on both the theory and practice of automated planning. The book goes well beyond classical planning, to include temporal planning, resource scheduling, planning under uncertainty, and modern techniques for plan generation, such as task decomposition, propositional satisfiability, constraint satisfaction, and model checking. The authors combine over 30 years experience in planning research and development to offer an invaluable text to researchers, professionals, and graduate students.  Provides a thorough understanding of AI planning theory and practice, and how they relate to each other Covers all the contemporary topics of planning, as well as important practical applications of planning, such as model checking and game playing Presents case studies and applications in planning engineering, space, robotics, CAD/CAM, process control, emergency operations, and games Provides lecture notes, examples of programming assignments, pointers to downloadable planning systems and related information online},
  isbn = {978-1-55860-856-6},
  langid = {english},
  pagetotal = {665},
  keywords = {Business & Economics / Management,BUSINESS & ECONOMICS / Production & Operations Management,Computers / Artificial Intelligence / Expert Systems,Computers / Artificial Intelligence / General,TECHNOLOGY & ENGINEERING / Industrial Engineering,TECHNOLOGY & ENGINEERING / Industrial Technology,Technology & Engineering / Robotics}
}

@online{guanLeveragingPretrainedLarge2023,
  title = {Leveraging {{Pre-trained Large Language Models}} to {{Construct}} and {{Utilize World Models}} for {{Model-based Task Planning}}},
  author = {Guan, Lin and Valmeekam, Karthik and Sreedharan, Sarath and Kambhampati, Subbarao},
  date = {2023-11-02},
  eprint = {2305.14909},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.14909},
  url = {http://arxiv.org/abs/2305.14909},
  urldate = {2025-11-10},
  abstract = {There is a growing interest in applying pre-trained large language models (LLMs) to planning problems. However, methods that use LLMs directly as planners are currently impractical due to several factors, including limited correctness of plans, strong reliance on feedback from interactions with simulators or even the actual environment, and the inefficiency in utilizing human feedback. In this work, we introduce a novel alternative paradigm that constructs an explicit world (domain) model in planning domain definition language (PDDL) and then uses it to plan with sound domain-independent planners. To address the fact that LLMs may not generate a fully functional PDDL model initially, we employ LLMs as an interface between PDDL and sources of corrective feedback, such as PDDL validators and humans. For users who lack a background in PDDL, we show that LLMs can translate PDDL into natural language and effectively encode corrective feedback back to the underlying domain model. Our framework not only enjoys the correctness guarantee offered by the external planners but also reduces human involvement by allowing users to correct domain models at the beginning, rather than inspecting and correcting (through interactive prompting) every generated plan as in previous work. On two IPC domains and a Household domain that is more complicated than commonly used benchmarks such as ALFWorld, we demonstrate that GPT-4 can be leveraged to produce high-quality PDDL models for over 40 actions, and the corrected PDDL models are then used to successfully solve 48 challenging planning tasks. Resources, including the source code, are released at: https://guansuns.github.io/pages/llm-dm.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence},
  file = {C\:\\Users\\s233148\\Zotero\\storage\\5ZJUZBPK\\Guan et al. - 2023 - Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based T.pdf;C\:\\Users\\s233148\\Zotero\\storage\\TA57MGJ3\\2305.html}
}

@book{haslumIntroductionPlanningDomain2019,
  title = {An {{Introduction}} to the {{Planning Domain Definition Language}}},
  author = {Haslum, Patrik and Lipovetzky, Nir and Magazzeni, Daniele and Muise, Christian},
  date = {2019-04-02},
  eprint = {bA6QDwAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {Morgan \& Claypool Publishers},
  abstract = {Planning is the branch of Artificial Intelligence (AI) that seeks to automate reasoning about plans, most importantly the reasoning that goes into formulating a plan to achieve a given goal in a given situation. AI planning is model-based: a planning system takes as input a description (or model) of the initial situation, the actions available to change it, and the goal condition to output a plan composed of those actions that will accomplish the goal when executed from the initial situation. The Planning Domain Definition Language (PDDL) is a formal knowledge representation language designed to express planning models. Developed by the planning research community as a means of facilitating systems comparison, it has become a de-facto standard input language of many planning systems, although it is not the only modelling language for planning. Several variants of PDDL have emerged that capture planning problems of different natures and complexities, with a focus on deterministic problems. The purpose of this book is two-fold. First, we present a unified and current account of PDDL, covering the subsets of PDDL that express discrete, numeric, temporal, and hybrid planning. Second, we want to introduce readers to the art of modelling planning problems in this language, through educational examples that demonstrate how PDDL is used to model realistic planning problems. The book is intended for advanced students and researchers in AI who want to dive into the mechanics of AI planning, as well as those who want to be able to use AI planning systems without an in-depth explanation of the algorithms and implementation techniques they use.},
  isbn = {978-1-62705-737-0},
  langid = {english},
  pagetotal = {189},
  keywords = {Computers / Artificial Intelligence / General,Computers / Computer Science,Computers / Languages / General}
}

@unpublished{huang2022zeroshotplanning,
  title = {Language Models as Zero-Shot Planners: {{Extracting}} Actionable Knowledge for Embodied Agents},
  author = {Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor},
  date = {2022},
  eprint = {2201.07207},
  eprinttype = {arXiv},
  url = {https://arxiv.org/abs/2201.07207}
}

@online{huangPlanningDarkLLMSymbolic2024,
  title = {Planning in the {{Dark}}: {{LLM-Symbolic Planning Pipeline}} without {{Experts}}},
  shorttitle = {Planning in the {{Dark}}},
  author = {Huang, Sukai and Lipovetzky, Nir and Cohn, Trevor},
  date = {2024-09-24},
  eprint = {2409.15915},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2409.15915},
  url = {http://arxiv.org/abs/2409.15915},
  urldate = {2025-11-04},
  abstract = {Large Language Models (LLMs) have shown promise in solving natural language-described planning tasks, but their direct use often leads to inconsistent reasoning and hallucination. While hybrid LLM-symbolic planning pipelines have emerged as a more robust alternative, they typically require extensive expert intervention to refine and validate generated action schemas. It not only limits scalability but also introduces a potential for biased interpretation, as a single expert's interpretation of ambiguous natural language descriptions might not align with the user's actual intent. To address this, we propose a novel approach that constructs an action schema library to generate multiple candidates, accounting for the diverse possible interpretations of natural language descriptions. We further introduce a semantic validation and ranking module that automatically filter and rank the generated schemas and plans without expert-in-the-loop. The experiments showed our pipeline maintains superiority in planning over the direct LLM planning approach. These findings demonstrate the feasibility of a fully automated end-to-end LLM-symbolic planner that requires no expert intervention, opening up the possibility for a broader audience to engage with AI planning with less prerequisite of domain expertise.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence},
  file = {C\:\\Users\\s233148\\Zotero\\storage\\TRJ63QFI\\Huang et al. - 2024 - Planning in the Dark LLM-Symbolic Planning Pipeline without Experts.pdf;C\:\\Users\\s233148\\Zotero\\storage\\XD8X6Z4N\\2409.html}
}

@online{kambhampatiLLMsCantPlan2024,
  title = {{{LLMs Can}}'t {{Plan}}, {{But Can Help Planning}} in {{LLM-Modulo Frameworks}}},
  author = {Kambhampati, Subbarao and Valmeekam, Karthik and Guan, Lin and Verma, Mudit and Stechly, Kaya and Bhambri, Siddhant and Saldyt, Lucas and Murthy, Anil},
  date = {2024-06-12},
  eprint = {2402.01817},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2402.01817},
  url = {http://arxiv.org/abs/2402.01817},
  urldate = {2025-11-10},
  abstract = {There is considerable confusion about the role of Large Language Models (LLMs) in planning and reasoning tasks. On one side are over-optimistic claims that LLMs can indeed do these tasks with just the right prompting or self-verification strategies. On the other side are perhaps over-pessimistic claims that all that LLMs are good for in planning/reasoning tasks are as mere translators of the problem specification from one syntactic format to another, and ship the problem off to external symbolic solvers. In this position paper, we take the view that both these extremes are misguided. We argue that auto-regressive LLMs cannot, by themselves, do planning or self-verification (which is after all a form of reasoning), and shed some light on the reasons for misunderstandings in the literature. We will also argue that LLMs should be viewed as universal approximate knowledge sources that have much more meaningful roles to play in planning/reasoning tasks beyond simple front-end/back-end format translators. We present a vision of \{\textbackslash bf LLM-Modulo Frameworks\} that combine the strengths of LLMs with external model-based verifiers in a tighter bi-directional interaction regime. We will show how the models driving the external verifiers themselves can be acquired with the help of LLMs. We will also argue that rather than simply pipelining LLMs and symbolic components, this LLM-Modulo Framework provides a better neuro-symbolic approach that offers tighter integration between LLMs and symbolic components, and allows extending the scope of model-based planning/reasoning regimes towards more flexible knowledge, problem and preference specifications.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\s233148\\Zotero\\storage\\H97MKNPX\\Kambhampati et al. - 2024 - LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks.pdf;C\:\\Users\\s233148\\Zotero\\storage\\NGZTSWLT\\2402.html}
}

@unpublished{kojima2022large,
  title = {Large Language Models Are Zero-Shot Reasoners},
  author = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  date = {2022},
  eprint = {2205.11916},
  eprinttype = {arXiv},
  url = {https://arxiv.org/abs/2205.11916}
}

@article{kwonFastAccurateTask,
  title = {Fast and {{Accurate Task Planning}} Using {{Neuro-Symbolic Language Models}} and {{Multi-level Goal Decomposition}}},
  author = {Kwon, Minseo and Kim, Yaesol and Kim, Young J},
  abstract = {In robotic task planning, symbolic planners using rule-based representations like PDDL are effective but struggle with long-sequential tasks in complicated environments due to exponentially increasing search space. Meanwhile, LLM-based approaches, which are grounded in artificial neural networks, offer faster inference and commonsense reasoning but suffer from lower success rates. To address the limitations of the current symbolic (slow speed) or LLM-based approaches (low accuracy), we propose a novel neuro-symbolic task planner that decomposes complex tasks into subgoals using LLM and carries out task planning for each subgoal using either symbolic or MCTS-based LLM planners, depending on the subgoal complexity. This decomposition reduces planning time and improves success rates by narrowing the search space and enabling LLMs to focus on more manageable tasks. Our method significantly reduces planning time while maintaining high success rates across task planning domains, as well as realworld and simulated robotics environments. More details are available at http://graphics.ewha.ac.kr/LLMTAMP/.},
  langid = {english},
  file = {C:\Users\s233148\Zotero\storage\YJJ7RIXK\Kwon et al. - Fast and Accurate Task Planning using Neuro-Symbolic Language Models and Multi-level Goal Decomposit.pdf}
}

@article{loyallBelievableAgentsBuilding,
  title = {Believable {{Agents}}: {{Building Interactive Personalities}}},
  author = {Loyall, A Bryan},
  langid = {english},
  file = {C:\Users\s233148\Zotero\storage\4BEAP3J6\Loyall - Believable Agents Building Interactive Personalities.pdf}
}

@online{lyuFaithfulChainofThoughtReasoning2023,
  title = {Faithful {{Chain-of-Thought Reasoning}}},
  author = {Lyu, Qing and Havaldar, Shreya and Stein, Adam and Zhang, Li and Rao, Delip and Wong, Eric and Apidianaki, Marianna and Callison-Burch, Chris},
  date = {2023-09-20},
  eprint = {2301.13379},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2301.13379},
  url = {http://arxiv.org/abs/2301.13379},
  urldate = {2025-11-10},
  abstract = {While Chain-of-Thought (CoT) prompting boosts Language Models' (LM) performance on a gamut of complex reasoning tasks, the generated reasoning chain does not necessarily reflect how the model arrives at the answer (aka. faithfulness). We propose Faithful CoT, a reasoning framework involving two stages: Translation (Natural Language query \$\textbackslash rightarrow\$ symbolic reasoning chain) and Problem Solving (reasoning chain \$\textbackslash rightarrow\$ answer), using an LM and a deterministic solver respectively. This guarantees that the reasoning chain provides a faithful explanation of the final answer. Aside from interpretability, Faithful CoT also improves empirical performance: it outperforms standard CoT on 9 of 10 benchmarks from 4 diverse domains, with a relative accuracy gain of 6.3\% on Math Word Problems (MWP), 3.4\% on Planning, 5.5\% on Multi-hop Question Answering (QA), and 21.4\% on Relational Inference. Furthermore, with GPT-4 and Codex, it sets the new state-of-the-art few-shot performance on 7 datasets (with 95.0+ accuracy on 6 of them), showing a strong synergy between faithfulness and accuracy.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\s233148\\Zotero\\storage\\FWDREI2B\\Lyu et al. - 2023 - Faithful Chain-of-Thought Reasoning.pdf;C\:\\Users\\s233148\\Zotero\\storage\\UUYDB6KJ\\2301.html}
}

@incollection{mateasOzCentricReviewInteractive1999,
  title = {An {{Oz-Centric Review}} of {{Interactive Drama}} and {{Believable Agents}}},
  booktitle = {Artificial {{Intelligence Today}}},
  author = {Mateas, Michael},
  editor = {Wooldridge, Michael J. and Veloso, Manuela},
  date = {1999},
  volume = {1600},
  pages = {297--328},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/3-540-48317-9_12},
  url = {http://link.springer.com/10.1007/3-540-48317-9_12},
  urldate = {2025-11-10},
  abstract = {Believable agents are autonomous agents that exhibit rich personalities. Interactive dramas take place in virtual worlds inhabited by believable agents with whom an audience interacts. In the course of this interaction, the audience experiences a story. This paper presents the research philosophy behind the Oz Project, a research group at CMU that has spent the last ten years studying believable agents and interactive drama. The paper then surveys current work from an Oz perspective.},
  isbn = {978-3-540-66428-4 978-3-540-48317-5},
  langid = {english},
  file = {C:\Users\s233148\Zotero\storage\BRNZNWQX\Mateas - 1999 - An Oz-Centric Review of Interactive Drama and Believable Agents.pdf}
}

@article{mcdermottPDDLPlanningDomain1998,
  title = {{{PDDL}} - {{The Planning Domain Definition Language}}},
  shorttitle = {{{PDDL}}},
  author = {McDermott, Drew and Ghallab, Malik and Knoblock, Craig and Ram, Ashwin and Veloso, Manuela and Weld, Daniel and Wilkins, David},
  date = {1998-10-01},
  url = {https://www.cs.cmu.edu/~mmv/planning/readings/98aips-PDDL.pdf},
  urldate = {2025-11-10},
  file = {C:\Users\s233148\Zotero\storage\RBNCDTYW\cs.cmu.edu~mmvplanningreadings98aips-PDDL.pdf.pdf}
}

@article{nauSHOP2HTNPlanning2003,
  title = {{{SHOP2}}: {{An HTN Planning System}}},
  shorttitle = {{{SHOP2}}},
  author = {Nau, D. S. and Au, T. C. and Ilghami, O. and Kuter, U. and Murdock, J. W. and Wu, D. and Yaman, F.},
  date = {2003-12-01},
  journaltitle = {Journal of Artificial Intelligence Research},
  shortjournal = {jair},
  volume = {20},
  eprint = {1106.4869},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {379--404},
  issn = {1076-9757},
  doi = {10.1613/jair.1141},
  url = {http://arxiv.org/abs/1106.4869},
  urldate = {2025-11-10},
  abstract = {The SHOP2 planning system received one of the awards for distinguished performance in the 2002 International Planning Competition. This paper describes the features of SHOP2 which enabled it to excel in the competition, especially those aspects of SHOP2 that deal with temporal and metric planning domains.},
  keywords = {Computer Science - Artificial Intelligence},
  file = {C\:\\Users\\s233148\\Zotero\\storage\\RGQNNJGT\\Au et al. - 2003 - SHOP2 An HTN Planning System.pdf;C\:\\Users\\s233148\\Zotero\\storage\\LR3WVT6Z\\1106.html}
}

@article{nayakLongHorizonPlanningMultiAgent,
  title = {Long-{{Horizon Planning}} for {{Multi-Agent Robots}} in {{Partially Observable Environments}}},
  author = {Nayak, Siddharth and Orozco, Adelmo Morrison and Zhang, Jackson and Chen, Darren and Kapoor, Aditya and Robinson, Eric and Gopalakrishnan, Karthik and Harrison, James and Ichter, Brian and Mahajan, Anuj and Balakrishnan, Hamsa},
  langid = {english},
  file = {C:\Users\s233148\Zotero\storage\7JRYCB3W\Nayak et al. - Long-Horizon Planning for Multi-Agent Robots in Partially Observable Environments.pdf}
}

@inproceedings{parkGenerativeAgentsInteractive2023a,
  title = {Generative {{Agents}}: {{Interactive Simulacra}} of {{Human Behavior}}},
  shorttitle = {Generative {{Agents}}},
  booktitle = {Proceedings of the 36th {{Annual ACM Symposium}} on {{User Interface Software}} and {{Technology}}},
  author = {Park, Joon Sung and O'Brien, Joseph and Cai, Carrie Jun and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
  date = {2023-10-29},
  series = {{{UIST}} '23},
  pages = {1--22},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3586183.3606763},
  url = {https://dl.acm.org/doi/10.1145/3586183.3606763},
  urldate = {2025-09-10},
  abstract = {Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents: computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent’s experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty-five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors. For example, starting with only a single user-specified notion that one agent wants to throw a Valentine’s Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture—observation, planning, and reflection—each contribute critically to the believability of agent behavior. By fusing large language models with computational interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.},
  isbn = {979-8-4007-0132-0},
  file = {C:\Users\s233148\Zotero\storage\F2TZRCHS\Park et al. - 2023 - Generative Agents Interactive Simulacra of Human Behavior.pdf}
}

@article{riedlNarrativePlanningBalancing2010,
  title = {Narrative {{Planning}}: {{Balancing Plot}} and {{Character}}},
  shorttitle = {Narrative {{Planning}}},
  author = {Riedl, M. O. and Young, R. M.},
  date = {2010-09-29},
  journaltitle = {Journal of Artificial Intelligence Research},
  shortjournal = {jair},
  volume = {39},
  pages = {217--268},
  issn = {1076-9757},
  doi = {10.1613/jair.2989},
  url = {https://jair.org/index.php/jair/article/view/10669},
  urldate = {2025-11-10},
  abstract = {Narrative, and in particular storytelling, is an important part of the human experience. Consequently, computational systems that can reason about narrative can be more effective communicators, entertainers, educators, and trainers. One of the central challenges in computational narrative reasoning is narrative generation, the automated creation of meaningful event sequences. There are many factors – logical and aesthetic – that contribute to the success of a narrative artifact. Central to this success is its understandability. We argue that the following two attributes of narratives are universal: (a) the logical causal progression of plot, and (b) character believability. Character believability is the perception by the audience that the actions performed by characters do not negatively impact the audience’s suspension of disbelief. Specifically, characters must be perceived by the audience to be intentional agents. In this article, we explore the use of refinement search as a technique for solving the narrative generation problem – to find a sound and believable sequence of character actions that transforms an initial world state into a world state in which goal propositions hold. We describe a novel refinement search planning algorithm –the Intent-based Partial Order Causal Link (IPOCL) planner – that, in addition to creating causally sound plot progression, reasons about character intentionality by identifying possible character goals that explain their actions and creating plan structures that explain why those characters commit to their goals. We present the results of an empirical evaluation that demonstrates that narrative plans generated by the IPOCL algorithm support audience comprehension of character intentions better than plans generated by conventional partial-order planners.},
  langid = {english},
  file = {C:\Users\s233148\Zotero\storage\QMWCCV9Y\Riedl and Young - 2010 - Narrative Planning Balancing Plot and Character.pdf}
}

@book{russellArtificialIntelligenceModern2022,
  title = {Artificial Intelligence: A Modern Approach},
  shorttitle = {Artificial Intelligence},
  author = {Russell, Stuart J. and Norvig, Peter},
  namea = {Chang, Ming-wei and Devlin, Jacob and Dragan, Anca and Forsyth, David and Goodfellow, Ian and Malik, Jitendra and Mansinghka, Vikash and Pearl, Judea and Wooldridge, Michael J.},
  nameatype = {collaborator},
  date = {2022},
  series = {Prentice {{Hall}} Series in Artificial Intelligence},
  edition = {Fourth edition, global edition},
  publisher = {Pearson},
  location = {Boston},
  abstract = {The most comprehensive, up-to-date introduction to the theory and practice of artificial intelligence The long-anticipated revision of Artificial Intelligence: A Modern Approach explores the full breadth and depth of the field of artificial intelligence (AI). The 4th Edition brings readers up to date on the latest technologies, present concepts in a more unified manner, and offers new or expanded coverage of machine learning, deep learning, transfer learning, multi agent systems, robotics, natural language processing, causality, probabilistic programming, privacy, fairness, and safe AI},
  isbn = {978-1-292-40113-3 978-1-292-40117-1},
  langid = {english},
  pagetotal = {1},
  file = {C:\Users\s233148\Zotero\storage\N9LVXMBA\preview-9781292401171_A41586057.pdf}
}

@unpublished{shinn2023reflexion,
  title = {Reflexion: {{Language}} Agents with Verbal Reinforcement Learning},
  author = {Shinn, Noah and Labash, Federico and Rumshisky, Anna},
  date = {2023},
  eprint = {2303.11366},
  eprinttype = {arXiv},
  url = {https://arxiv.org/abs/2303.11366}
}

@online{silverLearningNeuroSymbolicSkills2022,
  title = {Learning {{Neuro-Symbolic Skills}} for {{Bilevel Planning}}},
  author = {Silver, Tom and Athalye, Ashay and Tenenbaum, Joshua B. and Lozano-Perez, Tomas and Kaelbling, Leslie Pack},
  date = {2022-10-12},
  eprint = {2206.10680},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2206.10680},
  url = {http://arxiv.org/abs/2206.10680},
  urldate = {2025-11-10},
  abstract = {Decision-making is challenging in robotics environments with continuous object-centric states, continuous actions, long horizons, and sparse feedback. Hierarchical approaches, such as task and motion planning (TAMP), address these challenges by decomposing decision-making into two or more levels of abstraction. In a setting where demonstrations and symbolic predicates are given, prior work has shown how to learn symbolic operators and neural samplers for TAMP with manually designed parameterized policies. Our main contribution is a method for learning parameterized polices in combination with operators and samplers. These components are packaged into modular neuro-symbolic skills and sequenced together with search-then-sample TAMP to solve new tasks. In experiments in four robotics domains, we show that our approach -- bilevel planning with neuro-symbolic skills -- can solve a wide range of tasks with varying initial states, goals, and objects, outperforming six baselines and ablations. Video: https://youtu.be/PbFZP8rPuGg Code: https://tinyurl.com/skill-learning},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C\:\\Users\\s233148\\Zotero\\storage\\HG9G4836\\Silver et al. - 2022 - Learning Neuro-Symbolic Skills for Bilevel Planning.pdf;C\:\\Users\\s233148\\Zotero\\storage\\639EDY9V\\2206.html}
}

@online{tantakounLLMsPlanningModelers2025,
  title = {{{LLMs}} as {{Planning Modelers}}: {{A Survey}} for {{Leveraging Large Language Models}} to {{Construct Automated Planning Models}}},
  shorttitle = {{{LLMs}} as {{Planning Modelers}}},
  author = {Tantakoun, Marcus and Zhu, Xiaodan and Muise, Christian},
  date = {2025-03-22},
  eprint = {2503.18971},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2503.18971},
  url = {http://arxiv.org/abs/2503.18971},
  urldate = {2025-09-10},
  abstract = {Large Language Models (LLMs) excel in various natural language tasks but often struggle with long-horizon planning problems requiring structured reasoning. This limitation has drawn interest in integrating neuro-symbolic approaches within the Automated Planning (AP) and Natural Language Processing (NLP) communities. However, identifying optimal AP deployment frameworks can be daunting. This paper aims to provide a timely survey of the current research with an in-depth analysis, positioning LLMs as tools for extracting and refining planning models to support reliable AP planners. By systematically reviewing the current state of research, we highlight methodologies, and identify critical challenges and future directions, hoping to contribute to the joint research on NLP and Automated Planning.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence},
  file = {C\:\\Users\\s233148\\Zotero\\storage\\ZKXTYSW5\\Tantakoun et al. - 2025 - LLMs as Planning Modelers A Survey for Leveraging Large Language Models to Construct Automated Plan.pdf;C\:\\Users\\s233148\\Zotero\\storage\\W7TGQFIZ\\2503.html}
}

@online{tenceAutomatableEvaluationMethod2010,
  title = {Automatable {{Evaluation Method Oriented}} toward {{Behaviour Believability}} for {{Video Games}}},
  author = {Tencé, Fabien and Buche, Cédric},
  date = {2010-09-02},
  eprint = {1009.0501},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1009.0501},
  url = {http://arxiv.org/abs/1009.0501},
  urldate = {2025-10-20},
  abstract = {Classic evaluation methods of believable agents are time-consuming because they involve many human to judge agents. They are well suited to validate work on new believable behaviours models. However, during the implementation, numerous experiments can help to improve agents' believability. We propose a method which aim at assessing how much an agent's behaviour looks like humans' behaviours. By representing behaviours with vectors, we can store data computed for humans and then evaluate as many agents as needed without further need of humans. We present a test experiment which shows that even a simple evaluation following our method can reveal differences between quite believable agents and humans. This method seems promising although, as shown in our experiment, results' analysis can be difficult.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence},
  file = {C\:\\Users\\s233148\\Zotero\\storage\\7PYZAXBN\\Tencé and Buche - 2010 - Automatable Evaluation Method Oriented toward Behaviour Believability for Video Games.pdf;C\:\\Users\\s233148\\Zotero\\storage\\BEY9MXXW\\1009.html}
}

@inproceedings{vaswani2017attention,
  title = {Attention Is All You Need},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Łukasz and Polosukhin, Illia},
  date = {2017},
  eprint = {1706.03762},
  eprinttype = {arXiv},
  url = {https://arxiv.org/abs/1706.03762}
}

@online{wangGroundingLanguagePlans2024,
  title = {Grounding {{Language Plans}} in {{Demonstrations Through Counterfactual Perturbations}}},
  author = {Wang, Yanwei and Wang, Tsun-Hsuan and Mao, Jiayuan and Hagenow, Michael and Shah, Julie},
  date = {2024-04-29},
  eprint = {2403.17124},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2403.17124},
  url = {http://arxiv.org/abs/2403.17124},
  urldate = {2025-11-10},
  abstract = {Grounding the common-sense reasoning of Large Language Models (LLMs) in physical domains remains a pivotal yet unsolved problem for embodied AI. Whereas prior works have focused on leveraging LLMs directly for planning in symbolic spaces, this work uses LLMs to guide the search of task structures and constraints implicit in multi-step demonstrations. Specifically, we borrow from manipulation planning literature the concept of mode families, which group robot configurations by specific motion constraints, to serve as an abstraction layer between the high-level language representations of an LLM and the low-level physical trajectories of a robot. By replaying a few human demonstrations with synthetic perturbations, we generate coverage over the demonstrations' state space with additional successful executions as well as counterfactuals that fail the task. Our explanation-based learning framework trains an end-to-end differentiable neural network to predict successful trajectories from failures and as a by-product learns classifiers that ground low-level states and images in mode families without dense labeling. The learned grounding classifiers can further be used to translate language plans into reactive policies in the physical domain in an interpretable manner. We show our approach improves the interpretability and reactivity of imitation learning through 2D navigation and simulated and real robot manipulation tasks. Website: https://yanweiw.github.io/glide},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C\:\\Users\\s233148\\Zotero\\storage\\NC85KMZ4\\Wang et al. - 2024 - Grounding Language Plans in Demonstrations Through Counterfactual Perturbations.pdf;C\:\\Users\\s233148\\Zotero\\storage\\9QE4Y34N\\2403.html}
}

@online{weiChainofThoughtPromptingElicits2023,
  title = {Chain-of-{{Thought Prompting Elicits Reasoning}} in {{Large Language Models}}},
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  date = {2023-01-10},
  eprint = {2201.11903},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2201.11903},
  url = {http://arxiv.org/abs/2201.11903},
  urldate = {2025-11-10},
  abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\s233148\\Zotero\\storage\\YNM85DY7\\Wei et al. - 2023 - Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.pdf;C\:\\Users\\s233148\\Zotero\\storage\\ZVMTFDPB\\2201.html}
}

@online{xiaoHowFarAre2024,
  title = {How {{Far Are LLMs}} from {{Believable AI}}? {{A Benchmark}} for {{Evaluating}} the {{Believability}} of {{Human Behavior Simulation}}},
  shorttitle = {How {{Far Are LLMs}} from {{Believable AI}}?},
  author = {Xiao, Yang and Cheng, Yi and Fu, Jinlan and Wang, Jiashuo and Li, Wenjie and Liu, Pengfei},
  date = {2024-06-15},
  eprint = {2312.17115},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.17115},
  url = {http://arxiv.org/abs/2312.17115},
  urldate = {2025-10-20},
  abstract = {In recent years, AI has demonstrated remarkable capabilities in simulating human behaviors, particularly those implemented with large language models (LLMs). However, due to the lack of systematic evaluation of LLMs' simulated behaviors, the believability of LLMs among humans remains ambiguous, i.e., it is unclear which behaviors of LLMs are convincingly human-like and which need further improvements. In this work, we design SimulateBench to evaluate the believability of LLMs when simulating human behaviors. In specific, we evaluate the believability of LLMs based on two critical dimensions: 1) consistency: the extent to which LLMs can behave consistently with the given information of a human to simulate; and 2) robustness: the ability of LLMs' simulated behaviors to remain robust when faced with perturbations. SimulateBench includes 65 character profiles and a total of 8,400 questions to examine LLMs' simulated behaviors. Based on SimulateBench, we evaluate the performances of 10 widely used LLMs when simulating characters. The experimental results reveal that current LLMs struggle to align their behaviors with assigned characters and are vulnerable to perturbations in certain factors.},
  pubstate = {prepublished},
  version = {2},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society},
  file = {C\:\\Users\\s233148\\Zotero\\storage\\RNPMXZDD\\Xiao et al. - 2024 - How Far Are LLMs from Believable AI A Benchmark for Evaluating the Believability of Human Behavior.pdf;C\:\\Users\\s233148\\Zotero\\storage\\A2JUFH4H\\2312.html}
}

@unpublished{yao2023treeofthoughts,
  title = {Tree of Thoughts: {{Deliberate}} Problem Solving with Large Language Models},
  author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey Zhao and others},
  date = {2023},
  eprint = {2305.10601},
  eprinttype = {arXiv},
  url = {https://arxiv.org/abs/2305.10601}
}

@article{ye2024domainindependent,
  title = {Domain-Independent Automated Planning with Llms},
  author = {Ye, ??? and others},
  date = {2024},
  journaltitle = {arXiv preprint}
}

@article{youngOverviewMimesisArchitecture,
  title = {An {{Overview}} of the {{Mimesis Architecture}}: {{Integrating Intelligent Narrative Control}} into an {{Existing Gaming Environment}}},
  author = {Young, R Michael},
  langid = {english},
  file = {C:\Users\s233148\Zotero\storage\65AIEUW4\Young - An Overview of the Mimesis Architecture Integrating Intelligent Narrative Control into an Existing.pdf}
}

@online{zhangAgenticContextEngineering2025,
  title = {Agentic {{Context Engineering}}: {{Evolving Contexts}} for {{Self-Improving Language Models}}},
  shorttitle = {Agentic {{Context Engineering}}},
  author = {Zhang, Qizheng and Hu, Changran and Upasani, Shubhangi and Ma, Boyuan and Hong, Fenglu and Kamanuru, Vamsidhar and Rainton, Jay and Wu, Chen and Ji, Mengmeng and Li, Hanchen and Thakker, Urmish and Zou, James and Olukotun, Kunle},
  date = {2025-10-06},
  eprint = {2510.04618},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2510.04618},
  url = {http://arxiv.org/abs/2510.04618},
  urldate = {2025-10-13},
  abstract = {Large language model (LLM) applications such as agents and domain-specific reasoning increasingly rely on context adaptation -- modifying inputs with instructions, strategies, or evidence, rather than weight updates. Prior approaches improve usability but often suffer from brevity bias, which drops domain insights for concise summaries, and from context collapse, where iterative rewriting erodes details over time. Building on the adaptive memory introduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context Engineering), a framework that treats contexts as evolving playbooks that accumulate, refine, and organize strategies through a modular process of generation, reflection, and curation. ACE prevents collapse with structured, incremental updates that preserve detailed knowledge and scale with long-context models. Across agent and domain-specific benchmarks, ACE optimizes contexts both offline (e.g., system prompts) and online (e.g., agent memory), consistently outperforming strong baselines: +10.6\% on agents and +8.6\% on finance, while significantly reducing adaptation latency and rollout cost. Notably, ACE could adapt effectively without labeled supervision and instead by leveraging natural execution feedback. On the AppWorld leaderboard, ACE matches the top-ranked production-level agent on the overall average and surpasses it on the harder test-challenge split, despite using a smaller open-source model. These results show that comprehensive, evolving contexts enable scalable, efficient, and self-improving LLM systems with low overhead.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\s233148\\Zotero\\storage\\NVNZX7HK\\Zhang et al. - 2025 - Agentic Context Engineering Evolving Contexts for Self-Improving Language Models.pdf;C\:\\Users\\s233148\\Zotero\\storage\\WQPF2DRQ\\2510.html}
}

@online{zhangVerbalizedSamplingHow2025,
  title = {Verbalized {{Sampling}}: {{How}} to {{Mitigate Mode Collapse}} and {{Unlock LLM Diversity}}},
  shorttitle = {Verbalized {{Sampling}}},
  author = {Zhang, Jiayi and Yu, Simon and Chong, Derek and Sicilia, Anthony and Tomz, Michael R. and Manning, Christopher D. and Shi, Weiyan},
  date = {2025-10-10},
  eprint = {2510.01171},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2510.01171},
  url = {http://arxiv.org/abs/2510.01171},
  urldate = {2025-10-30},
  abstract = {Post-training alignment often reduces LLM diversity, leading to a phenomenon known as mode collapse. Unlike prior work that attributes this effect to algorithmic limitations, we identify a fundamental, pervasive data-level driver: typicality bias in preference data, whereby annotators systematically favor familiar text as a result of well-established findings in cognitive psychology. We formalize this bias theoretically, verify it on preference datasets empirically, and show that it plays a central role in mode collapse. Motivated by this analysis, we introduce Verbalized Sampling, a simple, training-free prompting strategy to circumvent mode collapse. VS prompts the model to verbalize a probability distribution over a set of responses (e.g., "Generate 5 jokes about coffee and their corresponding probabilities"). Comprehensive experiments show that VS significantly improves performance across creative writing (poems, stories, jokes), dialogue simulation, open-ended QA, and synthetic data generation, without sacrificing factual accuracy and safety. For instance, in creative writing, VS increases diversity by 1.6-2.1x over direct prompting. We further observe an emergent trend that more capable models benefit more from VS. In sum, our work provides a new data-centric perspective on mode collapse and a practical inference-time remedy that helps unlock pre-trained generative diversity.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\s233148\\Zotero\\storage\\RDJ8WMU6\\Zhang et al. - 2025 - Verbalized Sampling How to Mitigate Mode Collapse and Unlock LLM Diversity.pdf;C\:\\Users\\s233148\\Zotero\\storage\\LVY554YX\\2510.html}
}

@online{zhouISRLLMIterativeSelfRefined2023,
  title = {{{ISR-LLM}}: {{Iterative Self-Refined Large Language Model}} for {{Long-Horizon Sequential Task Planning}}},
  shorttitle = {{{ISR-LLM}}},
  author = {Zhou, Zhehua and Song, Jiayang and Yao, Kunpeng and Shu, Zhan and Ma, Lei},
  date = {2023-08-26},
  eprint = {2308.13724},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2308.13724},
  url = {http://arxiv.org/abs/2308.13724},
  urldate = {2025-11-04},
  abstract = {Motivated by the substantial achievements observed in Large Language Models (LLMs) in the field of natural language processing, recent research has commenced investigations into the application of LLMs for complex, long-horizon sequential task planning challenges in robotics. LLMs are advantageous in offering the potential to enhance the generalizability as task-agnostic planners and facilitate flexible interaction between human instructors and planning systems. However, task plans generated by LLMs often lack feasibility and correctness. To address this challenge, we introduce ISR-LLM, a novel framework that improves LLM-based planning through an iterative self-refinement process. The framework operates through three sequential steps: preprocessing, planning, and iterative self-refinement. During preprocessing, an LLM translator is employed to convert natural language input into a Planning Domain Definition Language (PDDL) formulation. In the planning phase, an LLM planner formulates an initial plan, which is then assessed and refined in the iterative self-refinement step by using a validator. We examine the performance of ISR-LLM across three distinct planning domains. The results show that ISR-LLM is able to achieve markedly higher success rates in task accomplishments compared to state-of-the-art LLM-based planners. Moreover, it also preserves the broad applicability and generalizability of working with natural language instructions.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {C\:\\Users\\s233148\\Zotero\\storage\\MNH3JEXP\\Zhou et al. - 2023 - ISR-LLM Iterative Self-Refined Large Language Model for Long-Horizon Sequential Task Planning.pdf;C\:\\Users\\s233148\\Zotero\\storage\\A7S42Z6M\\2308.html}
}
